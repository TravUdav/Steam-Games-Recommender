{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "project_root = os.path.dirname(os.path.dirname(current_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_json_path = os.path.join(project_root, 'data', 'raw', 'steam_games_data.json')\n",
    "df_raw_csv_path = os.path.join(project_root, 'data', 'raw', 'steam_games_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed_json_path = os.path.join(project_root, 'data', 'processed', 'steam_games_data.json')\n",
    "df_processed_csv_path = os.path.join(project_root, 'data', 'processed', 'steam_games_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from src.kaggle_downloader import SteamGameDataDownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAGGLE_API_KEY = os.environ.get('KAGGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader = SteamGameDataDownloader()  # Uses default values for dataset_name, filename, and force_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = downloader.download()\n",
    "path = r\"{}\".format(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\pyramidheadshark\\\\Repos\\\\Steam-AI-Recommendations\\\\data\\\\raw\\\\games.json\"\n",
    "path  = r\"{}\".format(path)\n",
    "# TODO: fix encoding of the original kagglehub download (unusual line separators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(path):\n",
    "    df = pd.read_json(path)\n",
    "else:\n",
    "    print(f\"File not found: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.T\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    'price', 'dlc_count', 'about_the_game',\n",
    "    'reviews', 'website', 'support_url',\n",
    "    'support_email', 'metacritic_score',\n",
    "    'metacritic_url', 'achievements', 'recommendations',\n",
    "    'notes', 'full_audio_languages', 'packages',\n",
    "    'user_score', 'score_rank', 'positive', 'negative',\n",
    "    'screenshots', 'movies',\n",
    "    'average_playtime_forever', 'average_playtime_2weeks',\n",
    "    'median_playtime_forever', 'median_playtime_2weeks',\n",
    "    'peak_ccu'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_to_remove = (\n",
    "    ((df['short_description'].isna()) | (df['short_description'] == '')) |\n",
    "    ((df['detailed_description'].isna()) | (df['detailed_description'] == '')) |\n",
    "    (df['name'].str.contains('playtest', case=False, na=False)) |\n",
    "    ((df['header_image'].isna()) | (df['header_image'] == '')) |\n",
    "    (df['supported_languages'].astype(str) == '[]') |\n",
    "    (df['categories'].astype(str) == '[]') |\n",
    "    (df['tags'].astype(str) == '[]')\n",
    ")\n",
    "\n",
    "df_filtered = df[~mask_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_only_asian_chars(text):\n",
    "    if isinstance(text, str):\n",
    "        # Регулярное выражение для поиска азиатских символов (CJK Unified Ideographs)\n",
    "        return bool(re.fullmatch(r'[\\u4E00-\\u9FFF\\u3400-\\u4DBF\\u20000-\\u2A6DF\\u2A700-\\u2B73F\\u2B740-\\u2B81F\\u2B820-\\u2CEAF\\uF900-\\uFAFF\\u3300-\\u33FF\\uFE30-\\uFE4F]+', text))\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_only_digits(text):\n",
    "    if isinstance(text, str):\n",
    "        return text.isdigit()\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_to_remove = (\n",
    "    (df['name'].apply(contains_only_asian_chars)) |\n",
    "    (df['name'].apply(contains_only_digits))\n",
    ")\n",
    "\n",
    "df_filtered = df[~mask_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['release_date'] = pd.to_datetime(df['release_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_mapping = {'true': True, 'false': False,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['windows', 'mac', 'linux']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str).str.lower().replace({'nan': None})\n",
    "        df[col] = df[col].map(bool_mapping).fillna(False).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_first_number(owner_range):\n",
    "    if isinstance(owner_range, str):\n",
    "        parts = owner_range.split(' ', 1)\n",
    "        first_part = parts[0].replace(',', '')\n",
    "        try:\n",
    "            return int(first_part)\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['estimated_owners'] = df['estimated_owners'].apply(extract_first_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_tags(row):\n",
    "    all_tags_list = []\n",
    "\n",
    "    if isinstance(row['categories'], list):\n",
    "        all_tags_list.extend(row['categories'])\n",
    "\n",
    "    if isinstance(row['genres'], list):\n",
    "        all_tags_list.extend(row['genres'])\n",
    "\n",
    "    if isinstance(row['tags'], dict):\n",
    "        all_tags_list.extend(row['tags'].keys())\n",
    "\n",
    "    return list(set(all_tags_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['all_tags'] = df.apply(combine_tags, axis=1)\n",
    "df = df.drop(columns=['categories', 'genres', 'tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_empty_with_none(series):\n",
    "    def replace_item(item):\n",
    "        if item == [] or item == [''] or item == [\"\"] or item == \"\":\n",
    "            return None\n",
    "        return item\n",
    "\n",
    "    return series.apply(replace_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['developers'] = replace_empty_with_none(df['developers'])\n",
    "df['publishers'] = replace_empty_with_none(df['publishers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, LangDetectException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english_or_russian(text):\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        return lang == 'en' or lang == 'ru'\n",
    "    except LangDetectException:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['detailed_is_en_ru'] = df['detailed_description'].apply(is_english_or_russian)\n",
    "df['short_is_en_ru'] = df['short_description'].apply(is_english_or_russian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[df['detailed_is_en_ru'] & df['short_is_en_ru']]\n",
    "df_filtered = df_filtered.drop(columns=['detailed_is_en_ru', 'short_is_en_ru'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    wordnet_lemmatizer.lemmatize('cats')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer_en = WordNetLemmatizer()\n",
    "stop_words_en = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "stop_words_ru = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_lemmatize(text, lang='en'):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    # Удаление знаков пунктуации и цифр\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    words = text.split()\n",
    "\n",
    "    if lang == 'ru':\n",
    "        lemmatized_words = [morph.parse(word)[0].normal_form for word in words if word not in stop_words_ru]\n",
    "    else:  # Предполагаем английский, если не указан русский\n",
    "        lemmatized_words = [lemmatizer_en.lemmatize(word) for word in words if word not in stop_words_en]\n",
    "\n",
    "    return \" \".join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['detailed_description_clean'] = df['detailed_description'].apply(lambda x: clean_and_lemmatize(x))\n",
    "df['short_description_clean'] = df['short_description'].apply(lambda x: clean_and_lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_specific_words(df, column, words_to_remove):\n",
    "    def remove_words(text):\n",
    "       if isinstance(text, str):\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in words_to_remove]\n",
    "        return ' '.join(filtered_words)\n",
    "       return text\n",
    "    \n",
    "    df[column] = df[column].apply(remove_words)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_description_length(df, column, min_length, max_length):\n",
    "    df_filtered = df[(df[column].str.len() >= min_length) & (df[column].str.len() <= max_length)]\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_lowercase_tags(df, column):\n",
    "    def clean_tags(tags):\n",
    "        if isinstance(tags, list):\n",
    "            cleaned_tags = [re.sub(r'[^a-zA-Z0-9\\s]', '', tag).lower().strip() for tag in tags]\n",
    "            return cleaned_tags\n",
    "        return tags\n",
    "        \n",
    "    df[column] = df[column].apply(clean_tags)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tags_count(df, column, min_tags):\n",
    "    df_filtered = df[df[column].apply(lambda x: isinstance(x, list) and len(x) >= min_tags)]\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_and_lowercase_tags(df, 'all_tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove = ['game', 'world']\n",
    "df = remove_specific_words(df, 'short_description_clean', words_to_remove)\n",
    "df = remove_specific_words(df, 'detailed_description_clean', words_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = 30\n",
    "max_length = 240\n",
    "df = filter_description_length(df, 'short_description_clean', min_length, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_tags = 3\n",
    "df = filter_tags_count(df, 'all_tags', min_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_and_lowercase_tags(df, 'all_tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(df, title=\"Profile Report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_path = os.path.join(project_root, 'src', 'visualization', 'Report.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_file(profile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(df_processed_json_path)\n",
    "df.to_csv(df_processed_csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Steam-AI-Recommendations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
