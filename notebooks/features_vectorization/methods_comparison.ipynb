{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from dotenv import load_dotenv\n",
    "from gensim.models import FastText, Word2Vec\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF, PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import Node2Vec\n",
    "from torch_geometric.utils import from_networkx\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка переменных окружения из файла .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение текущей директории\n",
    "current_dir = os.getcwd()\n",
    "# Получение корневой директории проекта\n",
    "project_root = os.path.dirname(os.path.dirname(current_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавление корневой директории проекта в sys.path для импорта модулей\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пути к файлам с исходными данными\n",
    "df_raw_json_path = os.path.join(project_root, 'data', 'raw', 'steam_games_data.json')\n",
    "df_raw_csv_path = os.path.join(project_root, 'data', 'raw', 'steam_games_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пути к файлам с обработанными данными\n",
    "df_processed_json_path = os.path.join(project_root, 'data', 'processed', 'steam_games_data.json')\n",
    "df_processed_csv_path = os.path.join(project_root, 'data', 'processed', 'steam_games_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение устройства для работы с Torch (GPU при наличии, иначе CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Используемое устройство: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка обработанных данных\n",
    "print(\"Загрузка данных...\")\n",
    "df = pd.read_json(df_processed_json_path)\n",
    "print(\"Данные загружены.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение данных на обучающую и тестовую выборки\n",
    "print(\"Разделение данных на обучающую и тестовую выборки...\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.05, random_state=42)\n",
    "print(\"Данные разделены.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dataset(df, percentage=0.1):\n",
    "    \"\"\"\n",
    "    Уменьшает размер датасета до указанного процента.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Исходный DataFrame.\n",
    "        percentage (float): Процент данных для сохранения (от 0 до 1).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Уменьшенный DataFrame.\n",
    "    \"\"\"\n",
    "    if not 0 <= percentage <= 1:\n",
    "        raise ValueError(\"Процент должен быть в диапазоне от 0 до 1\")\n",
    "\n",
    "    print(f\"Уменьшение датасета до {percentage * 100}%...\")\n",
    "\n",
    "    # Сортировка по столбцу 'estimated_owners' в порядке убывания\n",
    "    df_sorted = df.sort_values(by='estimated_owners', ascending=False)\n",
    "\n",
    "    # Вычисление количества строк для сохранения\n",
    "    num_rows = int(len(df_sorted) * percentage)\n",
    "\n",
    "    # Выборка первых строк\n",
    "    reduced_df = df_sorted.head(num_rows)\n",
    "\n",
    "    print(f\"Датасет уменьшен до {len(reduced_df)} строк.\")\n",
    "\n",
    "    return reduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = reduce_dataset(df, percentage=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_owners(df, method='log_scale'):\n",
    "    \"\"\"\n",
    "    Векторизует данные о владельцах игр.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame с данными об играх.\n",
    "        method (str): Метод векторизации ('log_scale' или 'standard').\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Векторизованные данные о владельцах.\n",
    "    \"\"\"\n",
    "    print(f\"Начало векторизации владельцев с использованием метода: {method}\")\n",
    "    owners = df['estimated_owners'].values.reshape(-1, 1)\n",
    "    if method == 'log_scale':\n",
    "        owners = np.log1p(owners)\n",
    "        scaler = MinMaxScaler()\n",
    "        owners = scaler.fit_transform(owners)\n",
    "        # Усиление значений для игр с большим количеством владельцев\n",
    "        owners_weighted = owners * (1 + (owners * 2))  # Пример: нелинейное усиление\n",
    "        return owners_weighted\n",
    "    elif method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "        owners = scaler.fit_transform(owners)\n",
    "        # Можно также применить нелинейное преобразование после стандартизации\n",
    "        return owners\n",
    "    print(f\"Завершение векторизации владельцев с использованием метода: {method}\")\n",
    "    return owners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_tags(df, method='tfidf', vectorizer=None, word2vec_params=None, fasttext_params=None, multilabel_params=None):\n",
    "    \"\"\"\n",
    "    Векторизует теги игр различными методами.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame с данными об играх.\n",
    "        method (str): Метод векторизации ('tfidf', 'multilabel', 'word2vec', 'fasttext', 'node2vec').\n",
    "        vectorizer: Объект векторизатора для TF-IDF.\n",
    "        word2vec_params (dict): Параметры для Word2Vec.\n",
    "        fasttext_params (dict): Параметры для FastText.\n",
    "        multilabel_params (dict): Параметры для MultiLabelBinarizer.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Векторизованные теги и модель (или векторизатор).\n",
    "    \"\"\"\n",
    "    print(f\"Начало векторизации тегов с использованием метода: {method}\")\n",
    "    tags = df['all_tags'].apply(lambda x: ' '.join(x))\n",
    "    if method == 'tfidf':\n",
    "        if vectorizer is None:\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            tags_vectorized = vectorizer.fit_transform(tags).toarray()\n",
    "        else:\n",
    "            tags_vectorized = vectorizer.transform(tags).toarray()\n",
    "        print(f\"Завершение векторизации тегов с использованием метода: {method}\")\n",
    "        return tags_vectorized, vectorizer\n",
    "    elif method == 'multilabel':\n",
    "        default_params = {'sparse_output': False}\n",
    "        params = multilabel_params if multilabel_params else default_params\n",
    "        print(f\"Параметры MultiLabelBinarizer: {params}\")\n",
    "        mlb = MultiLabelBinarizer(**params)\n",
    "        mlb.fit(df['all_tags'])\n",
    "        tags_vectorized = mlb.transform(df['all_tags'])\n",
    "        print(f\"Завершение векторизации тегов с использованием метода: {method}\")\n",
    "        return tags_vectorized, mlb\n",
    "    elif method == 'word2vec':\n",
    "        sentences = df['all_tags'].tolist()\n",
    "        default_params = {'vector_size': 100, 'window': 5, 'min_count': 5, 'sg': 1, 'epochs': 10,\n",
    "                          'workers': psutil.cpu_count(logical=False)}\n",
    "        params = word2vec_params if word2vec_params else default_params\n",
    "        print(f\"Параметры Word2Vec: {params}\")\n",
    "        model = Word2Vec(sentences=sentences, **params)\n",
    "        tags_vectorized = np.array(\n",
    "            [np.mean([model.wv[word] for word in tag if word in model.wv], axis=0) for tag in sentences])\n",
    "        print(f\"Завершение векторизации тегов с использованием метода: {method}\")\n",
    "        return tags_vectorized, model\n",
    "    elif method == 'fasttext':\n",
    "        sentences = df['all_tags'].tolist()\n",
    "        default_params = {'vector_size': 100, 'window': 5, 'min_count': 5, 'sg': 1, 'epochs': 10, 'min_n': 3,\n",
    "                          'max_n': 6, 'workers': psutil.cpu_count(logical=False)}\n",
    "        params = fasttext_params if fasttext_params else default_params\n",
    "        print(f\"Параметры FastText: {params}\")\n",
    "        model = FastText(sentences=sentences, **params)\n",
    "        tags_vectorized = np.array(\n",
    "            [np.mean([model.wv[word] for word in tag if word in model.wv], axis=0) for tag in sentences])\n",
    "        print(f\"Завершение векторизации тегов с использованием метода: {method}\")\n",
    "        return tags_vectorized, model\n",
    "    elif method == 'node2vec':\n",
    "        # Создание графа и обучение Node2Vec выполняется в функции evaluate_vectors\n",
    "        print(f\"Завершение векторизации тегов с использованием метода: {method}\")\n",
    "        return None, None  # Возвращаем None, чтобы обработать в evaluate_vectors\n",
    "    else:\n",
    "        raise ValueError(\"Недопустимый метод векторизации тегов.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_descriptions(df, method='tfidf', vectorizer=None, word2vec_params=None, sentence_bert_model=None,\n",
    "                           lda_params=None, nmf_params=None):\n",
    "    \"\"\"\n",
    "    Векторизует описания игр различными методами.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame с данными об играх.\n",
    "        method (str): Метод векторизации ('tfidf', 'word2vec', 'sentence_bert', 'lda', 'nmf').\n",
    "        vectorizer: Объект векторизатора для TF-IDF.\n",
    "        word2vec_params (dict): Параметры для Word2Vec.\n",
    "        sentence_bert_model: Модель Sentence-BERT.\n",
    "        lda_params (dict): Параметры для LDA.\n",
    "        nmf_params (dict): Параметры для NMF.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Векторизованные описания и модель (или векторизатор).\n",
    "    \"\"\"\n",
    "    print(f\"Начало векторизации описаний с использованием метода: {method}\")\n",
    "    if method == 'tfidf':\n",
    "        if vectorizer is None:\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            desc_vectorized = vectorizer.fit_transform(df['short_description_clean']).toarray()\n",
    "        else:\n",
    "            desc_vectorized = vectorizer.transform(df['short_description_clean']).toarray()\n",
    "        print(f\"Завершение векторизации описаний с использованием метода: {method}\")\n",
    "        return desc_vectorized, vectorizer\n",
    "    elif method == 'word2vec':\n",
    "        sentences = df['short_description_clean'].apply(lambda x: x.split()).tolist()\n",
    "        default_params = {'vector_size': 100, 'window': 5, 'min_count': 5, 'sg': 0, 'epochs': 10,\n",
    "                          'workers': psutil.cpu_count(logical=False)}\n",
    "        params = word2vec_params if word2vec_params else default_params\n",
    "        print(f\"Параметры Word2Vec: {params}\")\n",
    "        model = Word2Vec(sentences=sentences, **params)\n",
    "        desc_vectorized = np.array([\n",
    "            np.mean([model.wv[word] for word in desc if word in model.wv], axis=0)\n",
    "            if any(word in model.wv for word in desc)  # Проверка наличия хотя бы одного слова в словаре\n",
    "            else np.zeros(model.vector_size)  # Возврат нулевого вектора, если нет слов в словаре\n",
    "            for desc in sentences\n",
    "        ])\n",
    "        print(f\"Завершение векторизации описаний с использованием метода: {method}\")\n",
    "        return desc_vectorized, model\n",
    "    elif method == 'sentence_bert':\n",
    "        model = sentence_bert_model if sentence_bert_model is not None else SentenceTransformer('all-mpnet-base-v2').to(\n",
    "            device)\n",
    "        desc_vectorized = model.encode(df['short_description_clean'].tolist(), device=device)\n",
    "        print(f\"Завершение векторизации описаний с использованием метода: {method}\")\n",
    "        return desc_vectorized, model\n",
    "    elif method == 'lda':\n",
    "        if vectorizer is None:\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            desc_vectorized = vectorizer.fit_transform(df['short_description_clean'])\n",
    "        else:\n",
    "            desc_vectorized = vectorizer.transform(df['short_description_clean'])\n",
    "        default_params = {'n_components': 10, 'random_state': 0}\n",
    "        params = lda_params if lda_params else default_params\n",
    "        print(f\"Параметры LDA: {params}\")\n",
    "        lda = LatentDirichletAllocation(**params)\n",
    "        lda_vectorized = lda.fit_transform(desc_vectorized)\n",
    "        print(f\"Завершение векторизации описаний с использованием метода: {method}\")\n",
    "        return lda_vectorized, (lda, vectorizer)\n",
    "    elif method == 'nmf':\n",
    "        if vectorizer is None:\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            desc_vectorized = vectorizer.fit_transform(df['short_description_clean'])\n",
    "        else:\n",
    "            desc_vectorized = vectorizer.transform(df['short_description_clean'])\n",
    "        default_params = {'n_components': 10, 'init': 'nndsvdar', 'solver': 'mu', 'beta_loss': 'frobenius',\n",
    "                          'random_state': 0}\n",
    "        params = nmf_params if nmf_params else default_params\n",
    "        print(f\"Параметры NMF: {params}\")\n",
    "        nmf = NMF(**params)\n",
    "        nmf_vectorized = nmf.fit_transform(desc_vectorized)\n",
    "        print(f\"Завершение векторизации описаний с использованием метода: {method}\")\n",
    "        return nmf_vectorized, (nmf, vectorizer)\n",
    "    else:\n",
    "        raise ValueError(\"Недопустимый метод векторизации описаний.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vectors(train_df, owners_method='log_scale', tags_method='tfidf', desc_method='tfidf', node2vec_model=None,\n",
    "                     owners_weight_factor=1.0,\n",
    "                     word2vec_params_tags=None, fasttext_params_tags=None, multilabel_params=None,\n",
    "                     word2vec_params_desc=None, lda_params=None, nmf_params=None,\n",
    "                     sentence_bert_model=None):\n",
    "    \"\"\"\n",
    "    Вычисляет и объединяет векторы признаков для игр.\n",
    "\n",
    "    Args:\n",
    "        train_df (pd.DataFrame): Обучающий DataFrame.\n",
    "        owners_method (str): Метод векторизации владельцев.\n",
    "        tags_method (str): Метод векторизации тегов.\n",
    "        desc_method (str): Метод векторизации описаний.\n",
    "        node2vec_model: Обученная модель Node2Vec.\n",
    "        owners_weight_factor (float): Весовой коэффициент для векторов владельцев.\n",
    "        word2vec_params_tags (dict): Параметры для Word2Vec (теги).\n",
    "        fasttext_params_tags (dict): Параметры для FastText (теги).\n",
    "        multilabel_params (dict): Параметры для MultiLabelBinarizer.\n",
    "        word2vec_params_desc (dict): Параметры для Word2Vec (описания).\n",
    "        lda_params (dict): Параметры для LDA.\n",
    "        nmf_params (dict): Параметры для NMF.\n",
    "        sentence_bert_model: Модель Sentence-BERT.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Объединенные векторы, матрица схожести, скалер, модели векторизации, PCA.\n",
    "    \"\"\"\n",
    "    print(f\"Начало evaluate_vectors с владельцами: {owners_method}, тегами: {tags_method}, описаниями: {desc_method}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Векторизация данных о владельцах\n",
    "    owners_start_time = time.time()\n",
    "    owners_vectors = vectorize_owners(train_df, method=owners_method)\n",
    "    owners_vectors_weighted = owners_vectors * owners_weight_factor\n",
    "    print(f\"Применен весовой коэффициент {owners_weight_factor} к вектору владельцев.\")\n",
    "    owners_end_time = time.time()\n",
    "    print(f\"Векторизация владельцев завершена за {owners_end_time - owners_start_time:.2f} секунд\")\n",
    "\n",
    "    # Векторизация тегов\n",
    "    tags_start_time = time.time()\n",
    "    tags_model = None\n",
    "    tags_vectorizer = None\n",
    "    if tags_method == 'node2vec':\n",
    "        if node2vec_model is not None:\n",
    "            with torch.no_grad():\n",
    "                tags_vectors_full = node2vec_model().cpu()\n",
    "            node_id_to_tag = {i: node for i, node in enumerate(node2vec_model.node_names)}\n",
    "            tag_to_embeddings = {node_id_to_tag[i]: tags_vectors_full[i].detach().numpy() for i in\n",
    "                                 range(len(tags_vectors_full))}\n",
    "            sentences = train_df['all_tags'].tolist()\n",
    "            tags_vectors = np.array([np.mean([tag_to_embeddings[word] for word in tag if word in tag_to_embeddings],\n",
    "                                              axis=0) if any(word in tag_to_embeddings for word in tag) else np.zeros(\n",
    "                node2vec_model.embedding_dim) for tag in sentences])\n",
    "            tags_model = (node2vec_model, tag_to_embeddings)\n",
    "        else:\n",
    "            raise ValueError(\"Модель Node2Vec не предоставлена для оценки.\")\n",
    "    elif tags_method == 'tfidf':\n",
    "        tags_vectors, tags_vectorizer = vectorize_tags(train_df, method=tags_method)\n",
    "    elif tags_method == 'multilabel':\n",
    "        tags_vectors, tags_mlb = vectorize_tags(train_df, method=tags_method, multilabel_params=multilabel_params)\n",
    "        tags_vectorizer = tags_mlb\n",
    "    elif tags_method == 'word2vec':\n",
    "        tags_vectors, tags_model = vectorize_tags(train_df, method=tags_method, word2vec_params=word2vec_params_tags)\n",
    "    elif tags_method == 'fasttext':\n",
    "        tags_vectors, tags_model = vectorize_tags(train_df, method=tags_method, fasttext_params=fasttext_params_tags)\n",
    "    else:\n",
    "        tags_vectors, tags_model = vectorize_tags(train_df, method=tags_method)\n",
    "    tags_end_time = time.time()\n",
    "    print(f\"Векторизация тегов завершена за {tags_end_time - tags_start_time:.2f} секунд\")\n",
    "\n",
    "    # Векторизация описаний\n",
    "    desc_start_time = time.time()\n",
    "    desc_model = None\n",
    "    desc_vectorizer = None\n",
    "    if desc_method == 'tfidf':\n",
    "        desc_vectors, desc_vectorizer = vectorize_descriptions(train_df, method=desc_method)\n",
    "    elif desc_method == 'word2vec':\n",
    "        desc_vectors, desc_model = vectorize_descriptions(train_df, method=desc_method,\n",
    "                                                           word2vec_params=word2vec_params_desc)\n",
    "    elif desc_method == 'sentence_bert':\n",
    "        desc_vectors, desc_model = vectorize_descriptions(train_df, method=desc_method,\n",
    "                                                           sentence_bert_model=sentence_bert_model)\n",
    "    elif desc_method == 'lda':\n",
    "        desc_vectors, (desc_model, desc_vectorizer) = vectorize_descriptions(train_df, method=desc_method,\n",
    "                                                                             lda_params=lda_params)\n",
    "    elif desc_method == 'nmf':\n",
    "        desc_vectors, (desc_model, desc_vectorizer) = vectorize_descriptions(train_df, method=desc_method,\n",
    "                                                                             nmf_params=nmf_params)\n",
    "    else:\n",
    "        desc_vectors, desc_model = vectorize_descriptions(train_df, method=desc_method)\n",
    "    desc_end_time = time.time()\n",
    "    print(f\"Векторизация описаний завершена за {desc_end_time - desc_start_time:.2f} секунд\")\n",
    "\n",
    "    # Объединение векторов\n",
    "    combine_start_time = time.time()\n",
    "    combined_vectors = np.hstack([owners_vectors_weighted, tags_vectors, desc_vectors])\n",
    "\n",
    "    pca = None\n",
    "    # Ограничение размерности объединенных векторов с использованием PCA\n",
    "    if combined_vectors.shape[1] > 10000:  # Применяется, только если размерность превышает 10000\n",
    "        n_components = min(10000, combined_vectors.shape[0])  # Гарантируем, что n_components не больше числа образцов\n",
    "        print(\n",
    "            f\"Уменьшение размерности объединенного вектора с {combined_vectors.shape[1]} до {n_components} с помощью PCA...\")\n",
    "        pca = PCA(n_components=n_components)\n",
    "        combined_vectors = pca.fit_transform(combined_vectors)\n",
    "\n",
    "    # Масштабирование объединенных векторов\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(combined_vectors)\n",
    "    combined_vectors = scaler.transform(combined_vectors)\n",
    "    combine_end_time = time.time()\n",
    "    print(f\"Объединение векторов завершено за {combine_end_time - combine_start_time:.2f} секунд\")\n",
    "\n",
    "    # Вычисление матрицы схожести\n",
    "    similarity_start_time = time.time()\n",
    "    # Ограничение расчета матрицы схожести для больших наборов данных\n",
    "    if combined_vectors.shape[0] > 2000:  # Произвольное пороговое значение\n",
    "        print(\"Пропуск расчета полной матрицы схожести для большого набора данных.\")\n",
    "        similarity_matrix = None\n",
    "    else:\n",
    "        similarity_matrix = cosine_similarity(combined_vectors)\n",
    "    similarity_end_time = time.time()\n",
    "    print(f\"Расчет схожести завершен за {similarity_end_time - similarity_start_time:.2f} секунд\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Выполнение evaluate_vectors завершено за {end_time - start_time:.2f} секунд\")\n",
    "    return combined_vectors, similarity_matrix, scaler, (tags_model, desc_model, tags_vectorizer, desc_vectorizer), pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vector_combination(test_df, train_df, owners_method, tags_method, desc_method, tags_model=None, desc_model=None,\n",
    "                            scaler=None, tags_vectorizer=None, desc_vectorizer=None, node2vec_model=None,\n",
    "                            test_game_name=None, pca=None, sentence_bert_model=None):\n",
    "    \"\"\"\n",
    "    Тестирует комбинацию векторизации и оценивает схожесть.\n",
    "\n",
    "    Args:\n",
    "        test_df (pd.DataFrame): Тестовый DataFrame.\n",
    "        train_df (pd.DataFrame): Обучающий DataFrame.\n",
    "        owners_method (str): Метод векторизации владельцев.\n",
    "        tags_method (str): Метод векторизации тегов.\n",
    "        desc_method (str): Метод векторизации описаний.\n",
    "        tags_model: Модель векторизации тегов.\n",
    "        desc_model: Модель векторизации описаний.\n",
    "        scaler: Скалер для масштабирования векторов.\n",
    "        tags_vectorizer: Векторизатор тегов.\n",
    "        desc_vectorizer: Векторизатор описаний.\n",
    "        node2vec_model: Обученная модель Node2Vec.\n",
    "        test_game_name (str): Название тестовой игры.\n",
    "        pca: Модель PCA для уменьшения размерности.\n",
    "        sentence_bert_model: Модель Sentence-BERT.\n",
    "\n",
    "    Returns:\n",
    "        dict: Результаты тестирования (методы, название тестовой игры, топ-5 похожих игр и их оценки).\n",
    "    \"\"\"\n",
    "    print(\n",
    "        f\"--- Тестирование комбинации векторов: Владельцы={owners_method}, Теги={tags_method}, Описания={desc_method} ---\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Векторизация обучающих данных (используются обученные ранее модели)\n",
    "    train_owners_vectors = vectorize_owners(train_df, method=owners_method)\n",
    "    if tags_method == 'tfidf':\n",
    "        train_tags_vectors = tags_vectorizer.transform(train_df['all_tags'].apply(lambda x: ' '.join(x))).toarray()\n",
    "    elif tags_method == 'multilabel':\n",
    "        train_tags_vectors = tags_vectorizer.transform(train_df['all_tags'])\n",
    "    elif tags_method == 'word2vec':\n",
    "        sentences = train_df['all_tags'].tolist()\n",
    "        train_tags_vectors = np.array([np.mean([tags_model.wv[word] for word in tag if word in tags_model.wv], axis=0)\n",
    "                                       if any(word in tags_model.wv for word in tag) else np.zeros(\n",
    "            tags_model.vector_size) for tag in sentences])\n",
    "    elif tags_method == 'fasttext':\n",
    "        sentences = train_df['all_tags'].tolist()\n",
    "        train_tags_vectors = np.array([np.mean([tags_model.wv[word] for word in tag if word in tags_model.wv], axis=0)\n",
    "                                       if any(word in tags_model.wv for word in tag) else np.zeros(\n",
    "            tags_model.vector_size) for tag in sentences])\n",
    "    elif tags_method == 'node2vec':\n",
    "        if node2vec_model is not None:\n",
    "            with torch.no_grad():\n",
    "                tags_vectors_full = node2vec_model().cpu()\n",
    "            node_id_to_tag = {i: node for i, node in enumerate(node2vec_model.node_names)}\n",
    "            tag_to_embeddings = {node_id_to_tag[i]: tags_vectors_full[i].detach().numpy() for i in\n",
    "                                 range(len(tags_vectors_full))}\n",
    "            sentences = train_df['all_tags'].tolist()\n",
    "            train_tags_vectors = np.array([np.mean([tag_to_embeddings[word] for word in tag if word in tag_to_embeddings],\n",
    "                                              axis=0) if any(word in tag_to_embeddings for word in tag) else np.zeros(\n",
    "                node2vec_model.embedding_dim) for tag in sentences])\n",
    "        else:\n",
    "            raise ValueError(\"Модель Node2Vec не предоставлена для тестирования.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Неизвестный метод векторизации тегов: {tags_method}\")\n",
    "\n",
    "    if desc_method == 'tfidf':\n",
    "        train_desc_vectors = desc_vectorizer.transform(train_df['short_description_clean']).toarray()\n",
    "    elif desc_method == 'word2vec':\n",
    "        sentences = train_df['short_description_clean'].apply(lambda x: x.split()).tolist()\n",
    "        train_desc_vectors = np.array(\n",
    "            [np.mean([desc_model.wv[word] for word in desc if word in desc_model.wv], axis=0) if any(\n",
    "                word in desc_model.wv for word in desc) else np.zeros(desc_model.vector_size) for desc in sentences])\n",
    "    elif desc_method == 'sentence_bert':\n",
    "        train_desc_vectors = sentence_bert_model.encode(train_df['short_description_clean'].tolist(), device=device)\n",
    "    elif desc_method == 'lda':\n",
    "        train_desc_vectors = desc_model.transform(desc_vectorizer.transform(train_df['short_description_clean']))\n",
    "    elif desc_method == 'nmf':\n",
    "        train_desc_vectors = desc_model.transform(desc_vectorizer.transform(train_df['short_description_clean']))\n",
    "    else:\n",
    "        raise ValueError(f\"Неизвестный метод векторизации описаний: {desc_method}\")\n",
    "\n",
    "    train_combined_vectors = np.hstack([train_owners_vectors, train_tags_vectors, train_desc_vectors])\n",
    "\n",
    "    # Выбор тестовой игры\n",
    "    if test_game_name:\n",
    "        test_game = test_df[test_df['name'] == test_game_name]\n",
    "    else:\n",
    "        test_game = test_df.sample(1)\n",
    "\n",
    "    # Векторизация тестовых данных\n",
    "    test_owners_vectors = vectorize_owners(test_game, method=owners_method)\n",
    "    if tags_method == 'tfidf':\n",
    "        test_tags_vectors = tags_vectorizer.transform(test_game['all_tags'].apply(lambda x: ' '.join(x))).toarray()\n",
    "    elif tags_method == 'multilabel':\n",
    "        test_tags_vectors = tags_vectorizer.transform(test_game['all_tags'])\n",
    "    elif tags_method == 'word2vec':\n",
    "        sentences = test_game['all_tags'].tolist()\n",
    "        test_tags_vectors = np.array([np.mean([tags_model.wv[word] for word in tag if word in tags_model.wv], axis=0)\n",
    "                                      if any(word in tags_model.wv for word in tag) else np.zeros(\n",
    "            tags_model.vector_size) for tag in sentences])\n",
    "    elif tags_method == 'fasttext':\n",
    "        sentences = test_game['all_tags'].tolist()\n",
    "        test_tags_vectors = np.array([np.mean([tags_model.wv[word] for word in tag if word in tags_model.wv], axis=0)\n",
    "                                      if any(word in tags_model.wv for word in tag) else np.zeros(\n",
    "            tags_model.vector_size) for tag in sentences])\n",
    "    elif tags_method == 'node2vec':\n",
    "        if node2vec_model is not None:\n",
    "            with torch.no_grad():\n",
    "                tags_vectors_full = node2vec_model().cpu()\n",
    "            node_id_to_tag = {i: node for i, node in enumerate(node2vec_model.node_names)}\n",
    "            tag_to_embeddings = {node_id_to_tag[i]: tags_vectors_full[i].detach().numpy() for i in\n",
    "                                 range(len(tags_vectors_full))}\n",
    "            sentences = test_game['all_tags'].tolist()\n",
    "            test_tags_vectors = np.array([np.mean([tag_to_embeddings[word] for word in tag if word in tag_to_embeddings],\n",
    "                                             axis=0) if any(word in tag_to_embeddings for word in tag) else np.zeros(\n",
    "                node2vec_model.embedding_dim) for tag in sentences])\n",
    "        else:\n",
    "            raise ValueError(\"Модель Node2Vec не предоставлена для тестирования.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Неизвестный метод векторизации тегов: {tags_method}\")\n",
    "\n",
    "    if desc_method == 'tfidf':\n",
    "        test_desc_vectors = desc_vectorizer.transform(test_game['short_description_clean']).toarray()\n",
    "    elif desc_method == 'word2vec':\n",
    "        sentences = test_game['short_description_clean'].apply(lambda x: x.split()).tolist()\n",
    "        test_desc_vectors = np.array(\n",
    "            [np.mean([desc_model.wv[word] for word in desc if word in desc_model.wv], axis=0) if any(\n",
    "                word in desc_model.wv for word in desc) else np.zeros(desc_model.vector_size) for desc in sentences])\n",
    "    elif desc_method == 'sentence_bert':\n",
    "        test_desc_vectors = sentence_bert_model.encode(test_game['short_description_clean'].tolist(), device=device)\n",
    "    elif desc_method == 'lda':\n",
    "        test_desc_vectors = desc_model.transform(desc_vectorizer.transform(test_game['short_description_clean']))\n",
    "    elif desc_method == 'nmf':\n",
    "        test_desc_vectors = desc_model.transform(desc_vectorizer.transform(test_game['short_description_clean']))\n",
    "    else:\n",
    "        raise ValueError(f\"Неизвестный метод векторизации описаний: {desc_method}\")\n",
    "    \n",
    "    test_combined_vectors = np.hstack([test_owners_vectors, test_tags_vectors, test_desc_vectors])\n",
    "\n",
    "    # Применение PCA к тестовым векторам, если PCA использовался при обучении\n",
    "    if pca is not None:\n",
    "        if train_combined_vectors.shape[1] > 10000:\n",
    "            test_combined_vectors = pca.transform(test_combined_vectors)\n",
    "\n",
    "    # Применение скалера к тестовым векторам, если скалер использовался при обучении\n",
    "    if scaler is not None:\n",
    "        # Проверка на случай, если размерность тестовых и обучающих векторов отличается после PCA\n",
    "        if train_combined_vectors.shape[1] != test_combined_vectors.shape[1]:\n",
    "            min_features = min(train_combined_vectors.shape[1], test_combined_vectors.shape[1])\n",
    "            train_combined_vectors = train_combined_vectors[:, :min_features]\n",
    "            test_combined_vectors = test_combined_vectors[:, :min_features]\n",
    "        train_combined_vectors = scaler.transform(train_combined_vectors)\n",
    "        test_combined_vectors = scaler.transform(test_combined_vectors)\n",
    "\n",
    "    # Оценка качества на тестовых данных (вывод 5 наиболее похожих игр)\n",
    "    similarities = cosine_similarity(test_combined_vectors.reshape(1, -1), train_combined_vectors)\n",
    "    most_similar_indices = np.argsort(similarities[0])[-5:][::-1]\n",
    "\n",
    "    # Получаем имя игры напрямую из test_game DataFrame\n",
    "    test_game_name_value = test_game['name'].iloc[0]\n",
    "\n",
    "    top_similar_games = [train_df['name'].iloc[idx] for idx in most_similar_indices]\n",
    "    similarity_scores = [similarities[0][idx] for idx in most_similar_indices]\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Выполнение test_vector_combination завершено за {end_time - start_time:.2f} секунд\")\n",
    "    return {\n",
    "        'owners_method': owners_method,\n",
    "        'tags_method': tags_method,\n",
    "        'desc_method': desc_method,\n",
    "        'test_game_name': test_game_name_value,\n",
    "        'top_similar_game_1': top_similar_games[0],\n",
    "        'similarity_score_1': similarity_scores[0],\n",
    "        'top_similar_game_2': top_similar_games[1],\n",
    "        'similarity_score_2': similarity_scores[1],\n",
    "        'top_similar_game_3': top_similar_games[2],\n",
    "        'similarity_score_3': similarity_scores[2],\n",
    "        'top_similar_game_4': top_similar_games[3],\n",
    "        'similarity_score_4': similarity_scores[3],\n",
    "        'top_similar_game_5': top_similar_games[4],\n",
    "        'similarity_score_5': similarity_scores[4],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_combinations = [\n",
    "    {'owners': 'log_scale', 'tags': 'multilabel', 'desc': 'nmf'},\n",
    "    {'owners': 'log_scale', 'tags': 'multilabel', 'desc': 'lda'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if any(combo['tags'] == 'node2vec' for combo in vector_combinations):\n",
    "    print(\"Обучение модели Node2Vec...\")\n",
    "    start_time = time.time()\n",
    "    G = nx.Graph()\n",
    "    for tags_list in train_df['all_tags']:\n",
    "        for i in range(len(tags_list)):\n",
    "            for j in range(i + 1, len(tags_list)):\n",
    "                tag1 = tags_list[i]\n",
    "                tag2 = tags_list[j]\n",
    "                if G.has_edge(tag1, tag2):\n",
    "                    G[tag1][tag2]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(tag1, tag2, weight=1)\n",
    "\n",
    "    # Добавление ребер на основе общих соседей\n",
    "    for tag1 in list(G.nodes()): # Проходим по копии, чтобы избежать проблем с изменением размера графа\n",
    "        for tag2 in list(G.nodes()):\n",
    "            if tag1 != tag2 and not G.has_edge(tag1, tag2):\n",
    "                common_neighbors = list(nx.common_neighbors(G, tag1, tag2))\n",
    "                if common_neighbors:\n",
    "                    weight = len(common_neighbors)\n",
    "                    G.add_edge(tag1, tag2, weight=weight)\n",
    "\n",
    "    data = from_networkx(G).to(device)\n",
    "    torch.manual_seed(42)\n",
    "    embedding_dim = 120\n",
    "    walk_length = 20\n",
    "    context_size = 8\n",
    "    walks_per_node = 20\n",
    "    num_negative_samples = 10\n",
    "    p = 1.0\n",
    "    q = 0.8\n",
    "    num_epochs = 40\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    node2vec_model = Node2Vec(data.edge_index, embedding_dim=embedding_dim, walk_length=walk_length,\n",
    "                               context_size=context_size, walks_per_node=walks_per_node,\n",
    "                               num_negative_samples=num_negative_samples, p=p, q=q).to(device)\n",
    "    loader = node2vec_model.loader(batch_size=256, shuffle=True)\n",
    "    optimizer = optim.Adam(node2vec_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        node2vec_model.train()\n",
    "        total_loss = 0\n",
    "        for pos_rw, neg_rw in loader:\n",
    "            pos_rw = pos_rw.to(device)\n",
    "            neg_rw = neg_rw.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = node2vec_model.loss(pos_rw, neg_rw)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        epoch_loss = total_loss / len(loader)\n",
    "        losses.append(epoch_loss)\n",
    "        print(f'Эпоха Node2Vec {epoch + 1}: Loss = {epoch_loss}')\n",
    "\n",
    "    # Визуализация кривой обучения\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(1, num_epochs + 1), losses, marker='o')\n",
    "    plt.title('Кривая обучения Node2Vec')\n",
    "    plt.xlabel('Эпоха')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    node2vec_model.node_names = list(G.nodes())\n",
    "    end_time = time.time()\n",
    "    print(f\"Модель Node2Vec обучена за {end_time - start_time:.2f} секунд\")\n",
    "\n",
    "    # Визуализация эмбеддингов\n",
    "    if len(G.nodes()) <= 1000:\n",
    "        print(\"Визуализация эмбеддингов Node2Vec...\")\n",
    "        with torch.no_grad():\n",
    "            node_embeddings = node2vec_model().cpu().numpy()\n",
    "\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(G.nodes()) - 1), n_iter=300, early_exaggeration=20)\n",
    "        embeddings_reduced = tsne.fit_transform(node_embeddings)\n",
    "\n",
    "        tags = list(G.nodes())\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        sns.scatterplot(x=embeddings_reduced[:, 0], y=embeddings_reduced[:, 1], alpha=0.7, s=100)\n",
    "\n",
    "        for i, tag in enumerate(tags):\n",
    "            plt.annotate(str(i),\n",
    "                         xy=(embeddings_reduced[i, 0], embeddings_reduced[i, 1]),\n",
    "                         xytext=(5, 5),\n",
    "                         textcoords='offset points',\n",
    "                         ha='center',\n",
    "                         va='bottom',\n",
    "                         fontsize=12)\n",
    "\n",
    "        plt.title('Визуализация эмбеддингов Node2Vec (t-SNE)', fontsize=16)\n",
    "        plt.xlabel('Измерение t-SNE 1', fontsize=14)\n",
    "        plt.ylabel('Измерение t-SNE 2', fontsize=14)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\nСписок тегов:\")\n",
    "        for i, tag in enumerate(tags):\n",
    "            print(f\"{i}: {tag}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Слишком много тегов для эффективной визуализации эмбеддингов.\")\n",
    "\n",
    "else:\n",
    "    node2vec_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_test_game_name = \"Baldur's Gate 3\"\n",
    "results = [] # Список для хранения результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_bert_model = SentenceTransformer('all-mpnet-base-v2').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_test_games = test_df.sort_values(by='estimated_owners', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for combo in vector_combinations:\n",
    "    print(f\"Тестирование комбинации: {combo}\")\n",
    "\n",
    "    # Извлекаем параметры векторизации из combo\n",
    "    owners_method = combo['owners']\n",
    "    tags_method = combo['tags']\n",
    "    desc_method = combo['desc']\n",
    "    word2vec_params_tags = combo.get('word2vec_params_tags')\n",
    "    fasttext_params_tags = combo.get('fasttext_params_tags')\n",
    "    multilabel_params = combo.get('multilabel_params')\n",
    "    word2vec_params_desc = combo.get('word2vec_params_desc')\n",
    "    lda_params = combo.get('lda_params')\n",
    "    nmf_params = combo.get('nmf_params')\n",
    "    owners_weight_factor = combo.get('owners_weight_factor', 1.0)\n",
    "\n",
    "    # Обучаем модели векторизации на обучающей выборке\n",
    "    combined_vectors, similarity_matrix, scaler, (tags_model, desc_model, tags_vectorizer, desc_vectorizer), pca = evaluate_vectors(\n",
    "        train_df,\n",
    "        owners_method=owners_method,\n",
    "        tags_method=tags_method,\n",
    "        desc_method=desc_method,\n",
    "        node2vec_model=node2vec_model,\n",
    "        owners_weight_factor=owners_weight_factor,\n",
    "        word2vec_params_tags=word2vec_params_tags,\n",
    "        fasttext_params_tags=fasttext_params_tags,\n",
    "        multilabel_params=multilabel_params,\n",
    "        word2vec_params_desc=word2vec_params_desc,\n",
    "        lda_params=lda_params,\n",
    "        nmf_params=nmf_params,\n",
    "        sentence_bert_model=sentence_bert_model\n",
    "    )\n",
    "\n",
    "    # Итерируемся по топ-10 играм из test_df\n",
    "    for index, test_game_row in top_10_test_games.iterrows():\n",
    "        test_game_name = test_game_row['name']\n",
    "        print(f\"Тестирование с игрой из топ-10: '{test_game_name}'\")\n",
    "        result = test_vector_combination(\n",
    "            test_df, train_df,\n",
    "            owners_method=owners_method,\n",
    "            tags_method=tags_method,\n",
    "            desc_method=desc_method,\n",
    "            tags_model=tags_model,\n",
    "            desc_model=desc_model,\n",
    "            scaler=scaler,\n",
    "            tags_vectorizer=tags_vectorizer,\n",
    "            desc_vectorizer=desc_vectorizer,\n",
    "            node2vec_model=node2vec_model,\n",
    "            test_game_name=test_game_name,\n",
    "            pca=pca,\n",
    "            sentence_bert_model=sentence_bert_model\n",
    "        )\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nТаблица лучших результатов:\")\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
