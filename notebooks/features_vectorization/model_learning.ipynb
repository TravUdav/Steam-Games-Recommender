{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from cuml.preprocessing import MinMaxScaler as CumlMinMaxScaler\n",
    "from cuml.feature_extraction.text import TfidfVectorizer as CumlTfidfVectorizer\n",
    "import cupy as cp\n",
    "import cudf\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ —Ñ–∞–π–ª–∞ .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏\n",
    "current_dir = os.getcwd()\n",
    "# –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞\n",
    "project_root = os.path.dirname(os.path.dirname(current_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏\n",
    "df_raw_json_path = os.path.join(project_root, 'data', 'raw', 'steam_games_data.json')\n",
    "df_raw_csv_path = os.path.join(project_root, 'data', 'raw', 'steam_games_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏\n",
    "df_processed_json_path = os.path.join(project_root, 'data', 'processed', 'steam_games_data.json')\n",
    "df_processed_csv_path = os.path.join(project_root, 'data', 'processed', 'steam_games_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—É—Ç–∏ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º param grid\n",
    "file_lda = \"manual_coherence_results_lda.csv\"\n",
    "file_nmf = \"manual_coherence_results_nmf.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—É—Ç—å –∫ –∏—Ç–æ–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏\n",
    "model_path = \"best_model_manual_coherence.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ Torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‚öôÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "print(\"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "df = pd.read_json(df_processed_json_path)\n",
    "print(\"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dataset(df, percentage=0.1):\n",
    "    \"\"\"–£–º–µ–Ω—å—à–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ–Ω—Ç–∞.\n",
    "\n",
    "    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "        df (pd.DataFrame): –ò—Å—Ö–æ–¥–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç.\n",
    "        percentage (float): –ü—Ä–æ—Ü–µ–Ω—Ç —Ä–∞–∑–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –Ω—É–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å (–æ—Ç 0 –¥–æ 1).\n",
    "\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "        pd.DataFrame: –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç.\n",
    "\n",
    "    –í—ã–∑—ã–≤–∞–µ—Ç ValueError, –µ—Å–ª–∏ percentage –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 1].\n",
    "    \"\"\"\n",
    "    if not 0 <= percentage <= 1:\n",
    "        raise ValueError(\"‚ùå –ü—Ä–æ—Ü–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç 0 –¥–æ 1\")\n",
    "\n",
    "    print(f\"üìâ –£–º–µ–Ω—å—à–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–æ {percentage * 100}%...\")\n",
    "    df_sorted = df.sort_values(by='estimated_owners', ascending=False)\n",
    "    num_rows = int(len(df_sorted) * percentage)\n",
    "    reduced_df = df_sorted.head(num_rows)\n",
    "    print(f\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —É–º–µ–Ω—å—à–µ–Ω –¥–æ {len(reduced_df)} —Å—Ç—Ä–æ–∫.\")\n",
    "    return reduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = reduce_dataset(df, percentage=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏\n",
    "print(\"‚ûó –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏...\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.001, random_state=42)\n",
    "print(\"‚úÖ –î–∞–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchLDA(nn.Module):\n",
    "    \"\"\"–†–µ–∞–ª–∏–∑–∞—Ü–∏—è LDA –Ω–∞ PyTorch.\n",
    "\n",
    "    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏ –æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å LDA, –∏—Å–ø–æ–ª—å–∑—É—è EM-–∞–ª–≥–æ—Ä–∏—Ç–º.\n",
    "\n",
    "    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "        n_topics (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º.\n",
    "        n_vocab (int): –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è.\n",
    "        device (torch.device): –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (CPU –∏–ª–∏ GPU).\n",
    "        alpha (float): –ü–∞—Ä–∞–º–µ—Ç—Ä —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.\n",
    "        beta (float): –ü–∞—Ä–∞–º–µ—Ç—Ä —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º –ø–æ —Å–ª–æ–≤–∞–º.\n",
    "        max_iterations (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞.\n",
    "        tolerance (float): –ü–æ—Ä–æ–≥ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–ª—è EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_topics, n_vocab, device, alpha=0.1, beta=0.01, max_iterations=100, tolerance=1e-4):\n",
    "        super().__init__()\n",
    "        self.n_topics = n_topics\n",
    "        self.n_vocab = n_vocab\n",
    "        self.device = device\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.topic_term_matrix = nn.Parameter(torch.randn(n_topics, n_vocab, device=device).abs())\n",
    "        self.doc_topic_matrix = None\n",
    "        self.norm_topic_term_matrix = None\n",
    "\n",
    "    def initialize_parameters(self, docs):\n",
    "        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ LDA.\n",
    "\n",
    "        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–∞—Ç—Ä–∏—Ü—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º –∏ –º–∞—Ç—Ä–∏—Ü—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º –ø–æ —Å–ª–æ–≤–∞–º —Å–ª—É—á–∞–π–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏.\n",
    "\n",
    "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "            docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ x —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è).\n",
    "        \"\"\"\n",
    "        self.doc_topic_matrix = torch.rand(docs.shape[0], self.n_topics, device=self.device).abs()\n",
    "        self.topic_term_matrix.data = torch.randn(self.n_topics, self.n_vocab, device=self.device).abs()\n",
    "\n",
    "    def fit(self, docs, log=False):\n",
    "        \"\"\"–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å LDA –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è EM-–∞–ª–≥–æ—Ä–∏—Ç–º.\n",
    "\n",
    "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "            docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ x —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è).\n",
    "            log (bool, optional): –í–∫–ª—é—á–∞–µ—Ç –≤—ã–≤–æ–¥ –ª–æ–≥–æ–≤ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é False.\n",
    "\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "            TorchLDA: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å LDA.\n",
    "        \"\"\"\n",
    "        if log: print(\"LDA Fit started\")\n",
    "        self.initialize_parameters(docs)\n",
    "        docs = docs.to(self.device)\n",
    "        prev_likelihood = float('-inf')\n",
    "        for iteration in range(self.max_iterations):\n",
    "            doc_topic_distribution = self.expect(docs)\n",
    "            self.topic_term_matrix = self.maximize(docs, doc_topic_distribution)\n",
    "            current_likelihood = self.likelihood(docs, doc_topic_distribution)\n",
    "            if log: print(f\"Iteration {iteration+1}, Likelihood {current_likelihood:.2f}\")\n",
    "            if abs(current_likelihood - prev_likelihood) < self.tolerance:\n",
    "                if log: print(\"LDA Converged\")\n",
    "                break\n",
    "            prev_likelihood = current_likelihood\n",
    "        self.norm_topic_term_matrix = self.normalize(self.topic_term_matrix)\n",
    "        if log: print(\"LDA Fit ended\")\n",
    "        return self\n",
    "\n",
    "    def expect(self, docs):\n",
    "        \"\"\"–í—ã–ø–æ–ª–Ω—è–µ—Ç E-—à–∞–≥ EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞ –¥–ª—è LDA.\n",
    "\n",
    "        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º, —É—á–∏—Ç—ã–≤–∞—è —Ç–µ–∫—É—â—É—é –º–∞—Ç—Ä–∏—Ü—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º –ø–æ —Å–ª–æ–≤–∞–º.\n",
    "\n",
    "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "            docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\n",
    "\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "            torch.Tensor: –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.\n",
    "        \"\"\"\n",
    "        doc_topic_distribution = torch.matmul(docs, self.topic_term_matrix.T) + self.alpha\n",
    "        doc_topic_distribution = self.normalize(doc_topic_distribution)\n",
    "        return doc_topic_distribution\n",
    "\n",
    "    def maximize(self, docs, doc_topic_distribution):\n",
    "        \"\"\"–í—ã–ø–æ–ª–Ω—è–µ—Ç M-—à–∞–≥ EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞ –¥–ª—è LDA.\n",
    "\n",
    "        –û–±–Ω–æ–≤–ª—è–µ—Ç –º–∞—Ç—Ä–∏—Ü—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º –ø–æ —Å–ª–æ–≤–∞–º, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –æ—Ü–µ–Ω–µ–Ω–Ω–æ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.\n",
    "\n",
    "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "            docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\n",
    "            doc_topic_distribution (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.\n",
    "\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "            torch.Tensor: –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º –ø–æ —Å–ª–æ–≤–∞–º.\n",
    "        \"\"\"\n",
    "        topic_term_matrix = torch.matmul(doc_topic_distribution.T, docs) + self.beta\n",
    "        return topic_term_matrix\n",
    "\n",
    "    def likelihood(self, docs, doc_topic_distribution):\n",
    "        \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ –¥–ª—è —Ç–µ–∫—É—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞.\n",
    "\n",
    "        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞.\n",
    "\n",
    "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "            docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\n",
    "            doc_topic_distribution (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.\n",
    "\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "            float: –ó–Ω–∞—á–µ–Ω–∏–µ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è.\n",
    "        \"\"\"\n",
    "        log_likelihood = torch.sum(docs * torch.log(torch.matmul(doc_topic_distribution, self.normalize(self.topic_term_matrix))))\n",
    "        return log_likelihood.item()\n",
    "\n",
    "    def normalize(self, matrix):\n",
    "        \"\"\"–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –º–∞—Ç—Ä–∏—Ü—É, –ø—Ä–∏–≤–æ–¥—è —Å—É–º–º—ã —Å—Ç—Ä–æ–∫ –∫ –µ–¥–∏–Ω–∏—Ü–µ.\n",
    "\n",
    "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "            matrix (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.\n",
    "\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "            torch.Tensor: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞.\n",
    "        \"\"\"\n",
    "        row_sums = matrix.sum(axis=1, keepdim=True)\n",
    "        return matrix / row_sums\n",
    "\n",
    "    def transform(self, docs):\n",
    "        \"\"\"–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ç–µ–º.\n",
    "\n",
    "        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å LDA –¥–ª—è –ø—Ä–æ–µ—Ü–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ.\n",
    "\n",
    "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "            docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\n",
    "\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "            torch.Tensor: –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º –¥–ª—è –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\n",
    "\n",
    "        –í—ã–∑—ã–≤–∞–µ—Ç ValueError, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å LDA –µ—â–µ –Ω–µ –æ–±—É—á–µ–Ω–∞.\n",
    "        \"\"\"\n",
    "        if self.norm_topic_term_matrix is None:\n",
    "            raise ValueError(\"‚ùå –ú–æ–¥–µ–ª—å LDA –µ—â–µ –Ω–µ –æ–±—É—á–µ–Ω–∞.\")\n",
    "        docs = docs.to(self.device)\n",
    "        doc_topic_distribution = torch.matmul(docs, self.norm_topic_term_matrix.T) + self.alpha\n",
    "        return self.normalize(doc_topic_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_owners(df, method='log_scale', scaler=None):\n",
    "    \"\"\"–í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö –∏–≥—Ä.\n",
    "\n",
    "    –ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –∫ –¥–∞–Ω–Ω—ã–º –æ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤ –∏–≥—Ä.\n",
    "\n",
    "    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "        df (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Å—Ç–æ–ª–±–µ—Ü 'estimated_owners'.\n",
    "        method (str, optional): –ú–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: 'log_scale' –∏–ª–∏ 'standard'. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 'log_scale'.\n",
    "        scaler (CumlMinMaxScaler, optional): –û–±—É—á–µ–Ω–Ω—ã–π scaler –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö. –ï—Å–ª–∏ None, scaler –Ω–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è.\n",
    "\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "        np.ndarray: –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö.\n",
    "\n",
    "    –í—ã–∑—ã–≤–∞–µ—Ç ValueError, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π –º–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏.\n",
    "    \"\"\"\n",
    "    owners = df['estimated_owners'].values.reshape(-1, 1)\n",
    "    owners = np.array(owners, dtype=float)\n",
    "    owners = np.nan_to_num(owners, nan=0)\n",
    "    if method == 'log_scale':\n",
    "        owners = np.log1p(owners)\n",
    "        if scaler is not None:\n",
    "           owners = scaler.transform(owners)\n",
    "        owners_weighted = owners * (1 + (owners * 2))\n",
    "        return owners_weighted\n",
    "    elif method == 'standard':\n",
    "        if scaler is not None:\n",
    "           owners = scaler.transform(owners)\n",
    "        return owners\n",
    "    else:\n",
    "        raise ValueError(\"‚ùå –ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π –º–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_tags(df, multilabel_params=None):\n",
    "    \"\"\"–í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç —Ç–µ–≥–∏ –∏–≥—Ä, –∏—Å–ø–æ–ª—å–∑—É—è MultiLabelBinarizer.\n",
    "\n",
    "    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–µ–≥–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –∏–≥—Ä—ã –≤ –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–π –±–∏–Ω–∞—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä.\n",
    "\n",
    "    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "        df (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Å—Ç–æ–ª–±–µ—Ü 'all_tags' —Å–æ —Å–ø–∏—Å–∫–∞–º–∏ —Ç–µ–≥–æ–≤.\n",
    "        multilabel_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è MultiLabelBinarizer. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é {'sparse_output': False}.\n",
    "\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "        tuple: –ö–æ—Ä—Ç–µ–∂, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π:\n",
    "            - np.ndarray: –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–≥–∏.\n",
    "            - MultiLabelBinarizer: –û–±—É—á–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç MultiLabelBinarizer.\n",
    "    \"\"\"\n",
    "    default_params = {'sparse_output': False}\n",
    "    params = multilabel_params if multilabel_params else default_params\n",
    "    mlb = MultiLabelBinarizer(**params)\n",
    "    mlb.fit(df['all_tags'])\n",
    "    tags_vectorized = mlb.transform(df['all_tags'])\n",
    "    return tags_vectorized, mlb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_descriptions(df, nmf_params=None, lda_params=None, vectorizer_cuml=None):\n",
    "    \"\"\"–í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –∏–≥—Ä, –∏—Å–ø–æ–ª—å–∑—É—è TF-IDF –∏ NMF –∏–ª–∏ LDA.\n",
    "\n",
    "    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç CumlTfidfVectorizer –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π –≤ –≤–µ–∫—Ç–æ—Ä—ã TF-IDF,\n",
    "    –∞ –∑–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω—è–µ—Ç NMF –∏–ª–∏ LDA –¥–ª—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è.\n",
    "\n",
    "    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "        df (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Å—Ç–æ–ª–±–µ—Ü 'short_description_clean' —Å –æ—á–∏—â–µ–Ω–Ω—ã–º–∏ –æ–ø–∏—Å–∞–Ω–∏—è–º–∏.\n",
    "        nmf_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è NMF. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è NMF.\n",
    "        lda_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è LDA. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è LDA.\n",
    "        vectorizer_cuml (CumlTfidfVectorizer, optional): –û–±—É—á–µ–Ω–Ω—ã–π CumlTfidfVectorizer.\n",
    "\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "        tuple: –ö–æ—Ä—Ç–µ–∂, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π:\n",
    "            - np.ndarray: –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è (—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤–µ–∫—Ç–æ—Ä—ã).\n",
    "            - NMF –∏–ª–∏ LatentDirichletAllocation: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å NMF –∏–ª–∏ LDA.\n",
    "\n",
    "    –í—ã–∑—ã–≤–∞–µ—Ç ValueError, –µ—Å–ª–∏ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω –æ–±—É—á–µ–Ω–Ω—ã–π CumlTfidfVectorizer –∏–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã nmf_params –∏–ª–∏ lda_params.\n",
    "    \"\"\"\n",
    "    if vectorizer_cuml is None:\n",
    "        raise ValueError(\"‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –æ–±—É—á–µ–Ω–Ω—ã–π CumlTfidfVectorizer.\")\n",
    "    desc_vectorized_cuml = vectorizer_cuml.transform(df['short_description_clean'])\n",
    "    desc_vectorized_cuml_cpu = desc_vectorized_cuml.get()\n",
    "    data = cp.asnumpy(desc_vectorized_cuml_cpu.data)\n",
    "    indices = cp.asnumpy(desc_vectorized_cuml_cpu.indices)\n",
    "    indptr = cp.asnumpy(desc_vectorized_cuml_cpu.indptr)\n",
    "    shape = desc_vectorized_cuml_cpu.shape\n",
    "    desc_vectorized_cpu = csr_matrix((data, indices, indptr), shape=shape)\n",
    "\n",
    "    if nmf_params:\n",
    "        nmf = NMF(**nmf_params)\n",
    "        nmf_vectorized = nmf.fit_transform(desc_vectorized_cpu)\n",
    "        return nmf_vectorized, nmf\n",
    "    elif lda_params:\n",
    "        lda = LatentDirichletAllocation(**lda_params)\n",
    "        lda_vectorized = lda.fit_transform(desc_vectorized_cpu)\n",
    "        return lda_vectorized, lda\n",
    "    else:\n",
    "        raise ValueError(\"‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å nmf_params –∏–ª–∏ lda_params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_topic_coherence(model, vectorizer, texts):\n",
    "    \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ç–µ–º—ã –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç UMass coherence –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "        model: –û–±—É—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å (NMF –∏–ª–∏ LDA) —Å –∞—Ç—Ä–∏–±—É—Ç–æ–º 'components_'.\n",
    "        vectorizer: –û–±—É—á–µ–Ω–Ω—ã–π TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —Å –º–µ—Ç–æ–¥–æ–º 'get_feature_names'.\n",
    "        texts (list): –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "        float: –ó–Ω–∞—á–µ–Ω–∏–µ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–º—ã. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç -999 –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        feature_names_cuml = vectorizer.get_feature_names()\n",
    "\n",
    "        if isinstance(feature_names_cuml, cudf.core.series.Series):\n",
    "            feature_names = feature_names_cuml.to_pandas().tolist()\n",
    "        elif not isinstance(feature_names_cuml, list):\n",
    "            return -999\n",
    "\n",
    "        if hasattr(model, 'components_') and feature_names is not None:\n",
    "            topic_vectors = model.components_\n",
    "            if isinstance(topic_vectors, cp.ndarray):\n",
    "                topic_vectors_np = topic_vectors.get()\n",
    "            else:\n",
    "                topic_vectors_np = topic_vectors\n",
    "\n",
    "            top_words_idx = topic_vectors_np.argsort()[:, ::-1]\n",
    "            top_words = [[feature_names[i] for i in topic_word_idx[:10]] for topic_word_idx in top_words_idx]\n",
    "\n",
    "            dictionary = Dictionary([text.split() for text in texts])\n",
    "            tokenized_texts = [text.split() for text in texts]\n",
    "\n",
    "            cm = CoherenceModel(topics=top_words, texts=tokenized_texts, dictionary=dictionary, coherence='u_mass')\n",
    "\n",
    "            coherence_score = cm.get_coherence()\n",
    "            return coherence_score\n",
    "        else:\n",
    "            return -999\n",
    "    except Exception:\n",
    "        return -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_topic_diversity(model):\n",
    "    \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è.\n",
    "\n",
    "    –ò–∑–º–µ—Ä—è–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ç–µ–º –≤ –º–æ–¥–µ–ª–∏, —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞—è —Å—Ä–µ–¥–Ω–µ–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏ —Ç–µ–º.\n",
    "\n",
    "    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "        model: –û–±—É—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å (NMF –∏–ª–∏ LDA) —Å –∞—Ç—Ä–∏–±—É—Ç–æ–º 'components_'.\n",
    "\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "        float: –°—Ä–µ–¥–Ω–µ–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ç–µ–º.\n",
    "               –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç -1, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∏–º–µ–µ—Ç –∞—Ç—Ä–∏–±—É—Ç–∞ 'components_' –∏–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º –º–µ–Ω—å—à–µ 2.\n",
    "    \"\"\"\n",
    "    if not hasattr(model, 'components_'):\n",
    "        print(\"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –∏–º–µ–µ—Ç –∞—Ç—Ä–∏–±—É—Ç–∞ components_.\")\n",
    "        return -1\n",
    "\n",
    "    topic_vectors = model.components_\n",
    "    if isinstance(topic_vectors, cp.ndarray):\n",
    "        topic_vectors_np = topic_vectors.get()\n",
    "    else:\n",
    "        topic_vectors_np = topic_vectors\n",
    "\n",
    "    if topic_vectors_np.shape[0] < 2:\n",
    "        print(\"‚ö†Ô∏è –ú–µ–Ω–µ–µ –¥–≤—É—Ö —Ç–µ–º. –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤—ã—á–∏—Å–ª–∏—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ.\")\n",
    "        return -1\n",
    "\n",
    "    num_topics = topic_vectors_np.shape[0]\n",
    "    total_similarity = 0\n",
    "    num_pairs = 0\n",
    "\n",
    "    for i in range(num_topics):\n",
    "      for j in range(i+1, num_topics):\n",
    "         similarity = cosine_similarity(topic_vectors_np[i].reshape(1, -1), topic_vectors_np[j].reshape(1, -1))[0][0]\n",
    "         total_similarity += similarity\n",
    "         num_pairs += 1\n",
    "\n",
    "    if num_pairs == 0:\n",
    "      return -1\n",
    "\n",
    "    average_similarity = total_similarity / num_pairs\n",
    "    average_distance = 1 - average_similarity\n",
    "    return average_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_intra_topic_diversity(model, feature_names, num_top_words=10):\n",
    "    \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Å–ª–æ–≤ –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–π —Ç–µ–º—ã, –∏—Å–ø–æ–ª—å–∑—É—è —ç–Ω—Ç—Ä–æ–ø–∏—é.\n",
    "\n",
    "    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã —Å–ª–æ–≤–∞ –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–π —Ç–µ–º—ã, –∏—Å–ø–æ–ª—å–∑—É—è —ç–Ω—Ç—Ä–æ–ø–∏—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —Ç–æ–ø-—Å–ª–æ–≤.\n",
    "\n",
    "    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "        model: –û–±—É—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å (NMF –∏–ª–∏ LDA) —Å –∞—Ç—Ä–∏–±—É—Ç–æ–º 'components_'.\n",
    "        feature_names (list): –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Å–ª–æ–≤) –∏–∑ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.\n",
    "        num_top_words (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-—Å–ª–æ–≤, —É—á–∏—Ç—ã–≤–∞–µ–º—ã—Ö –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —ç–Ω—Ç—Ä–æ–ø–∏–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 10.\n",
    "\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "        float: –°—Ä–µ–¥–Ω—è—è —ç–Ω—Ç—Ä–æ–ø–∏—è –ø–æ –≤—Å–µ–º —Ç–µ–º–∞–º, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∞—è –≤–Ω—É—Ç—Ä–∏—Ç–æ–ø–∏–∫–æ–≤–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ.\n",
    "               –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç -1, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∏–º–µ–µ—Ç –∞—Ç—Ä–∏–±—É—Ç–∞ 'components_' –∏–ª–∏ –Ω–µ—Ç —Ç–µ–º –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞.\n",
    "    \"\"\"\n",
    "    if not hasattr(model, 'components_'):\n",
    "        print(\"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –∏–º–µ–µ—Ç –∞—Ç—Ä–∏–±—É—Ç–∞ components_.\")\n",
    "        return -1\n",
    "\n",
    "    topic_vectors = model.components_\n",
    "    if isinstance(topic_vectors, cp.ndarray):\n",
    "        topic_vectors_np = topic_vectors.get()\n",
    "    else:\n",
    "        topic_vectors_np = topic_vectors\n",
    "    num_topics = topic_vectors_np.shape[0]\n",
    "\n",
    "    if num_topics == 0:\n",
    "        print(\"‚ö†Ô∏è –ù–µ—Ç —Ç–µ–º –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è.\")\n",
    "        return -1\n",
    "\n",
    "    topic_entropies = []\n",
    "    for topic in topic_vectors_np:\n",
    "        top_word_indices = np.argsort(topic)[::-1][:num_top_words]\n",
    "        top_word_probabilities = topic[top_word_indices]\n",
    "        normalized_probabilities = top_word_probabilities / np.sum(top_word_probabilities)\n",
    "        topic_entropy = entropy(normalized_probabilities, base=2)\n",
    "        topic_entropies.append(topic_entropy)\n",
    "\n",
    "    return np.mean(topic_entropies) if topic_entropies else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_top_words=10):\n",
    "    \"\"\"–í—ã–≤–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã.\n",
    "\n",
    "    –û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —Ç–æ–ø-—Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã –≤ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "        model: –û–±—É—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å (NMF –∏–ª–∏ LDA) —Å –∞—Ç—Ä–∏–±—É—Ç–æ–º 'components_'.\n",
    "        feature_names (list): –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Å–ª–æ–≤) –∏–∑ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.\n",
    "        num_top_words (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 10.\n",
    "    \"\"\"\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"   –¢–µ–º–∞ #{topic_idx}:\", end=' ')\n",
    "        top_word_indices = topic.argsort()[::-1][:num_top_words]\n",
    "        top_words = [feature_names[i] for i in top_word_indices]\n",
    "        print(\" \".join(top_words))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics_with_diversity(model, feature_names, num_top_words=25, num_display_words = 10):\n",
    "    \"\"\"–í—ã–≤–æ–¥–∏—Ç —Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã –∏ –∏—Ö —ç–Ω—Ç—Ä–æ–ø–∏—é.\n",
    "\n",
    "    –û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã, –∞ —Ç–∞–∫–∂–µ –∑–Ω–∞—á–µ–Ω–∏–µ —ç–Ω—Ç—Ä–æ–ø–∏–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Å–ª–æ–≤ –≤ —Ç–µ–º–µ.\n",
    "\n",
    "    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "        model: –û–±—É—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å (NMF –∏–ª–∏ LDA) —Å –∞—Ç—Ä–∏–±—É—Ç–æ–º 'components_'.\n",
    "        feature_names (list): –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Å–ª–æ–≤) –∏–∑ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.\n",
    "        num_top_words (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-—Å–ª–æ–≤, —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º—ã—Ö –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —ç–Ω—Ç—Ä–æ–ø–∏–∏ –∏ –æ—Ç–±–æ—Ä–∞. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 25.\n",
    "        num_display_words (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 10.\n",
    "    \"\"\"\n",
    "    if not hasattr(model, 'components_'):\n",
    "        print(\"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –∏–º–µ–µ—Ç –∞—Ç—Ä–∏–±—É—Ç–∞ components_.\")\n",
    "        return\n",
    "\n",
    "    topic_vectors = model.components_\n",
    "    if isinstance(topic_vectors, cp.ndarray):\n",
    "        topic_vectors_np = topic_vectors.get()\n",
    "    else:\n",
    "        topic_vectors_np = topic_vectors\n",
    "\n",
    "    for topic_idx, topic in enumerate(topic_vectors_np):\n",
    "        print(f\"   –¢–µ–º–∞ #{topic_idx}. \", end=' ')\n",
    "        top_word_indices = np.argsort(topic)[::-1][:num_top_words]\n",
    "        top_words = [feature_names[i] for i in top_word_indices]\n",
    "        print(f\"   –¢–æ–ø-{num_display_words} —Å–ª–æ–≤: {' '.join(top_words[:num_display_words])}\")\n",
    "        normalized_probabilities = topic[top_word_indices] / np.sum(topic[top_word_indices])\n",
    "        topic_entropy = entropy(normalized_probabilities, base=2)\n",
    "        print(f\"   –≠–Ω—Ç—Ä–æ–ø–∏—è —Ç–µ–º—ã: {topic_entropy:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.\n",
    "\n",
    "    –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö, —Ç–µ–≥–∏ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –∏–≥—Ä, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä—ã.\n",
    "\n",
    "    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "        owners_method (str, optional): –ú–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö ('log_scale' –∏–ª–∏ 'standard'). –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 'log_scale'.\n",
    "        multilabel_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è MultiLabelBinarizer. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.\n",
    "        nmf_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è NMF. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è NMF –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏–π. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.\n",
    "        lda_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è LDA. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è LDA –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏–π. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.\n",
    "        tag_weight (float, optional): –í–µ—Å, –ø—Ä–∏–º–µ–Ω—è–µ–º—ã–π –∫ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–º —Ç–µ–≥–∞–º. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 1.0.\n",
    "        tfidf_cuml_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è CumlTfidfVectorizer. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.\n",
    "    \"\"\"\n",
    "    def __init__(self, owners_method='log_scale', multilabel_params=None, nmf_params=None, lda_params=None, tag_weight=1.0, tfidf_cuml_params=None):\n",
    "        self.owners_method = owners_method\n",
    "        self.multilabel_params = multilabel_params\n",
    "        self.nmf_params = nmf_params\n",
    "        self.lda_params = lda_params\n",
    "        self.tag_weight = tag_weight\n",
    "        self.tfidf_cuml_params = tfidf_cuml_params if tfidf_cuml_params else {}\n",
    "        self.tfidf_cuml = CumlTfidfVectorizer(**self.tfidf_cuml_params)\n",
    "        self.mlb = None\n",
    "        self.nmf = None\n",
    "        self.lda = None\n",
    "        self.tfidf_feature_names_out_ = None\n",
    "        self.scaler = CumlMinMaxScaler()\n",
    "        self.transformed_owners_vectors = None\n",
    "        self.transformed_tags_vectors = None\n",
    "        self.transformed_desc_vectors = None\n",
    "        self.transformed_combined_vectors = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"–û–±—É—á–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.\n",
    "\n",
    "        –í—ã–ø–æ–ª–Ω—è–µ—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤, —Ç–µ–≥–æ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–π, –∞ —Ç–∞–∫–∂–µ –æ–±—É—á–∞–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä—ã –∏ —Å–∫–∞–ª–µ—Ä—ã.\n",
    "\n",
    "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "            X (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ ('estimated_owners', 'all_tags', 'short_description_clean').\n",
    "            y (None): –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω—É–∂–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ API scikit-learn.\n",
    "\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "            CombinedVectorizer: –û–±—É—á–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä.\n",
    "        \"\"\"\n",
    "        self.owners_vectors = vectorize_owners(X, method=self.owners_method)\n",
    "        self.tags_vectors, self.mlb = vectorize_tags(X, multilabel_params=self.multilabel_params)\n",
    "        if isinstance(self.tags_vectors, cp.sparse.csr_matrix):\n",
    "            print(\"‚ÑπÔ∏è –í–µ–∫—Ç–æ—Ä—ã —Ç–µ–≥–æ–≤ - cupy sparse matrix, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ numpy...\")\n",
    "            self.tags_vectors = np.array(cp.asnumpy(self.tags_vectors.todense()), dtype=np.float64)\n",
    "        if self.tags_vectors.ndim == 1:\n",
    "            self.tags_vectors = self.tags_vectors.reshape(-1, 1)\n",
    "\n",
    "        cleaned_descriptions = X['short_description_clean'].str.lower()\n",
    "        self.tfidf_cuml.fit(cleaned_descriptions)\n",
    "        self.tfidf_feature_names_out_ = [word for word, index in sorted(self.tfidf_cuml.vocabulary_.to_pandas().items(), key=lambda item: item[1])]\n",
    "\n",
    "        if self.nmf_params and self.lda_params is None:\n",
    "            self.desc_vectors, self.nmf = vectorize_descriptions(X, nmf_params=self.nmf_params, vectorizer_cuml=self.tfidf_cuml)\n",
    "            self.lda = None\n",
    "        elif self.lda_params and self.nmf_params is None:\n",
    "             self.desc_vectors, self.lda = vectorize_descriptions(X, lda_params=self.lda_params, vectorizer_cuml=self.tfidf_cuml)\n",
    "             self.nmf = None\n",
    "        else:\n",
    "            raise ValueError(\"‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å nmf_params –∏–ª–∏ lda_params\")\n",
    "        print(\"‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n",
    "\n",
    "        if self.nmf and hasattr(self.nmf, 'components_'):\n",
    "             if np.isnan(self.nmf.components_).any():\n",
    "                print(\"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN –∑–Ω–∞—á–µ–Ω–∏—è –≤ self.nmf.components_ –≤ fit()!\")\n",
    "        if self.lda and hasattr(self.lda, 'components_'):\n",
    "             if np.isnan(self.lda.components_).any():\n",
    "                 print(\"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN –∑–Ω–∞—á–µ–Ω–∏—è –≤ self.lda.components_ –≤ fit()!\")\n",
    "\n",
    "        owners_vectors = vectorize_owners(X, method=self.owners_method)\n",
    "        self.scaler.fit(owners_vectors)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.\n",
    "\n",
    "        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä—ã –∏ —Å–∫–∞–ª–µ—Ä—ã –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö, —Ç–µ–≥–æ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–π –≤ –µ–¥–∏–Ω–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ.\n",
    "\n",
    "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "            X (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏.\n",
    "            y (None): –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω—É–∂–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ API scikit-learn.\n",
    "\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "            np.ndarray: –ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.\n",
    "        \"\"\"\n",
    "        owners_vectors = vectorize_owners(X, method=self.owners_method, scaler=self.scaler)\n",
    "        owners_vectors = owners_vectors.reshape(owners_vectors.shape[0], -1)\n",
    "        tags_vectors = self.mlb.transform(X['all_tags'])\n",
    "\n",
    "        tag_weight = self.tag_weight\n",
    "        tags_vectors_weighted = tags_vectors * tag_weight\n",
    "        tags_vectors = tags_vectors_weighted\n",
    "\n",
    "        tfidf_transformed_cuml = self.tfidf_cuml.transform(X['short_description_clean'])\n",
    "        tfidf_transformed_cpu = tfidf_transformed_cuml.get()\n",
    "        data = cp.asnumpy(tfidf_transformed_cpu.data)\n",
    "        indices = cp.asnumpy(tfidf_transformed_cpu.indices)\n",
    "        indptr = cp.asnumpy(tfidf_transformed_cpu.indptr)\n",
    "        shape = tfidf_transformed_cpu.shape\n",
    "        tfidf_transformed = csr_matrix((data, indices, indptr), shape=shape)\n",
    "\n",
    "        desc_vectors = None\n",
    "        if self.nmf_params:\n",
    "            desc_vectors = self.nmf.transform(tfidf_transformed)\n",
    "        elif self.lda_params:\n",
    "            desc_vectors = self.lda.transform(tfidf_transformed)\n",
    "\n",
    "        if desc_vectors is not None and desc_vectors.shape[0] != owners_vectors.shape[0]:\n",
    "            raise ValueError(f\"‚ùå –ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—Ä–∞–∑—Ü–æ–≤ –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–π: {owners_vectors.shape[0]} vs {desc_vectors.shape[0]}\")\n",
    "\n",
    "        self.transformed_owners_vectors = owners_vectors\n",
    "        self.transformed_tags_vectors = tags_vectors\n",
    "        self.transformed_desc_vectors = desc_vectors\n",
    "\n",
    "        combined_vectors = np.hstack([owners_vectors, tags_vectors.toarray() if hasattr(tags_vectors, 'toarray') else tags_vectors, desc_vectors])\n",
    "        self.transformed_combined_vectors = combined_vectors\n",
    "\n",
    "        return combined_vectors\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.\n",
    "\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–∞–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞, –≤–∫–ª—é—á–∞—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤—Å–µ—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–æ–≤ –∏ –º–µ—Ç–æ–¥–æ–≤.\n",
    "\n",
    "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "            deep (bool, optional): –ï—Å–ª–∏ True, —Ç–∞–∫–∂–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –æ—Ü–µ–Ω—â–∏–∫–∞–º–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é True.\n",
    "\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "            dict: –°–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'owners_method': self.owners_method,\n",
    "            'multilabel_params': self.multilabel_params,\n",
    "             'nmf_params': self.nmf_params,\n",
    "            'lda_params': self.lda_params,\n",
    "            'tag_weight': self.tag_weight,\n",
    "            'tfidf_cuml_params': self.tfidf_cuml_params\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        \"\"\"–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.\n",
    "\n",
    "        –ü–æ–∑–≤–æ–ª—è–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏.\n",
    "\n",
    "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "            **params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ –≤ –≤–∏–¥–µ keyword arguments.\n",
    "\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "            CombinedVectorizer: –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.\n",
    "        \"\"\"\n",
    "        if 'owners_method' in params:\n",
    "            self.owners_method = params['owners_method']\n",
    "        if 'multilabel_params' in params:\n",
    "            self.multilabel_params = params['multilabel_params']\n",
    "        if 'nmf_params' in params:\n",
    "            self.nmf_params = params['nmf_params']\n",
    "        if 'lda_params' in params:\n",
    "             self.lda_params = params['lda_params']\n",
    "        if 'tag_weight' in params:\n",
    "            self.tag_weight = params['tag_weight']\n",
    "        if 'tfidf_cuml_params' in params:\n",
    "            self.tfidf_cuml_params = params['tfidf_cuml_params']\n",
    "            self.tfidf_cuml = CumlTfidfVectorizer(**self.tfidf_cuml_params)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_vector_dimensions(model, train_df):\n",
    "    \"\"\"–í—ã–≤–æ–¥–∏—Ç –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—è—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "    –ü–æ–º–æ–≥–∞–µ—Ç –ø–æ–Ω—è—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤, —Å–æ–∑–¥–∞–≤–∞–µ–º—ã—Ö Pipeline, –æ—Å–æ–±–µ–Ω–Ω–æ CombinedVectorizer.\n",
    "\n",
    "    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "        model: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Pipeline —Å —ç—Ç–∞–ø–æ–º CombinedVectorizer.\n",
    "        train_df (pd.DataFrame): DataFrame –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏.\n",
    "    \"\"\"\n",
    "    if 'vectorizer' not in model.named_steps:\n",
    "        print(\"‚ùå –û—à–∏–±–∫–∞: –í –º–æ–¥–µ–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —ç—Ç–∞–ø 'vectorizer'.\")\n",
    "        return\n",
    "\n",
    "    vectorizer = model.named_steps['vectorizer']\n",
    "\n",
    "    train_vectors = model.transform(train_df)\n",
    "    print(f\"üìê –§–æ—Ä–º–∞ –æ–±—â–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {train_vectors.shape}\")\n",
    "\n",
    "    owners_vector_size = 1\n",
    "    if hasattr(vectorizer, 'mlb') and hasattr(vectorizer.mlb, 'classes_'):\n",
    "        tags_vector_size = len(vectorizer.mlb.classes_)\n",
    "    else:\n",
    "        tags_vector_size = 0\n",
    "        print(\"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤ —Ç–µ–≥–æ–≤.\")\n",
    "\n",
    "    print(f\"üìè –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤: {owners_vector_size}\")\n",
    "    print(f\"üìè –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–≥–æ–≤: {tags_vector_size}\")\n",
    "\n",
    "    start_index_topics = owners_vector_size + tags_vector_size\n",
    "    end_index_topics = train_vectors.shape[1]\n",
    "    print(f\"üìç –ù–∞—á–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤: {start_index_topics}\")\n",
    "    print(f\"üèÅ –ö–æ–Ω–µ—á–Ω—ã–π –∏–Ω–¥–µ–∫—Å —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤: {end_index_topics}\")\n",
    "\n",
    "    train_topic_vectors = train_vectors[:, start_index_topics:end_index_topics]\n",
    "    print(f\"üìê –§–æ—Ä–º–∞ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {train_topic_vectors.shape}\")\n",
    "\n",
    "    if hasattr(vectorizer, 'transformed_owners_vectors'):\n",
    "        print(\"üìè –†–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤ (–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π):\", vectorizer.transformed_owners_vectors.shape)\n",
    "    if hasattr(vectorizer, 'transformed_tags_vectors'):\n",
    "        print(\"üìè –†–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–æ–≤ —Ç–µ–≥–æ–≤ (–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π):\", vectorizer.transformed_tags_vectors.shape)\n",
    "    if hasattr(vectorizer, 'transformed_desc_vectors'):\n",
    "        print(\"üìè –†–∞–∑–º–µ—Ä —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ (–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π):\", vectorizer.transformed_desc_vectors.shape)\n",
    "    if hasattr(vectorizer, 'transformed_combined_vectors'):\n",
    "        print(\"üìè –†–∞–∑–º–µ—Ä –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ (–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π):\", vectorizer.transformed_combined_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations_for_game(model, train_df, game_name, top_n=5):\n",
    "    \"\"\"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–π –∏–≥—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏.\n",
    "\n",
    "    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –∏–≥—Ä –∏ —Ä–∞—Å—á–µ—Ç–∞ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π.\n",
    "\n",
    "    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "        model: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Pipeline.\n",
    "        train_df (pd.DataFrame): DataFrame –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏.\n",
    "        game_name (str): –ù–∞–∑–≤–∞–Ω–∏–µ –∏–≥—Ä—ã, –¥–ª—è –∫–æ—Ç–æ—Ä–æ–π –Ω—É–∂–Ω—ã —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.\n",
    "        top_n (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã—Ö –∏–≥—Ä. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 5.\n",
    "\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "        list: –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã—Ö –∏–≥—Ä. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫, –µ—Å–ª–∏ –∏–≥—Ä–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ train_df –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–æ–ª–æ–Ω–∫—É 'name'.\n",
    "    \"\"\"\n",
    "    if 'name' not in train_df.columns:\n",
    "        print(\"‚ùå –û—à–∏–±–∫–∞: DataFrame train_df –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–æ–ª–æ–Ω–∫—É 'name'.\")\n",
    "        sys.stdout.flush()\n",
    "        return []\n",
    "\n",
    "    game_row = train_df[train_df['name'] == game_name]\n",
    "\n",
    "    if game_row.empty:\n",
    "        print(f\"‚ö†Ô∏è –ò–≥—Ä–∞ '{game_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ train_df.\")\n",
    "        sys.stdout.flush()\n",
    "        return []\n",
    "\n",
    "    game_vector = model.transform(game_row)\n",
    "    train_vectors = model.transform(train_df)\n",
    "\n",
    "    similarity_scores = cosine_similarity(game_vector, train_vectors)[0]\n",
    "\n",
    "    similarity_df = pd.DataFrame({'name': train_df['name'], 'similarity': similarity_scores})\n",
    "\n",
    "    sorted_similarity_df = similarity_df.sort_values(by='similarity', ascending=False)\n",
    "\n",
    "    recommendations_df = sorted_similarity_df[sorted_similarity_df['name'] != game_name].head(top_n)\n",
    "\n",
    "    recommendations = recommendations_df['name'].tolist()\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_hyperparameter_search(train_df, param_grid, results_file=\"manual_coherence_results.csv\"):\n",
    "    \"\"\"–í—ã–ø–æ–ª–Ω—è–µ—Ç —Ä—É—á–Ω–æ–π –ø–æ–∏—Å–∫ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏.\n",
    "\n",
    "    –ü–µ—Ä–µ–±–∏—Ä–∞–µ—Ç –∑–∞–¥–∞–Ω–Ω—É—é —Å–µ—Ç–∫—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤,\n",
    "    –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ç–µ–º—ã –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.\n",
    "\n",
    "    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "        train_df (pd.DataFrame): DataFrame –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏.\n",
    "        param_grid (dict): –°–µ—Ç–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞.\n",
    "        results_file (str, optional): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ CSV. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é \"manual_coherence_results.csv\".\n",
    "\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "        pd.DataFrame: DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ–∏—Å–∫–∞ –ø–æ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    grid = ParameterGrid(param_grid)\n",
    "    total_iterations = len(grid)\n",
    "    for i, params in enumerate(grid):\n",
    "        print('------------------------------------------------------------------------------------------')\n",
    "\n",
    "        BLUE = '\\033[94m'\n",
    "        RESET = '\\033[0m'\n",
    "\n",
    "        start_time = time.time()\n",
    "        print(f\"üß™ –û—Ü–µ–Ω–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {params} \\nüîÑ –ò—Ç–µ—Ä–∞—Ü–∏—è: ({i + 1}/{total_iterations})\")\n",
    "        try:\n",
    "            vectorizer_params = {k.split('__')[1]: v for k, v in params.items() if 'vectorizer__' in k}\n",
    "            vectorizer = CombinedVectorizer(**vectorizer_params)\n",
    "\n",
    "            pipeline = Pipeline([('vectorizer', vectorizer)])\n",
    "            pipeline.fit(train_df)\n",
    "\n",
    "            tfidf_matrix_cuml = vectorizer.tfidf_cuml.transform(train_df['short_description_clean'])\n",
    "\n",
    "            row_sums_cp = tfidf_matrix_cuml.sum(axis=1)\n",
    "\n",
    "            row_sums = row_sums_cp.get()\n",
    "\n",
    "            zero_vector_indices = np.where(row_sums == 0)[0]\n",
    "\n",
    "            if len(zero_vector_indices) > 0:\n",
    "                print(f\"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –Ω—É–ª–µ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã TF-IDF –¥–ª—è {len(zero_vector_indices)} –∏–≥—Ä:\")\n",
    "                zero_vector_indices_np = np.array(zero_vector_indices)\n",
    "                zero_vector_game_ids = train_df.iloc[zero_vector_indices_np].index.tolist()\n",
    "                print(f\"üÜî ID –∏–≥—Ä —Å –Ω—É–ª–µ–≤—ã–º–∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏: {zero_vector_game_ids}\")\n",
    "\n",
    "            tfidf_model = vectorizer.tfidf_cuml\n",
    "\n",
    "            model = None\n",
    "            diversity = -1\n",
    "            intra_topic_diversity = -1\n",
    "            model_type = None\n",
    "            if vectorizer.nmf_params:\n",
    "                model = vectorizer.nmf\n",
    "                model_type = 'nmf'\n",
    "                print(\"üé≠ –¢–µ–º—ã NMF:\")\n",
    "                if hasattr(model, 'components_'):\n",
    "                    feature_names = tfidf_model.get_feature_names()\n",
    "                    if isinstance(feature_names, cudf.core.series.Series):\n",
    "                        feature_names = feature_names.to_pandas().tolist()\n",
    "                    display_topics_with_diversity(model, feature_names)\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è –ú–æ–¥–µ–ª—å NMF –Ω–µ –∏–º–µ–µ—Ç –∞—Ç—Ä–∏–±—É—Ç–∞ components_\")\n",
    "            elif vectorizer.lda_params:\n",
    "                model = vectorizer.lda\n",
    "                model_type = 'lda'\n",
    "                print(\"üé≠ –¢–µ–º—ã LDA:\")\n",
    "                if hasattr(model, 'components_'):\n",
    "                    feature_names = tfidf_model.get_feature_names()\n",
    "                    if isinstance(feature_names, cudf.core.series.Series):\n",
    "                        feature_names = feature_names.to_pandas().tolist()\n",
    "                    display_topics_with_diversity(model, feature_names)\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è –ú–æ–¥–µ–ª—å LDA –Ω–µ –∏–º–µ–µ—Ç –∞—Ç—Ä–∏–±—É—Ç–∞ components_\")\n",
    "            else:\n",
    "                raise ValueError(\"‚ùå NMF –∏–ª–∏ LDA –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –≤ CombinedVectorizer.\")\n",
    "\n",
    "            texts_for_coherence = train_df['short_description_clean'].tolist()\n",
    "\n",
    "            coherence = calculate_topic_coherence(model, tfidf_model, texts_for_coherence)\n",
    "\n",
    "            diversity = -1\n",
    "            intra_topic_diversity = -1\n",
    "\n",
    "            if model is not None:\n",
    "                if hasattr(model, 'components_'):\n",
    "                    if np.isnan(model.components_).any():\n",
    "                        print(\"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN –∑–Ω–∞—á–µ–Ω–∏—è –≤ model.components_ –ø–µ—Ä–µ–¥ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ–º diversity!\")\n",
    "\n",
    "            diversity = calculate_topic_diversity(model)\n",
    "\n",
    "            if hasattr(model, 'components_'):\n",
    "                feature_names = tfidf_model.get_feature_names()\n",
    "                if isinstance(feature_names, cudf.core.series.Series):\n",
    "                    feature_names = feature_names.to_pandas().tolist()\n",
    "                intra_topic_diversity = calculate_intra_topic_diversity(model, feature_names)\n",
    "\n",
    "            end_time = time.time()\n",
    "            recommendations_stellaris = get_recommendations_for_game(pipeline, train_df, \"Stellaris\", top_n=5)\n",
    "\n",
    "            results.append({\n",
    "                'params': params,\n",
    "                'coherence': coherence,\n",
    "                'topic_diversity': diversity,\n",
    "                'intra_topic_diversity': intra_topic_diversity,\n",
    "                'time': end_time - start_time,\n",
    "                'recommendations_stellaris': recommendations_stellaris\n",
    "            })\n",
    "\n",
    "            output_string = \\\n",
    "            f\"\"\"\n",
    "            {BLUE}–ö–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å:{RESET}{BLUE}{coherence:>20.4f}{RESET}\n",
    "            {BLUE}–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ç–µ–º:{RESET}{BLUE}{diversity:>17.4f}{RESET}\n",
    "            {BLUE}–í–Ω—É—Ç—Ä–∏—Ç–æ–ø–∏–∫–æ–≤–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ:{RESET}{BLUE}{intra_topic_diversity:>4.4f}{RESET}\n",
    "            {BLUE}–í—Ä–µ–º—è:{RESET}{BLUE}{end_time - start_time:>24.2f} —Å–µ–∫—É–Ω–¥{RESET}\n",
    "            {BLUE}–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è Stellaris:{RESET}{BLUE}{', '.join(recommendations_stellaris) if recommendations_stellaris else '–ù–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π'}{RESET}\n",
    "            \"\"\"\n",
    "            print(output_string)\n",
    "\n",
    "        except Exception as e:\n",
    "            end_time = time.time()\n",
    "            print(f\"‚ùå –û—à–∏–±–∫–∞ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ {params}: {e}, –≤—Ä–µ–º—è: {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥\")\n",
    "            results.append({\n",
    "                'params': params,\n",
    "                'coherence': -1,\n",
    "                'topic_diversity': -1,\n",
    "                'intra_topic_diversity': -1,\n",
    "                'time': end_time - start_time,\n",
    "                'recommendations_stellaris': []\n",
    "            })\n",
    "\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(results_file, index=False)\n",
    "        print(f\"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {results_file}\")\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best_model(train_df, best_params, model_path=\"best_model.pkl\"):\n",
    "    \"\"\"–û–±—É—á–∞–µ—Ç –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –Ω–∞ –≤—Å–µ–º –æ–±—É—á–∞—é—â–µ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö.\n",
    "\n",
    "    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ø–æ–∏—Å–∫–∞, –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –µ–µ.\n",
    "\n",
    "    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "        train_df (pd.DataFrame): –ü–æ–ª–Ω—ã–π –æ–±—É—á–∞—é—â–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö.\n",
    "        best_params (dict): –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ø–æ–∏—Å–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.\n",
    "        model_path (str, optional): –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é \"best_model.pkl\".\n",
    "\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "        Pipeline: –û–±—É—á–µ–Ω–Ω–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_model = Pipeline([\n",
    "        ('vectorizer', CombinedVectorizer()),\n",
    "    ])\n",
    "    best_model.set_params(**best_params)\n",
    "    best_model.fit(train_df)\n",
    "\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(best_model, f)\n",
    "    print(f\"üíæ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: {model_path}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"‚úÖ –û–±—É—á–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥\")\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_nmf = {\n",
    "    'vectorizer__owners_method': ['log_scale'],\n",
    "    'vectorizer__multilabel_params': [{'sparse_output': True}],\n",
    "    'vectorizer__nmf_params': [\n",
    "        {'n_components': 5, 'init': 'nndsvda', 'solver': 'mu', 'beta_loss': 'frobenius'},\n",
    "        {'n_components': 25, 'init': 'nndsvda', 'solver': 'mu', 'beta_loss': 'frobenius'},\n",
    "        {'n_components': 50, 'init': 'nndsvda', 'solver': 'mu', 'beta_loss': 'frobenius'},\n",
    "        {'n_components': 100, 'init': 'nndsvda', 'solver': 'mu', 'beta_loss': 'frobenius'},\n",
    "        {'n_components': 200, 'init': 'nndsvda', 'solver': 'mu', 'beta_loss': 'frobenius'},\n",
    "    ],\n",
    "    'vectorizer__lda_params': [None],\n",
    "    'vectorizer__tfidf_cuml_params': [{'max_features': 10000}],\n",
    "    'vectorizer__tag_weight': [1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_lda = {\n",
    "    'vectorizer__owners_method': ['log_scale'],\n",
    "    'vectorizer__multilabel_params': [{'sparse_output': True}],\n",
    "    'vectorizer__nmf_params': [None],\n",
    "    'vectorizer__lda_params': [\n",
    "        {'n_components': 10, 'learning_method': 'batch', 'random_state': 42},\n",
    "        {'n_components': 10, 'learning_method': 'online', 'learning_offset': 10., 'random_state': 42},\n",
    "        {'n_components': 50, 'learning_method': 'batch', 'random_state': 42},\n",
    "        {'n_components': 50, 'learning_method': 'online', 'learning_offset': 10., 'random_state': 42},\n",
    "        {'n_components': 150, 'learning_method': 'batch', 'random_state': 42},\n",
    "        {'n_components': 150, 'learning_method': 'online', 'learning_offset': 10., 'random_state': 42},\n",
    "    ],\n",
    "    'vectorizer__tfidf_cuml_params': [{'max_features': 10000}],\n",
    "    'vectorizer__tag_weight': [1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_tfidf_max_features = {\n",
    "    'vectorizer__owners_method': ['log_scale'],\n",
    "    'vectorizer__multilabel_params': [{'sparse_output': True}],\n",
    "    'vectorizer__nmf_params': [\n",
    "        {'n_components': 50, 'init': 'nndsvda', 'solver': 'mu', 'beta_loss': 'frobenius'}\n",
    "    ],\n",
    "    'vectorizer__lda_params': [None],\n",
    "    'vectorizer__tfidf_cuml_params': [\n",
    "        {'max_features': None},\n",
    "        {'max_features': 2000},\n",
    "        {'max_features': 5000},\n",
    "        {'max_features': 10000},\n",
    "        {'max_features': 20000}\n",
    "    ],\n",
    "    'vectorizer__tag_weight': [1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_tag_weight = {\n",
    "    'vectorizer__owners_method': ['log_scale'],\n",
    "    'vectorizer__multilabel_params': [{'sparse_output': True}],\n",
    "    'vectorizer__nmf_params': [\n",
    "        {'n_components': 50, 'init': 'nndsvda', 'solver': 'mu', 'beta_loss': 'frobenius'}\n",
    "    ],\n",
    "    'vectorizer__lda_params': [None],\n",
    "    'vectorizer__tfidf_cuml_params': [{'max_features': 10000}],\n",
    "    'vectorizer__tag_weight': [\n",
    "        0.5,\n",
    "        1.0,\n",
    "        2.0,\n",
    "        3.0,\n",
    "        5.0\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_results_nmf = manual_hyperparameter_search(\n",
    "    train_df, param_grid_nmf, results_file=\"manual_coherence_results_nmf.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ NMF:\")\n",
    "print(manual_results_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_manual_result_nmf = manual_results_nmf.sort_values(by='coherence', ascending=False).iloc[0]\n",
    "best_manual_params_nmf = best_manual_result_nmf['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüèÜ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã NMF, –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø–æ–∏—Å–∫–æ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏: {best_manual_params_nmf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_results_lda = manual_hyperparameter_search(\n",
    "    train_df, param_grid_lda, results_file=\"manual_coherence_results_lda.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ LDA:\")\n",
    "print(manual_results_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_manual_result_lda = manual_results_lda.sort_values(by='coherence', ascending=False).iloc[0]\n",
    "best_manual_params_lda = best_manual_result_lda['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüèÜ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã LDA, –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø–æ–∏—Å–∫–æ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏: {best_manual_params_lda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_results_max_features = manual_hyperparameter_search(\n",
    "    train_df, param_grid_nmf, results_file=\"manual_coherence_results_max_features.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ Max Features:\")\n",
    "print(manual_results_max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_manual_result_max_features = manual_results_max_features.sort_values(by='coherence', ascending=False).iloc[0]\n",
    "best_manual_params_max_features = best_manual_result_max_features['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüèÜ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã Max Features, –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø–æ–∏—Å–∫–æ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏: {best_manual_params_max_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_results_tag_weight = manual_hyperparameter_search(\n",
    "    train_df, param_grid_tag_weight, results_file=\"manual_coherence_results_tag_weight.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ Tag Weight:\")\n",
    "print(manual_results_max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_manual_result_tag_weight = manual_results_tag_weight.sort_values(by='coherence', ascending=False).iloc[0]\n",
    "best_manual_params_tag_weight = best_manual_result_tag_weight['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüèÜ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã Tag Weight, –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø–æ–∏—Å–∫–æ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏: {best_manual_params_tag_weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_manual_params = {\n",
    "    'vectorizer__owners_method': ['log_scale'],\n",
    "    'vectorizer__multilabel_params': [{'sparse_output': True}],\n",
    "    'vectorizer__nmf_params': \n",
    "        [{'n_components': 50, 'init': 'nndsvda', 'solver': 'mu', 'beta_loss': 'frobenius'}],\n",
    "    'vectorizer__lda_params': [None],\n",
    "    'vectorizer__tfidf_cuml_params': [{'max_features': 10000}],\n",
    "    'vectorizer__tag_weight': [2.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_result = manual_hyperparameter_search(\n",
    "    train_df, best_manual_params, results_file=\"manual_coherence_result.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_result = manual_result.sort_values(by='coherence', ascending=False).iloc[0]\n",
    "manual_params = manual_result['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_model = train_best_model(train_df, manual_params, model_path=model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
