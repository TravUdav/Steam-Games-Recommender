# Module `src.vectorizer`

## Functions

` def calculate_intra_topic_diversity(model, feature_names, num_top_words=10)
`

     Expand source code
    
    
    def calculate_intra_topic_diversity(model, feature_names, num_top_words=10):
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Å–ª–æ–≤ –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–π —Ç–µ–º—ã, –∏—Å–ø–æ–ª—å–∑—É—è —ç–Ω—Ç—Ä–æ–ø–∏—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ª–æ–≤.
    
        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            model: –û–±—É—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å (NMF –∏–ª–∏ LDA).
            feature_names (list): –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Å–ª–æ–≤) –∏–∑ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.
            num_top_words (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-—Å–ª–æ–≤, —É—á–∏—Ç—ã–≤–∞–µ–º—ã—Ö –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —ç–Ω—Ç—Ä–æ–ø–∏–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 10.
    
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            float: –°—Ä–µ–¥–Ω—è—è —ç–Ω—Ç—Ä–æ–ø–∏—è –ø–æ –≤—Å–µ–º —Ç–µ–º–∞–º, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∞—è –≤–Ω—É—Ç—Ä–∏—Ç–æ–ø–∏–∫–æ–≤–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç -1 –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
        """
        if not hasattr(model, 'components_'):
            print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –∏–º–µ–µ—Ç –∞—Ç—Ä–∏–±—É—Ç–∞ components_.")
            return -1
    
        topic_vectors = model.components_
        if isinstance(topic_vectors, cp.ndarray):
            topic_vectors_np = topic_vectors.get()
        else:
            topic_vectors_np = topic_vectors
        num_topics = topic_vectors_np.shape[0]
    
        if num_topics == 0:
            print("‚ö†Ô∏è –ù–µ—Ç —Ç–µ–º –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è.")
            return -1
    
        topic_entropies = []
        for topic in topic_vectors_np:
            top_word_indices = np.argsort(topic)[::-1][:num_top_words]
            top_word_probabilities = topic[top_word_indices]
            normalized_probabilities = top_word_probabilities / np.sum(top_word_probabilities)
            topic_entropy = entropy(normalized_probabilities, base=2)
            topic_entropies.append(topic_entropy)
    
        return np.mean(topic_entropies) if topic_entropies else -1

–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Å–ª–æ–≤ –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–π —Ç–µ–º—ã, –∏—Å–ø–æ–ª—å–∑—É—è —ç–Ω—Ç—Ä–æ–ø–∏—é
—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ª–æ–≤.

–ê—Ä–≥—É–º–µ–Ω—Ç—ã: model: –û–±—É—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å (NMF –∏–ª–∏ LDA). feature_names
(list): –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Å–ª–æ–≤) –∏–∑ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.
num_top_words (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-—Å–ª–æ–≤, —É—á–∏—Ç—ã–≤–∞–µ–º—ã—Ö –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞
—ç–Ω—Ç—Ä–æ–ø–∏–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 10.

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: float: –°—Ä–µ–¥–Ω—è—è —ç–Ω—Ç—Ä–æ–ø–∏—è –ø–æ –≤—Å–µ–º —Ç–µ–º–∞–º, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∞—è
–≤–Ω—É—Ç—Ä–∏—Ç–æ–ø–∏–∫–æ–≤–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç -1 –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.

` def calculate_topic_coherence(model, vectorizer, texts) `

     Expand source code
    
    
    def calculate_topic_coherence(model, vectorizer, texts):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ç–µ–º—ã –º–æ–¥–µ–ª–∏.
    
        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            model: –û–±—É—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å (NMF –∏–ª–∏ LDA).
            vectorizer: –û–±—É—á–µ–Ω–Ω—ã–π TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä.
            texts (list): –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.
    
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            float: –ó–Ω–∞—á–µ–Ω–∏–µ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–º—ã. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç -999 –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
        """
        try:
            feature_names_cuml = vectorizer.get_feature_names()
    
            if isinstance(feature_names_cuml, cudf.core.series.Series):
                feature_names = feature_names_cuml.to_pandas().tolist()
            elif not isinstance(feature_names_cuml, list):
                return -999
    
            if hasattr(model, 'components_') and feature_names is not None:
                topic_vectors = model.components_
                if isinstance(topic_vectors, cp.ndarray):
                    topic_vectors_np = topic_vectors.get()
                else:
                    topic_vectors_np = topic_vectors
    
                top_words_idx = topic_vectors_np.argsort()[:, ::-1]
                top_words = [[feature_names[i] for i in topic_word_idx[:10]] for topic_word_idx in top_words_idx]
    
                dictionary = Dictionary([text.split() for text in texts])
                tokenized_texts = [text.split() for text in texts]
    
                cm = CoherenceModel(topics=top_words, texts=tokenized_texts, dictionary=dictionary, coherence='u_mass')
    
                coherence_score = cm.get_coherence()
                return coherence_score
            else:
                return -999
        except Exception:
            return -999

–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ç–µ–º—ã –º–æ–¥–µ–ª–∏.

–ê—Ä–≥—É–º–µ–Ω—Ç—ã: model: –û–±—É—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å (NMF –∏–ª–∏ LDA). vectorizer:
–û–±—É—á–µ–Ω–Ω—ã–π TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä. texts (list): –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö
–¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: float: –ó–Ω–∞—á–µ–Ω–∏–µ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–º—ã. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç -999 –≤ —Å–ª—É—á–∞–µ
–æ—à–∏–±–∫–∏.

` def calculate_topic_diversity(model) `

     Expand source code
    
    
    def calculate_topic_diversity(model):
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏ —Ç–µ–º.
    
        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            model: –û–±—É—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å (NMF –∏–ª–∏ LDA).
    
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            float: –ó–Ω–∞—á–µ–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Ç–µ–º. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç -1 –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –∏–ª–∏ –µ—Å–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º –º–µ–Ω—å—à–µ 2.
        """
        if not hasattr(model, 'components_'):
            print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –∏–º–µ–µ—Ç –∞—Ç—Ä–∏–±—É—Ç–∞ components_.")
            return -1
    
        topic_vectors = model.components_
        if isinstance(topic_vectors, cp.ndarray):
            topic_vectors_np = topic_vectors.get()
        else:
            topic_vectors_np = topic_vectors
    
        if topic_vectors_np.shape[0] < 2:
            print("‚ö†Ô∏è –ú–µ–Ω–µ–µ –¥–≤—É—Ö —Ç–µ–º. –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤—ã—á–∏—Å–ª–∏—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ.")
            return -1
    
        num_topics = topic_vectors_np.shape[0]
        total_similarity = 0
        num_pairs = 0
    
        for i in range(num_topics):
          for j in range(i+1, num_topics):
             similarity = cosine_similarity(topic_vectors_np[i].reshape(1, -1), topic_vectors_np[j].reshape(1, -1))[0][0]
             total_similarity += similarity
             num_pairs += 1
    
        if num_pairs == 0:
          return -1
    
        average_similarity = total_similarity / num_pairs
        average_distance = 1 - average_similarity
        return average_distance

–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏
—Ç–µ–º.

–ê—Ä–≥—É–º–µ–Ω—Ç—ã: model: –û–±—É—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å (NMF –∏–ª–∏ LDA).

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: float: –ó–Ω–∞—á–µ–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Ç–µ–º. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç -1 –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
–∏–ª–∏ –µ—Å–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º –º–µ–Ω—å—à–µ 2.

` def clean_text(text) `

     Expand source code
    
    
    def clean_text(text):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞: —É–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏, –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏ —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤.
    
        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.
    
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            str: –û—á–∏—â–µ–Ω–Ω—ã–π –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫–∏.
        """
        text = re.sub(r'[^\w\s]', '', text)
        text = text.lower()
        tokens = text.split()
        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
        return " ".join(tokens)

–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞: —É–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏, –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫
–Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏ —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤.

–ê—Ä–≥—É–º–µ–Ω—Ç—ã: text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: str: –û—á–∏—â–µ–Ω–Ω—ã–π –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫–∏.

` def display_topics(model, feature_names, num_top_words=10) `

     Expand source code
    
    
    def display_topics(model, feature_names, num_top_words=10):
        """–í—ã–≤–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã.
    
        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            model: –û–±—É—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å (NMF –∏–ª–∏ LDA).
            feature_names (list): –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Å–ª–æ–≤) –∏–∑ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.
            num_top_words (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 10.
        """
        for topic_idx, topic in enumerate(model.components_):
            print(f"   –¢–µ–º–∞ #{topic_idx}:", end=' ')
            top_word_indices = topic.argsort()[::-1][:num_top_words]
            top_words = [feature_names[i] for i in top_word_indices]
            print(" ".join(top_words))
        print()

–í—ã–≤–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã.

–ê—Ä–≥—É–º–µ–Ω—Ç—ã: model: –û–±—É—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å (NMF –∏–ª–∏ LDA). feature_names
(list): –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Å–ª–æ–≤) –∏–∑ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.
num_top_words (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π
—Ç–µ–º—ã. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 10.

` def display_topics_with_diversity(model, feature_names, num_top_words=25,
num_display_words=10) `

     Expand source code
    
    
    def display_topics_with_diversity(model, feature_names, num_top_words=25, num_display_words = 10):
        """–í—ã–≤–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã –∏ –∏—Ö —ç–Ω—Ç—Ä–æ–ø–∏—é.
    
        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            model: –û–±—É—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å (NMF –∏–ª–∏ LDA).
            feature_names (list): –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Å–ª–æ–≤) –∏–∑ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.
            num_top_words (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-—Å–ª–æ–≤, —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º—ã—Ö –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —ç–Ω—Ç—Ä–æ–ø–∏–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 25.
            num_display_words (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 10.
        """
        if not hasattr(model, 'components_'):
            print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –∏–º–µ–µ—Ç –∞—Ç—Ä–∏–±—É—Ç–∞ components_.")
            return
    
        topic_vectors = model.components_
        if isinstance(topic_vectors, cp.ndarray):
            topic_vectors_np = topic_vectors.get()
        else:
            topic_vectors_np = topic_vectors
    
        for topic_idx, topic in enumerate(topic_vectors_np):
            print(f"   –¢–µ–º–∞ #{topic_idx}. ", end=' ')
            top_word_indices = np.argsort(topic)[::-1][:num_top_words]
            top_words = [feature_names[i] for i in top_word_indices]
            print(f"–¢–æ–ø-{num_display_words} —Å–ª–æ–≤: {' '.join(top_words[:num_display_words])}")
            normalized_probabilities = topic[top_word_indices] / np.sum(topic[top_word_indices])
            topic_entropy = entropy(normalized_probabilities, base=2)
            print(f"   –≠–Ω—Ç—Ä–æ–ø–∏—è —Ç–µ–º—ã: {topic_entropy:.4f}")
        print()

–í—ã–≤–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã –∏ –∏—Ö —ç–Ω—Ç—Ä–æ–ø–∏—é.

–ê—Ä–≥—É–º–µ–Ω—Ç—ã: model: –û–±—É—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å (NMF –∏–ª–∏ LDA). feature_names
(list): –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Å–ª–æ–≤) –∏–∑ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.
num_top_words (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-—Å–ª–æ–≤, —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º—ã—Ö –¥–ª—è
—Ä–∞—Å—á–µ—Ç–∞ —ç–Ω—Ç—Ä–æ–ø–∏–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 25. num_display_words (int, optional):
–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 10.

` def reduce_dataset(df, percentage=0.1) `

     Expand source code
    
    
    def reduce_dataset(df, percentage=0.1):
        """–£–º–µ–Ω—å—à–∞–µ—Ç —Ä–∞–∑–º–µ—Ä DataFrame –¥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–æ–ª–∏, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø–æ —É–±—ã–≤–∞–Ω–∏—é 'estimated_owners'.
    
        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            df (pd.DataFrame): –ò—Å—Ö–æ–¥–Ω—ã–π DataFrame.
            percentage (float): –î–æ–ª—è DataFrame, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å (–∑–Ω–∞—á–µ–Ω–∏–µ –æ—Ç 0 –¥–æ 1).
    
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            pd.DataFrame: –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π DataFrame.
    
        –í—ã–∑—ã–≤–∞–µ—Ç ValueError, –µ—Å–ª–∏ percentage –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 1].
        """
        if not 0 <= percentage <= 1:
            raise ValueError("‚ùå –ü—Ä–æ—Ü–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç 0 –¥–æ 1")
    
        print(f"üìâ –£–º–µ–Ω—å—à–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–æ {percentage * 100}%...")
        df_sorted = df.sort_values(by='estimated_owners', ascending=False)
        num_rows = int(len(df_sorted) * percentage)
        reduced_df = df_sorted.head(num_rows)
        print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —É–º–µ–Ω—å—à–µ–Ω –¥–æ {len(reduced_df)} —Å—Ç—Ä–æ–∫.")
        return reduced_df

–£–º–µ–Ω—å—à–∞–µ—Ç —Ä–∞–∑–º–µ—Ä DataFrame –¥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–æ–ª–∏, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø–æ —É–±—ã–≤–∞–Ω–∏—é
'estimated_owners'.

–ê—Ä–≥—É–º–µ–Ω—Ç—ã: df (pd.DataFrame): –ò—Å—Ö–æ–¥–Ω—ã–π DataFrame. percentage (float): –î–æ–ª—è
DataFrame, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å (–∑–Ω–∞—á–µ–Ω–∏–µ –æ—Ç 0 –¥–æ 1).

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: pd.DataFrame: –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π DataFrame.

–í—ã–∑—ã–≤–∞–µ—Ç ValueError, –µ—Å–ª–∏ percentage –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 1].

` def vectorize_descriptions(df, nmf_params=None, lda_params=None,
vectorizer_cuml=None) `

     Expand source code
    
    
    def vectorize_descriptions(df, nmf_params=None, lda_params=None, vectorizer_cuml=None):
        """–í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –∏–≥—Ä, –∏—Å–ø–æ–ª—å–∑—É—è TF-IDF –∏ NMF –∏–ª–∏ LDA –¥–ª—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è.
    
        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            df (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Å—Ç–æ–ª–±–µ—Ü 'short_description_clean' —Å –æ—á–∏—â–µ–Ω–Ω—ã–º–∏ –æ–ø–∏—Å–∞–Ω–∏—è–º–∏.
            nmf_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è NMF. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è NMF. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.
            lda_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è LDA. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è LDA. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.
            vectorizer_cuml (CumlTfidfVectorizer, optional): –û–±—É—á–µ–Ω–Ω—ã–π CumlTfidfVectorizer. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.
    
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            tuple: –ö–æ—Ä—Ç–µ–∂, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π:
                - np.ndarray: –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è (—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤–µ–∫—Ç–æ—Ä—ã).
                - NMF –∏–ª–∏ LatentDirichletAllocation: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å NMF –∏–ª–∏ LDA.
    
        –í—ã–∑—ã–≤–∞–µ—Ç ValueError, –µ—Å–ª–∏ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω –æ–±—É—á–µ–Ω–Ω—ã–π CumlTfidfVectorizer –∏–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã nmf_params –∏–ª–∏ lda_params.
        """
        if vectorizer_cuml is None:
            raise ValueError("‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –æ–±—É—á–µ–Ω–Ω—ã–π CumlTfidfVectorizer.")
        desc_vectorized_cuml = vectorizer_cuml.transform(df['short_description_clean'])
        desc_vectorized_cuml_cpu = desc_vectorized_cuml.get()
        data = cp.asnumpy(desc_vectorized_cuml_cpu.data)
        indices = cp.asnumpy(desc_vectorized_cuml_cpu.indices)
        indptr = cp.asnumpy(desc_vectorized_cuml_cpu.indptr)
        shape = desc_vectorized_cuml_cpu.shape
        desc_vectorized_cpu = csr_matrix((data, indices, indptr), shape=shape)
    
        if nmf_params:
            nmf = NMF(**nmf_params)
            nmf_vectorized = nmf.fit_transform(desc_vectorized_cpu)
            return nmf_vectorized, nmf
        elif lda_params:
            lda = LatentDirichletAllocation(**lda_params)
            lda_vectorized = lda.fit_transform(desc_vectorized_cpu)
            return lda_vectorized, lda
        else:
            raise ValueError("‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å nmf_params –∏–ª–∏ lda_params")

–í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –∏–≥—Ä, –∏—Å–ø–æ–ª—å–∑—É—è TF-IDF –∏ NMF –∏–ª–∏ LDA –¥–ª—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ
–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è.

–ê—Ä–≥—É–º–µ–Ω—Ç—ã: df (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Å—Ç–æ–ª–±–µ—Ü
'short_description_clean' —Å –æ—á–∏—â–µ–Ω–Ω—ã–º–∏ –æ–ø–∏—Å–∞–Ω–∏—è–º–∏. nmf_params (dict,
optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è NMF. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è NMF. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
None. lda_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è LDA. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã,
–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è LDA. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None. vectorizer_cuml (CumlTfidfVectorizer,
optional): –û–±—É—á–µ–Ω–Ω—ã–π CumlTfidfVectorizer. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: tuple: –ö–æ—Ä—Ç–µ–∂, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π: \- np.ndarray: –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è
(—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤–µ–∫—Ç–æ—Ä—ã). \- NMF –∏–ª–∏ LatentDirichletAllocation: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
NMF –∏–ª–∏ LDA.

–í—ã–∑—ã–≤–∞–µ—Ç ValueError, –µ—Å–ª–∏ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω –æ–±—É—á–µ–Ω–Ω—ã–π CumlTfidfVectorizer –∏–ª–∏ –Ω–µ
—É–∫–∞–∑–∞–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã nmf_params –∏–ª–∏ lda_params.

` def vectorize_owners(df, method='log_scale', scaler=None) `

     Expand source code
    
    
    def vectorize_owners(df, method='log_scale', scaler=None):
        """–í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö –∏–≥—Ä, –∏—Å–ø–æ–ª—å–∑—É—è –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ.
    
        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            df (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Å—Ç–æ–ª–±–µ—Ü 'estimated_owners'.
            method (str, optional): –ú–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: 'log_scale' –∏–ª–∏ 'standard'. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 'log_scale'.
            scaler (CumlMinMaxScaler, optional): –û–±—É—á–µ–Ω–Ω—ã–π scaler –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.
    
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            np.ndarray: –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö –∏–≥—Ä.
    
        –í—ã–∑—ã–≤–∞–µ—Ç ValueError, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π –º–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤.
        """
        owners = df['estimated_owners'].values.reshape(-1, 1)
        owners = np.array(owners, dtype=float)
        owners = np.nan_to_num(owners, nan=0)
        if method == 'log_scale':
            owners = np.log1p(owners)
            if scaler is not None:
               owners = scaler.transform(owners)
            owners_weighted = owners * (1 + (owners * 2))
            return owners_weighted
        elif method == 'standard':
            if scaler is not None:
               owners = scaler.transform(owners)
            return owners
        else:
            raise ValueError("‚ùå –ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π –º–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤.")

–í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö –∏–≥—Ä, –∏—Å–ø–æ–ª—å–∑—É—è –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
–∏–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ.

–ê—Ä–≥—É–º–µ–Ω—Ç—ã: df (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Å—Ç–æ–ª–±–µ—Ü
'estimated_owners'. method (str, optional): –ú–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: 'log_scale'
–∏–ª–∏ 'standard'. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 'log_scale'. scaler (CumlMinMaxScaler, optional):
–û–±—É—á–µ–Ω–Ω—ã–π scaler –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: np.ndarray: –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö –∏–≥—Ä.

–í—ã–∑—ã–≤–∞–µ—Ç ValueError, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π –º–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤.

` def vectorize_tags(df, multilabel_params=None) `

     Expand source code
    
    
    def vectorize_tags(df, multilabel_params=None):
        """–í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç —Ç–µ–≥–∏ –∏–≥—Ä, –∏—Å–ø–æ–ª—å–∑—É—è MultiLabelBinarizer –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ –±–∏–Ω–∞—Ä–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã.
    
        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            df (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Å—Ç–æ–ª–±–µ—Ü 'all_tags' —Å–æ —Å–ø–∏—Å–∫–∞–º–∏ —Ç–µ–≥–æ–≤.
            multilabel_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è MultiLabelBinarizer. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.
    
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            tuple: –ö–æ—Ä—Ç–µ–∂, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π:
                - np.ndarray: –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–≥–∏.
                - MultiLabelBinarizer: –û–±—É—á–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç MultiLabelBinarizer.
        """
        default_params = {'sparse_output': False}
        params = multilabel_params if multilabel_params else default_params
        mlb = MultiLabelBinarizer(**params)
        mlb.fit(df['all_tags'])
        tags_vectorized = mlb.transform(df['all_tags'])
        return tags_vectorized, mlb

–í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç —Ç–µ–≥–∏ –∏–≥—Ä, –∏—Å–ø–æ–ª—å–∑—É—è MultiLabelBinarizer –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤
–±–∏–Ω–∞—Ä–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã.

–ê—Ä–≥—É–º–µ–Ω—Ç—ã: df (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Å—Ç–æ–ª–±–µ—Ü 'all_tags' —Å–æ
—Å–ø–∏—Å–∫–∞–º–∏ —Ç–µ–≥–æ–≤. multilabel_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è
MultiLabelBinarizer. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: tuple: –ö–æ—Ä—Ç–µ–∂, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π: \- np.ndarray: –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–≥–∏. \-
MultiLabelBinarizer: –û–±—É—á–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç MultiLabelBinarizer.

## Classes

` class CombinedVectorizer (owners_method='log_scale',  
multilabel_params=None,  
nmf_params=None,  
lda_params=None,  
tag_weight=1.0,  
tfidf_cuml_params=None) `

     Expand source code
    
    
    class CombinedVectorizer(BaseEstimator, TransformerMixin):
        """–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    
        –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö, —Ç–µ–≥–∏ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –∏–≥—Ä, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä—ã.
    
        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            owners_method (str, optional): –ú–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö ('log_scale' –∏–ª–∏ 'standard'). –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 'log_scale'.
            multilabel_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è MultiLabelBinarizer. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.
            nmf_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è NMF. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è NMF –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏–π. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.
            lda_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è LDA. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è LDA –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏–π. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.
            tag_weight (float, optional): –í–µ—Å, –ø—Ä–∏–º–µ–Ω—è–µ–º—ã–π –∫ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–º —Ç–µ–≥–∞–º. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 1.0.
            tfidf_cuml_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è CumlTfidfVectorizer. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.
        """
        def __init__(self, owners_method='log_scale', multilabel_params=None, nmf_params=None, lda_params=None, tag_weight=1.0, tfidf_cuml_params=None):
            self.owners_method = owners_method
            self.multilabel_params = multilabel_params
            self.nmf_params = nmf_params
            self.lda_params = lda_params
            self.tag_weight = tag_weight
            self.tfidf_cuml_params = tfidf_cuml_params if tfidf_cuml_params else {}
            self.mlb = None
            self.nmf = None
            self.lda = None
            self.tfidf_feature_names_out_ = None
            self.scaler = CumlMinMaxScaler()
            self.transformed_owners_vectors = None
            self.transformed_tags_vectors = None
            self.transformed_desc_vectors = None
            self.transformed_combined_vectors = None
    
        def fit(self, X, y=None):
            """–û–±—É—á–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    
            –í—ã–ø–æ–ª–Ω—è–µ—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤, —Ç–µ–≥–æ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–π, –∞ —Ç–∞–∫–∂–µ –æ–±—É—á–∞–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä—ã –∏ —Å–∫–∞–ª–µ—Ä—ã.
    
            –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
                X (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ ('estimated_owners', 'all_tags', 'short_description_clean').
                y (None): –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω—É–∂–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ API scikit-learn.
    
            –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
                CombinedVectorizer: –û–±—É—á–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä.
            """
            self.owners_vectors = vectorize_owners(X, method=self.owners_method)
            self.tags_vectors, self.mlb = vectorize_tags(X, multilabel_params=self.multilabel_params)
            if isinstance(self.tags_vectors, cp.sparse.csr_matrix):
                print("‚ÑπÔ∏è –í–µ–∫—Ç–æ—Ä—ã —Ç–µ–≥–æ–≤ - cupy sparse matrix, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ numpy...")
                self.tags_vectors = np.array(cp.asnumpy(self.tags_vectors.todense()), dtype=np.float64)
            if self.tags_vectors.ndim == 1:
                self.tags_vectors = self.tags_vectors.reshape(-1, 1)
    
            cleaned_descriptions = X['short_description_clean'].str.lower()
            self.tfidf_cuml.fit(cleaned_descriptions)
            self.tfidf_feature_names_out_ = [word for word, index in sorted(self.tfidf_cuml.vocabulary_.to_pandas().items(), key=lambda item: item[1])]
    
            if self.nmf_params and self.lda_params is None:
                self.desc_vectors, self.nmf = vectorize_descriptions(X, nmf_params=self.nmf_params, vectorizer_cuml=self.tfidf_cuml)
                self.lda = None
            elif self.lda_params and self.nmf_params is None:
                 self.desc_vectors, self.lda = vectorize_descriptions(X, lda_params=self.lda_params, vectorizer_cuml=self.tfidf_cuml)
                 self.nmf = None
            else:
                raise ValueError("‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å nmf_params –∏–ª–∏ lda_params")
            print("‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    
            if self.nmf and hasattr(self.nmf, 'components_'):
                 if np.isnan(self.nmf.components_).any():
                    print("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN –∑–Ω–∞—á–µ–Ω–∏—è –≤ self.nmf.components_ –≤ fit()!")
            if self.lda and hasattr(self.lda, 'components_'):
                 if np.isnan(self.lda.components_).any():
                     print("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN –∑–Ω–∞—á–µ–Ω–∏—è –≤ self.lda.components_ –≤ fit()!")
    
            owners_vectors = vectorize_owners(X, method=self.owners_method)
            self.scaler.fit(owners_vectors)
            return self
    
        def transform(self, X, y=None):
            """–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    
            –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä—ã –∏ —Å–∫–∞–ª–µ—Ä—ã –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö, —Ç–µ–≥–æ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–π –≤ –µ–¥–∏–Ω–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ.
    
            –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
                X (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏.
                y (None): –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω—É–∂–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ API scikit-learn.
    
            –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
                np.ndarray: –ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
            """
            owners_vectors = vectorize_owners(X, method=self.owners_method, scaler=self.scaler)
            owners_vectors = owners_vectors.reshape(owners_vectors.shape[0], -1)
            tags_vectors = self.mlb.transform(X['all_tags'])
    
            tag_weight = self.tag_weight
            tags_vectors_weighted = tags_vectors * tag_weight
            tags_vectors = tags_vectors_weighted
    
            tfidf_transformed_cuml = self.tfidf_cuml.transform(X['short_description_clean'])
            tfidf_transformed_cpu = tfidf_transformed_cuml.get()
            data = cp.asnumpy(tfidf_transformed_cpu.data)
            indices = cp.asnumpy(tfidf_transformed_cpu.indices)
            indptr = cp.asnumpy(tfidf_transformed_cpu.indptr)
            shape = tfidf_transformed_cpu.shape
            tfidf_transformed = csr_matrix((data, indices, indptr), shape=shape)
    
            desc_vectors = None
            if self.nmf_params:
                desc_vectors = self.nmf.transform(tfidf_transformed)
            elif self.lda_params:
                desc_vectors = self.lda.transform(tfidf_transformed)
    
            if desc_vectors is not None and desc_vectors.shape[0] != owners_vectors.shape[0]:
                raise ValueError(f"‚ùå –ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—Ä–∞–∑—Ü–æ–≤ –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–π: {owners_vectors.shape[0]} vs {desc_vectors.shape[0]}")
    
            self.transformed_owners_vectors = owners_vectors
            self.transformed_tags_vectors = tags_vectors
            self.transformed_desc_vectors = desc_vectors
    
            combined_vectors = np.hstack([owners_vectors, tags_vectors.toarray() if hasattr(tags_vectors, 'toarray') else tags_vectors, desc_vectors])
            self.transformed_combined_vectors = combined_vectors
    
            return combined_vectors
    
        def get_params(self, deep=True):
            """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.
    
            –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–∞–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞, –≤–∫–ª—é—á–∞—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤—Å–µ—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–æ–≤ –∏ –º–µ—Ç–æ–¥–æ–≤.
    
            –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
                deep (bool, optional): –ï—Å–ª–∏ True, —Ç–∞–∫–∂–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –æ—Ü–µ–Ω—â–∏–∫–∞–º–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é True.
    
            –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
                dict: –°–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.
            """
            return {
                'owners_method': self.owners_method,
                'multilabel_params': self.multilabel_params,
                 'nmf_params': self.nmf_params,
                'lda_params': self.lda_params,
                'tag_weight': self.tag_weight,
                'tfidf_cuml_params': self.tfidf_cuml_params
            }
    
        def set_params(self, **params):
            """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.
    
            –ü–æ–∑–≤–æ–ª—è–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏.
    
            –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
                **params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ –≤ –≤–∏–¥–µ keyword arguments.
    
            –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
                CombinedVectorizer: –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
            """
            if 'owners_method' in params:
                self.owners_method = params['owners_method']
            if 'multilabel_params' in params:
                self.multilabel_params = params['multilabel_params']
            if 'nmf_params' in params:
                self.nmf_params = params['nmf_params']
            if 'lda_params' in params:
                 self.lda_params = params['lda_params']
            if 'tag_weight' in params:
                self.tag_weight = params['tag_weight']
            if 'tfidf_cuml_params' in params:
                self.tfidf_cuml_params = params['tfidf_cuml_params']
                self.tfidf_cuml.set_params(**params['tfidf_cuml_params'])
            return self

–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

–í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö, —Ç–µ–≥–∏ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –∏–≥—Ä, –∏—Å–ø–æ–ª—å–∑—É—è
—Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä—ã.

–ê—Ä–≥—É–º–µ–Ω—Ç—ã: owners_method (str, optional): –ú–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –æ
–≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö ('log_scale' –∏–ª–∏ 'standard'). –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 'log_scale'.
multilabel_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è MultiLabelBinarizer. –ü–æ
—É–º–æ–ª—á–∞–Ω–∏—é None. nmf_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è NMF. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã,
–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è NMF –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏–π. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None. lda_params
(dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è LDA. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è LDA –¥–ª—è
–≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏–π. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None. tag_weight (float, optional): –í–µ—Å,
–ø—Ä–∏–º–µ–Ω—è–µ–º—ã–π –∫ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–º —Ç–µ–≥–∞–º. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 1.0. tfidf_cuml_params
(dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è CumlTfidfVectorizer. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.

### Ancestors

  * sklearn.base.BaseEstimator
  * sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin
  * sklearn.utils._metadata_requests._MetadataRequester
  * sklearn.base.TransformerMixin
  * sklearn.utils._set_output._SetOutputMixin

### Methods

` def fit(self, X, y=None) `

     Expand source code
    
    
    def fit(self, X, y=None):
        """–û–±—É—á–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤, —Ç–µ–≥–æ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–π, –∞ —Ç–∞–∫–∂–µ –æ–±—É—á–∞–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä—ã –∏ —Å–∫–∞–ª–µ—Ä—ã.
    
        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            X (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ ('estimated_owners', 'all_tags', 'short_description_clean').
            y (None): –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω—É–∂–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ API scikit-learn.
    
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            CombinedVectorizer: –û–±—É—á–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä.
        """
        self.owners_vectors = vectorize_owners(X, method=self.owners_method)
        self.tags_vectors, self.mlb = vectorize_tags(X, multilabel_params=self.multilabel_params)
        if isinstance(self.tags_vectors, cp.sparse.csr_matrix):
            print("‚ÑπÔ∏è –í–µ–∫—Ç–æ—Ä—ã —Ç–µ–≥–æ–≤ - cupy sparse matrix, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ numpy...")
            self.tags_vectors = np.array(cp.asnumpy(self.tags_vectors.todense()), dtype=np.float64)
        if self.tags_vectors.ndim == 1:
            self.tags_vectors = self.tags_vectors.reshape(-1, 1)
    
        cleaned_descriptions = X['short_description_clean'].str.lower()
        self.tfidf_cuml.fit(cleaned_descriptions)
        self.tfidf_feature_names_out_ = [word for word, index in sorted(self.tfidf_cuml.vocabulary_.to_pandas().items(), key=lambda item: item[1])]
    
        if self.nmf_params and self.lda_params is None:
            self.desc_vectors, self.nmf = vectorize_descriptions(X, nmf_params=self.nmf_params, vectorizer_cuml=self.tfidf_cuml)
            self.lda = None
        elif self.lda_params and self.nmf_params is None:
             self.desc_vectors, self.lda = vectorize_descriptions(X, lda_params=self.lda_params, vectorizer_cuml=self.tfidf_cuml)
             self.nmf = None
        else:
            raise ValueError("‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å nmf_params –∏–ª–∏ lda_params")
        print("‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    
        if self.nmf and hasattr(self.nmf, 'components_'):
             if np.isnan(self.nmf.components_).any():
                print("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN –∑–Ω–∞—á–µ–Ω–∏—è –≤ self.nmf.components_ –≤ fit()!")
        if self.lda and hasattr(self.lda, 'components_'):
             if np.isnan(self.lda.components_).any():
                 print("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN –∑–Ω–∞—á–µ–Ω–∏—è –≤ self.lda.components_ –≤ fit()!")
    
        owners_vectors = vectorize_owners(X, method=self.owners_method)
        self.scaler.fit(owners_vectors)
        return self

–û–±—É—á–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

–í—ã–ø–æ–ª–Ω—è–µ—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤, —Ç–µ–≥–æ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–π, –∞ —Ç–∞–∫–∂–µ –æ–±—É—á–∞–µ—Ç
–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä—ã –∏ —Å–∫–∞–ª–µ—Ä—ã.

–ê—Ä–≥—É–º–µ–Ω—Ç—ã: X (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
('estimated_owners', 'all_tags', 'short_description_clean'). y (None): –ù–µ
–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω—É–∂–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ API scikit-learn.

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: CombinedVectorizer: –û–±—É—á–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä.

` def get_params(self, deep=True) `

     Expand source code
    
    
    def get_params(self, deep=True):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.
    
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–∞–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞, –≤–∫–ª—é—á–∞—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤—Å–µ—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–æ–≤ –∏ –º–µ—Ç–æ–¥–æ–≤.
    
        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            deep (bool, optional): –ï—Å–ª–∏ True, —Ç–∞–∫–∂–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –æ—Ü–µ–Ω—â–∏–∫–∞–º–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é True.
    
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            dict: –°–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.
        """
        return {
            'owners_method': self.owners_method,
            'multilabel_params': self.multilabel_params,
             'nmf_params': self.nmf_params,
            'lda_params': self.lda_params,
            'tag_weight': self.tag_weight,
            'tfidf_cuml_params': self.tfidf_cuml_params
        }

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–∞–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞, –≤–∫–ª—é—á–∞—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è
–≤—Å–µ—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–æ–≤ –∏ –º–µ—Ç–æ–¥–æ–≤.

–ê—Ä–≥—É–º–µ–Ω—Ç—ã: deep (bool, optional): –ï—Å–ª–∏ True, —Ç–∞–∫–∂–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è
–≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –æ—Ü–µ–Ω—â–∏–∫–∞–º–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é True.

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: dict: –°–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.

` def set_params(self, **params) `

     Expand source code
    
    
    def set_params(self, **params):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.
    
        –ü–æ–∑–≤–æ–ª—è–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏.
    
        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            **params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ –≤ –≤–∏–¥–µ keyword arguments.
    
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            CombinedVectorizer: –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
        """
        if 'owners_method' in params:
            self.owners_method = params['owners_method']
        if 'multilabel_params' in params:
            self.multilabel_params = params['multilabel_params']
        if 'nmf_params' in params:
            self.nmf_params = params['nmf_params']
        if 'lda_params' in params:
             self.lda_params = params['lda_params']
        if 'tag_weight' in params:
            self.tag_weight = params['tag_weight']
        if 'tfidf_cuml_params' in params:
            self.tfidf_cuml_params = params['tfidf_cuml_params']
            self.tfidf_cuml.set_params(**params['tfidf_cuml_params'])
        return self

–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.

–ü–æ–∑–≤–æ–ª—è–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏.

–ê—Ä–≥—É–º–µ–Ω—Ç—ã: **params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ –≤ –≤–∏–¥–µ keyword arguments.

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: CombinedVectorizer: –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.

` def transform(self, X, y=None) `

     Expand source code
    
    
    def transform(self, X, y=None):
        """–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä—ã –∏ —Å–∫–∞–ª–µ—Ä—ã –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö, —Ç–µ–≥–æ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–π –≤ –µ–¥–∏–Ω–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ.
    
        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            X (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏.
            y (None): –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω—É–∂–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ API scikit-learn.
    
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            np.ndarray: –ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
        """
        owners_vectors = vectorize_owners(X, method=self.owners_method, scaler=self.scaler)
        owners_vectors = owners_vectors.reshape(owners_vectors.shape[0], -1)
        tags_vectors = self.mlb.transform(X['all_tags'])
    
        tag_weight = self.tag_weight
        tags_vectors_weighted = tags_vectors * tag_weight
        tags_vectors = tags_vectors_weighted
    
        tfidf_transformed_cuml = self.tfidf_cuml.transform(X['short_description_clean'])
        tfidf_transformed_cpu = tfidf_transformed_cuml.get()
        data = cp.asnumpy(tfidf_transformed_cpu.data)
        indices = cp.asnumpy(tfidf_transformed_cpu.indices)
        indptr = cp.asnumpy(tfidf_transformed_cpu.indptr)
        shape = tfidf_transformed_cpu.shape
        tfidf_transformed = csr_matrix((data, indices, indptr), shape=shape)
    
        desc_vectors = None
        if self.nmf_params:
            desc_vectors = self.nmf.transform(tfidf_transformed)
        elif self.lda_params:
            desc_vectors = self.lda.transform(tfidf_transformed)
    
        if desc_vectors is not None and desc_vectors.shape[0] != owners_vectors.shape[0]:
            raise ValueError(f"‚ùå –ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—Ä–∞–∑—Ü–æ–≤ –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–π: {owners_vectors.shape[0]} vs {desc_vectors.shape[0]}")
    
        self.transformed_owners_vectors = owners_vectors
        self.transformed_tags_vectors = tags_vectors
        self.transformed_desc_vectors = desc_vectors
    
        combined_vectors = np.hstack([owners_vectors, tags_vectors.toarray() if hasattr(tags_vectors, 'toarray') else tags_vectors, desc_vectors])
        self.transformed_combined_vectors = combined_vectors
    
        return combined_vectors

–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä—ã –∏ —Å–∫–∞–ª–µ—Ä—ã –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ
–≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö, —Ç–µ–≥–æ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–π –≤ –µ–¥–∏–Ω–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ.

–ê—Ä–≥—É–º–µ–Ω—Ç—ã: X (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏. y
(None): –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω—É–∂–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ API scikit-learn.

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: np.ndarray: –ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

` class TorchLDA (n_topics, n_vocab, device, alpha=0.1, beta=0.01,
max_iterations=100, tolerance=0.0001) `

     Expand source code
    
    
    class TorchLDA(nn.Module):
        """–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ LDA (Latent Dirichlet Allocation) –Ω–∞ PyTorch.
    
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è LDA, –ø–æ–∑–≤–æ–ª—è—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.
    
        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            n_topics (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è.
            n_vocab (int): –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤).
            device (torch.device): –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è –æ–±—É—á–µ–Ω–∏–µ (CPU –∏–ª–∏ GPU).
            alpha (float, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä –∞–ø—Ä–∏–æ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –î–∏—Ä–∏—Ö–ª–µ –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.1.
            beta (float, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä –∞–ø—Ä–∏–æ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –î–∏—Ä–∏—Ö–ª–µ –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º –ø–æ —Å–ª–æ–≤–∞–º. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.01.
            max_iterations (int, optional): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 100.
            tolerance (float, optional): –ü–æ—Ä–æ–≥ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–ª—è EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 1e-4.
        """
        def __init__(self, n_topics, n_vocab, device, alpha=0.1, beta=0.01, max_iterations=100, tolerance=1e-4):
            super().__init__()
            self.n_topics = n_topics
            self.n_vocab = n_vocab
            self.device = device
            self.alpha = alpha
            self.beta = beta
            self.max_iterations = max_iterations
            self.tolerance = tolerance
            self.topic_term_matrix = nn.Parameter(torch.randn(n_topics, n_vocab, device=device).abs())
            self.doc_topic_matrix = None
            self.norm_topic_term_matrix = None
    
        def initialize_parameters(self, docs):
            """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ LDA —Å–ª—É—á–∞–π–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏.
    
            –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
                docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏, –Ω–æ –æ–∂–∏–¥–∞–µ—Ç—Å—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞).
            """
            self.doc_topic_matrix = torch.rand(docs.shape[0], self.n_topics, device=self.device).abs()
            self.topic_term_matrix.data = torch.randn(self.n_topics, self.n_vocab, device=self.device).abs()
    
        def fit(self, docs, log=False):
            """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å LDA –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è EM-–∞–ª–≥–æ—Ä–∏—Ç–º.
    
            –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
                docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ x —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è).
                log (bool, optional): –í–∫–ª—é—á–∞–µ—Ç –≤—ã–≤–æ–¥ –ª–æ–≥–æ–≤ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é False.
    
            –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
                TorchLDA: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å LDA.
            """
            if log: print("LDA Fit started")
            self.initialize_parameters(docs)
            docs = docs.to(self.device)
            prev_likelihood = float('-inf')
            for iteration in range(self.max_iterations):
                doc_topic_distribution = self.expect(docs)
                self.topic_term_matrix = self.maximize(docs, doc_topic_distribution)
                current_likelihood = self.likelihood(docs, doc_topic_distribution)
                if log: print(f"Iteration {iteration+1}, Likelihood {current_likelihood:.2f}")
                if abs(current_likelihood - prev_likelihood) < self.tolerance:
                    if log: print("LDA Converged")
                    break
                prev_likelihood = current_likelihood
            self.norm_topic_term_matrix = self.normalize(self.topic_term_matrix)
            if log: print("LDA Fit ended")
            return self
    
        def expect(self, docs):
            """–í—ã–ø–æ–ª–Ω—è–µ—Ç E-—à–∞–≥ EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞ –¥–ª—è LDA: –æ—Ü–µ–Ω–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.
    
            –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
                docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    
            –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
                torch.Tensor: –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.
            """
            doc_topic_distribution = torch.matmul(docs, self.topic_term_matrix.T) + self.alpha
            doc_topic_distribution = self.normalize(doc_topic_distribution)
            return doc_topic_distribution
    
        def maximize(self, docs, doc_topic_distribution):
            """–í—ã–ø–æ–ª–Ω—è–µ—Ç M-—à–∞–≥ EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞ –¥–ª—è LDA: –æ—Ü–µ–Ω–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º –ø–æ —Å–ª–æ–≤–∞–º.
    
            –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
                docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
                doc_topic_distribution (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.
    
            –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
                torch.Tensor: –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º –ø–æ —Å–ª–æ–≤–∞–º.
            """
            topic_term_matrix = torch.matmul(doc_topic_distribution.T, docs) + self.beta
            return topic_term_matrix
    
        def likelihood(self, docs, doc_topic_distribution):
            """–í—ã—á–∏—Å–ª—è–µ—Ç –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞.
    
            –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
                docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
                doc_topic_distribution (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.
    
            –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
                float: –ó–Ω–∞—á–µ–Ω–∏–µ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è.
            """
            log_likelihood = torch.sum(docs * torch.log(torch.matmul(doc_topic_distribution, self.normalize(self.topic_term_matrix))))
            return log_likelihood.item()
    
        def normalize(self, matrix):
            """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –º–∞—Ç—Ä–∏—Ü—É, –ø—Ä–∏–≤–æ–¥—è —Å—É–º–º—ã —Å—Ç—Ä–æ–∫ –∫ –µ–¥–∏–Ω–∏—Ü–µ.
    
            –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
                matrix (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.
    
            –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
                torch.Tensor: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞.
            """
            row_sums = matrix.sum(axis=1, keepdim=True)
            return matrix / row_sums
    
        def transform(self, docs):
            """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ç–µ–º.
    
            –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
                docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    
            –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
                torch.Tensor: –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º –¥–ª—è –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    
            –í—ã–∑—ã–≤–∞–µ—Ç ValueError, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å LDA –µ—â–µ –Ω–µ –æ–±—É—á–µ–Ω–∞.
            """
            if self.norm_topic_term_matrix is None:
                raise ValueError("‚ùå LDA model has not been fitted yet.")
            docs = docs.to(self.device)
            doc_topic_distribution = torch.matmul(docs, self.norm_topic_term_matrix.T) + self.alpha
            return self.normalize(doc_topic_distribution)

–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ LDA (Latent Dirichlet Allocation) –Ω–∞ PyTorch.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è LDA, –ø–æ–∑–≤–æ–ª—è—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU –¥–ª—è
—É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.

–ê—Ä–≥—É–º–µ–Ω—Ç—ã: n_topics (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è. n_vocab (int):
–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤). device (torch.device):
–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è –æ–±—É—á–µ–Ω–∏–µ (CPU –∏–ª–∏ GPU). alpha (float,
optional): –ü–∞—Ä–∞–º–µ—Ç—Ä –∞–ø—Ä–∏–æ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –î–∏—Ä–∏—Ö–ª–µ –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.1. beta (float, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä
–∞–ø—Ä–∏–æ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –î–∏—Ä–∏—Ö–ª–µ –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º –ø–æ —Å–ª–æ–≤–∞–º. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
0.01. max_iterations (int, optional): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π EM-
–∞–ª–≥–æ—Ä–∏—Ç–º–∞. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 100. tolerance (float, optional): –ü–æ—Ä–æ–≥ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–ª—è
EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 1e-4.

Initialize internal Module state, shared by both nn.Module and ScriptModule.

### Ancestors

  * torch.nn.modules.module.Module

### Methods

` def expect(self, docs) `

     Expand source code
    
    
    def expect(self, docs):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç E-—à–∞–≥ EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞ –¥–ª—è LDA: –æ—Ü–µ–Ω–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.
    
        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            torch.Tensor: –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.
        """
        doc_topic_distribution = torch.matmul(docs, self.topic_term_matrix.T) + self.alpha
        doc_topic_distribution = self.normalize(doc_topic_distribution)
        return doc_topic_distribution

–í—ã–ø–æ–ª–Ω—è–µ—Ç E-—à–∞–≥ EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞ –¥–ª—è LDA: –æ—Ü–µ–Ω–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ
—Ç–µ–º–∞–º.

–ê—Ä–≥—É–º–µ–Ω—Ç—ã: docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: torch.Tensor: –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.

` def fit(self, docs, log=False) `

     Expand source code
    
    
    def fit(self, docs, log=False):
        """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å LDA –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è EM-–∞–ª–≥–æ—Ä–∏—Ç–º.
    
        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ x —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è).
            log (bool, optional): –í–∫–ª—é—á–∞–µ—Ç –≤—ã–≤–æ–¥ –ª–æ–≥–æ–≤ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é False.
    
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            TorchLDA: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å LDA.
        """
        if log: print("LDA Fit started")
        self.initialize_parameters(docs)
        docs = docs.to(self.device)
        prev_likelihood = float('-inf')
        for iteration in range(self.max_iterations):
            doc_topic_distribution = self.expect(docs)
            self.topic_term_matrix = self.maximize(docs, doc_topic_distribution)
            current_likelihood = self.likelihood(docs, doc_topic_distribution)
            if log: print(f"Iteration {iteration+1}, Likelihood {current_likelihood:.2f}")
            if abs(current_likelihood - prev_likelihood) < self.tolerance:
                if log: print("LDA Converged")
                break
            prev_likelihood = current_likelihood
        self.norm_topic_term_matrix = self.normalize(self.topic_term_matrix)
        if log: print("LDA Fit ended")
        return self

–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å LDA –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è EM-
–∞–ª–≥–æ—Ä–∏—Ç–º.

–ê—Ä–≥—É–º–µ–Ω—Ç—ã: docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ x —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è). log (bool, optional): –í–∫–ª—é—á–∞–µ—Ç –≤—ã–≤–æ–¥ –ª–æ–≥–æ–≤ –≤–æ
–≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é False.

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: TorchLDA: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å LDA.

` def initialize_parameters(self, docs) `

     Expand source code
    
    
    def initialize_parameters(self, docs):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ LDA —Å–ª—É—á–∞–π–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏.
    
        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏, –Ω–æ –æ–∂–∏–¥–∞–µ—Ç—Å—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞).
        """
        self.doc_topic_matrix = torch.rand(docs.shape[0], self.n_topics, device=self.device).abs()
        self.topic_term_matrix.data = torch.randn(self.n_topics, self.n_vocab, device=self.device).abs()

–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ LDA —Å–ª—É—á–∞–π–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏.

–ê—Ä–≥—É–º–µ–Ω—Ç—ã: docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤
–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏, –Ω–æ –æ–∂–∏–¥–∞–µ—Ç—Å—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞).

` def likelihood(self, docs, doc_topic_distribution) `

     Expand source code
    
    
    def likelihood(self, docs, doc_topic_distribution):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞.
    
        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
            doc_topic_distribution (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.
    
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            float: –ó–Ω–∞—á–µ–Ω–∏–µ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è.
        """
        log_likelihood = torch.sum(docs * torch.log(torch.matmul(doc_topic_distribution, self.normalize(self.topic_term_matrix))))
        return log_likelihood.item()

–í—ã—á–∏—Å–ª—è–µ—Ç –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞.

–ê—Ä–≥—É–º–µ–Ω—Ç—ã: docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. doc_topic_distribution
(torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: float: –ó–Ω–∞—á–µ–Ω–∏–µ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è.

` def maximize(self, docs, doc_topic_distribution) `

     Expand source code
    
    
    def maximize(self, docs, doc_topic_distribution):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç M-—à–∞–≥ EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞ –¥–ª—è LDA: –æ—Ü–µ–Ω–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º –ø–æ —Å–ª–æ–≤–∞–º.
    
        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
            doc_topic_distribution (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.
    
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            torch.Tensor: –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º –ø–æ —Å–ª–æ–≤–∞–º.
        """
        topic_term_matrix = torch.matmul(doc_topic_distribution.T, docs) + self.beta
        return topic_term_matrix

–í—ã–ø–æ–ª–Ω—è–µ—Ç M-—à–∞–≥ EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞ –¥–ª—è LDA: –æ—Ü–µ–Ω–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º –ø–æ —Å–ª–æ–≤–∞–º.

–ê—Ä–≥—É–º–µ–Ω—Ç—ã: docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. doc_topic_distribution
(torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: torch.Tensor: –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º –ø–æ —Å–ª–æ–≤–∞–º.

` def normalize(self, matrix) `

     Expand source code
    
    
    def normalize(self, matrix):
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –º–∞—Ç—Ä–∏—Ü—É, –ø—Ä–∏–≤–æ–¥—è —Å—É–º–º—ã —Å—Ç—Ä–æ–∫ –∫ –µ–¥–∏–Ω–∏—Ü–µ.
    
        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            matrix (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.
    
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            torch.Tensor: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞.
        """
        row_sums = matrix.sum(axis=1, keepdim=True)
        return matrix / row_sums

–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –º–∞—Ç—Ä–∏—Ü—É, –ø—Ä–∏–≤–æ–¥—è —Å—É–º–º—ã —Å—Ç—Ä–æ–∫ –∫ –µ–¥–∏–Ω–∏—Ü–µ.

–ê—Ä–≥—É–º–µ–Ω—Ç—ã: matrix (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: torch.Tensor: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞.

` def transform(self, docs) `

     Expand source code
    
    
    def transform(self, docs):
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ç–µ–º.
    
        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            torch.Tensor: –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º –¥–ª—è –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    
        –í—ã–∑—ã–≤–∞–µ—Ç ValueError, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å LDA –µ—â–µ –Ω–µ –æ–±—É—á–µ–Ω–∞.
        """
        if self.norm_topic_term_matrix is None:
            raise ValueError("‚ùå LDA model has not been fitted yet.")
        docs = docs.to(self.device)
        doc_topic_distribution = torch.matmul(docs, self.norm_topic_term_matrix.T) + self.alpha
        return self.normalize(doc_topic_distribution)

–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ç–µ–º.

–ê—Ä–≥—É–º–µ–Ω—Ç—ã: docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: torch.Tensor: –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º –¥–ª—è –Ω–æ–≤—ã—Ö
–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

–í—ã–∑—ã–≤–∞–µ—Ç ValueError, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å LDA –µ—â–µ –Ω–µ –æ–±—É—á–µ–Ω–∞.

  * ### Super-module

    * `[src](index.html "src")`
  * ### Functions

    * `calculate_intra_topic_diversity`
    * `calculate_topic_coherence`
    * `calculate_topic_diversity`
    * `clean_text`
    * `display_topics`
    * `display_topics_with_diversity`
    * `reduce_dataset`
    * `vectorize_descriptions`
    * `vectorize_owners`
    * `vectorize_tags`
  * ### Classes

    * #### `CombinedVectorizer`

      * `fit`
      * `get_params`
      * `set_params`
      * `transform`
    * #### `TorchLDA`

      * `expect`
      * `fit`
      * `initialize_parameters`
      * `likelihood`
      * `maximize`
      * `normalize`
      * `transform`

Generated by [pdoc 0.11.5](https://pdoc3.github.io/pdoc "pdoc: Python API
documentation generator").

