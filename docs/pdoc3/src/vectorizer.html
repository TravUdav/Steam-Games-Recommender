<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.5">
<title>src.vectorizer API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.vectorizer</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.vectorizer.calculate_intra_topic_diversity"><code class="name flex">
<span>def <span class="ident">calculate_intra_topic_diversity</span></span>(<span>model, feature_names, num_top_words=10)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_intra_topic_diversity(model, feature_names, num_top_words=10):
    &#34;&#34;&#34;–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Å–ª–æ–≤ –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–π —Ç–µ–º—ã, –∏—Å–ø–æ–ª—å–∑—É—è —ç–Ω—Ç—Ä–æ–ø–∏—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ª–æ–≤.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        model: –û–±—É—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å (NMF –∏–ª–∏ LDA).
        feature_names (list): –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Å–ª–æ–≤) –∏–∑ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.
        num_top_words (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-—Å–ª–æ–≤, —É—á–∏—Ç—ã–≤–∞–µ–º—ã—Ö –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —ç–Ω—Ç—Ä–æ–ø–∏–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 10.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        float: –°—Ä–µ–¥–Ω—è—è —ç–Ω—Ç—Ä–æ–ø–∏—è –ø–æ –≤—Å–µ–º —Ç–µ–º–∞–º, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∞—è –≤–Ω—É—Ç—Ä–∏—Ç–æ–ø–∏–∫–æ–≤–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç -1 –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
    &#34;&#34;&#34;
    if not hasattr(model, &#39;components_&#39;):
        print(&#34;‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –∏–º–µ–µ—Ç –∞—Ç—Ä–∏–±—É—Ç–∞ components_.&#34;)
        return -1

    topic_vectors = model.components_
    if isinstance(topic_vectors, cp.ndarray):
        topic_vectors_np = topic_vectors.get()
    else:
        topic_vectors_np = topic_vectors
    num_topics = topic_vectors_np.shape[0]

    if num_topics == 0:
        print(&#34;‚ö†Ô∏è –ù–µ—Ç —Ç–µ–º –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è.&#34;)
        return -1

    topic_entropies = []
    for topic in topic_vectors_np:
        top_word_indices = np.argsort(topic)[::-1][:num_top_words]
        top_word_probabilities = topic[top_word_indices]
        normalized_probabilities = top_word_probabilities / np.sum(top_word_probabilities)
        topic_entropy = entropy(normalized_probabilities, base=2)
        topic_entropies.append(topic_entropy)

    return np.mean(topic_entropies) if topic_entropies else -1</code></pre>
</details>
<div class="desc"><p>–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Å–ª–æ–≤ –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–π —Ç–µ–º—ã, –∏—Å–ø–æ–ª—å–∑—É—è —ç–Ω—Ç—Ä–æ–ø–∏—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ª–æ–≤.</p>
<p>–ê—Ä–≥—É–º–µ–Ω—Ç—ã:
model: –û–±—É—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å (NMF –∏–ª–∏ LDA).
feature_names (list): –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Å–ª–æ–≤) –∏–∑ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.
num_top_words (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-—Å–ª–æ–≤, —É—á–∏—Ç—ã–≤–∞–µ–º—ã—Ö –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —ç–Ω—Ç—Ä–æ–ø–∏–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 10.</p>
<p>–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
float: –°—Ä–µ–¥–Ω—è—è —ç–Ω—Ç—Ä–æ–ø–∏—è –ø–æ –≤—Å–µ–º —Ç–µ–º–∞–º, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∞—è –≤–Ω—É—Ç—Ä–∏—Ç–æ–ø–∏–∫–æ–≤–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç -1 –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.</p></div>
</dd>
<dt id="src.vectorizer.calculate_topic_coherence"><code class="name flex">
<span>def <span class="ident">calculate_topic_coherence</span></span>(<span>model, vectorizer, texts)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_topic_coherence(model, vectorizer, texts):
    &#34;&#34;&#34;–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ç–µ–º—ã –º–æ–¥–µ–ª–∏.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        model: –û–±—É—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å (NMF –∏–ª–∏ LDA).
        vectorizer: –û–±—É—á–µ–Ω–Ω—ã–π TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä.
        texts (list): –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        float: –ó–Ω–∞—á–µ–Ω–∏–µ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–º—ã. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç -999 –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
    &#34;&#34;&#34;
    try:
        feature_names_cuml = vectorizer.get_feature_names()

        if isinstance(feature_names_cuml, cudf.core.series.Series):
            feature_names = feature_names_cuml.to_pandas().tolist()
        elif not isinstance(feature_names_cuml, list):
            return -999

        if hasattr(model, &#39;components_&#39;) and feature_names is not None:
            topic_vectors = model.components_
            if isinstance(topic_vectors, cp.ndarray):
                topic_vectors_np = topic_vectors.get()
            else:
                topic_vectors_np = topic_vectors

            top_words_idx = topic_vectors_np.argsort()[:, ::-1]
            top_words = [[feature_names[i] for i in topic_word_idx[:10]] for topic_word_idx in top_words_idx]

            dictionary = Dictionary([text.split() for text in texts])
            tokenized_texts = [text.split() for text in texts]

            cm = CoherenceModel(topics=top_words, texts=tokenized_texts, dictionary=dictionary, coherence=&#39;u_mass&#39;)

            coherence_score = cm.get_coherence()
            return coherence_score
        else:
            return -999
    except Exception:
        return -999</code></pre>
</details>
<div class="desc"><p>–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ç–µ–º—ã –º–æ–¥–µ–ª–∏.</p>
<p>–ê—Ä–≥—É–º–µ–Ω—Ç—ã:
model: –û–±—É—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å (NMF –∏–ª–∏ LDA).
vectorizer: –û–±—É—á–µ–Ω–Ω—ã–π TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä.
texts (list): –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.</p>
<p>–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
float: –ó–Ω–∞—á–µ–Ω–∏–µ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–º—ã. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç -999 –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.</p></div>
</dd>
<dt id="src.vectorizer.calculate_topic_diversity"><code class="name flex">
<span>def <span class="ident">calculate_topic_diversity</span></span>(<span>model)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_topic_diversity(model):
    &#34;&#34;&#34;–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏ —Ç–µ–º.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        model: –û–±—É—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å (NMF –∏–ª–∏ LDA).

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        float: –ó–Ω–∞—á–µ–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Ç–µ–º. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç -1 –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –∏–ª–∏ –µ—Å–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º –º–µ–Ω—å—à–µ 2.
    &#34;&#34;&#34;
    if not hasattr(model, &#39;components_&#39;):
        print(&#34;‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –∏–º–µ–µ—Ç –∞—Ç—Ä–∏–±—É—Ç–∞ components_.&#34;)
        return -1

    topic_vectors = model.components_
    if isinstance(topic_vectors, cp.ndarray):
        topic_vectors_np = topic_vectors.get()
    else:
        topic_vectors_np = topic_vectors

    if topic_vectors_np.shape[0] &lt; 2:
        print(&#34;‚ö†Ô∏è –ú–µ–Ω–µ–µ –¥–≤—É—Ö —Ç–µ–º. –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤—ã—á–∏—Å–ª–∏—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ.&#34;)
        return -1

    num_topics = topic_vectors_np.shape[0]
    total_similarity = 0
    num_pairs = 0

    for i in range(num_topics):
      for j in range(i+1, num_topics):
         similarity = cosine_similarity(topic_vectors_np[i].reshape(1, -1), topic_vectors_np[j].reshape(1, -1))[0][0]
         total_similarity += similarity
         num_pairs += 1

    if num_pairs == 0:
      return -1

    average_similarity = total_similarity / num_pairs
    average_distance = 1 - average_similarity
    return average_distance</code></pre>
</details>
<div class="desc"><p>–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏ —Ç–µ–º.</p>
<p>–ê—Ä–≥—É–º–µ–Ω—Ç—ã:
model: –û–±—É—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å (NMF –∏–ª–∏ LDA).</p>
<p>–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
float: –ó–Ω–∞—á–µ–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Ç–µ–º. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç -1 –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –∏–ª–∏ –µ—Å–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º –º–µ–Ω—å—à–µ 2.</p></div>
</dd>
<dt id="src.vectorizer.clean_text"><code class="name flex">
<span>def <span class="ident">clean_text</span></span>(<span>text)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clean_text(text):
    &#34;&#34;&#34;–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞: —É–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏, –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏ —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        str: –û—á–∏—â–µ–Ω–Ω—ã–π –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫–∏.
    &#34;&#34;&#34;
    text = re.sub(r&#39;[^\w\s]&#39;, &#39;&#39;, text)
    text = text.lower()
    tokens = text.split()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return &#34; &#34;.join(tokens)</code></pre>
</details>
<div class="desc"><p>–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞: —É–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏, –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏ —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤.</p>
<p>–ê—Ä–≥—É–º–µ–Ω—Ç—ã:
text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.</p>
<p>–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
str: –û—á–∏—â–µ–Ω–Ω—ã–π –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫–∏.</p></div>
</dd>
<dt id="src.vectorizer.display_topics"><code class="name flex">
<span>def <span class="ident">display_topics</span></span>(<span>model, feature_names, num_top_words=10)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def display_topics(model, feature_names, num_top_words=10):
    &#34;&#34;&#34;–í—ã–≤–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        model: –û–±—É—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å (NMF –∏–ª–∏ LDA).
        feature_names (list): –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Å–ª–æ–≤) –∏–∑ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.
        num_top_words (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 10.
    &#34;&#34;&#34;
    for topic_idx, topic in enumerate(model.components_):
        print(f&#34;   –¢–µ–º–∞ #{topic_idx}:&#34;, end=&#39; &#39;)
        top_word_indices = topic.argsort()[::-1][:num_top_words]
        top_words = [feature_names[i] for i in top_word_indices]
        print(&#34; &#34;.join(top_words))
    print()</code></pre>
</details>
<div class="desc"><p>–í—ã–≤–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã.</p>
<p>–ê—Ä–≥—É–º–µ–Ω—Ç—ã:
model: –û–±—É—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å (NMF –∏–ª–∏ LDA).
feature_names (list): –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Å–ª–æ–≤) –∏–∑ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.
num_top_words (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 10.</p></div>
</dd>
<dt id="src.vectorizer.display_topics_with_diversity"><code class="name flex">
<span>def <span class="ident">display_topics_with_diversity</span></span>(<span>model, feature_names, num_top_words=25, num_display_words=10)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def display_topics_with_diversity(model, feature_names, num_top_words=25, num_display_words = 10):
    &#34;&#34;&#34;–í—ã–≤–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã –∏ –∏—Ö —ç–Ω—Ç—Ä–æ–ø–∏—é.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        model: –û–±—É—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å (NMF –∏–ª–∏ LDA).
        feature_names (list): –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Å–ª–æ–≤) –∏–∑ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.
        num_top_words (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-—Å–ª–æ–≤, —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º—ã—Ö –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —ç–Ω—Ç—Ä–æ–ø–∏–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 25.
        num_display_words (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 10.
    &#34;&#34;&#34;
    if not hasattr(model, &#39;components_&#39;):
        print(&#34;‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –∏–º–µ–µ—Ç –∞—Ç—Ä–∏–±—É—Ç–∞ components_.&#34;)
        return

    topic_vectors = model.components_
    if isinstance(topic_vectors, cp.ndarray):
        topic_vectors_np = topic_vectors.get()
    else:
        topic_vectors_np = topic_vectors

    for topic_idx, topic in enumerate(topic_vectors_np):
        print(f&#34;   –¢–µ–º–∞ #{topic_idx}. &#34;, end=&#39; &#39;)
        top_word_indices = np.argsort(topic)[::-1][:num_top_words]
        top_words = [feature_names[i] for i in top_word_indices]
        print(f&#34;–¢–æ–ø-{num_display_words} —Å–ª–æ–≤: {&#39; &#39;.join(top_words[:num_display_words])}&#34;)
        normalized_probabilities = topic[top_word_indices] / np.sum(topic[top_word_indices])
        topic_entropy = entropy(normalized_probabilities, base=2)
        print(f&#34;   –≠–Ω—Ç—Ä–æ–ø–∏—è —Ç–µ–º—ã: {topic_entropy:.4f}&#34;)
    print()</code></pre>
</details>
<div class="desc"><p>–í—ã–≤–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã –∏ –∏—Ö —ç–Ω—Ç—Ä–æ–ø–∏—é.</p>
<p>–ê—Ä–≥—É–º–µ–Ω—Ç—ã:
model: –û–±—É—á–µ–Ω–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å (NMF –∏–ª–∏ LDA).
feature_names (list): –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Å–ª–æ–≤) –∏–∑ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.
num_top_words (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-—Å–ª–æ–≤, —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º—ã—Ö –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —ç–Ω—Ç—Ä–æ–ø–∏–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 25.
num_display_words (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 10.</p></div>
</dd>
<dt id="src.vectorizer.reduce_dataset"><code class="name flex">
<span>def <span class="ident">reduce_dataset</span></span>(<span>df, percentage=0.1)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reduce_dataset(df, percentage=0.1):
    &#34;&#34;&#34;–£–º–µ–Ω—å—à–∞–µ—Ç —Ä–∞–∑–º–µ—Ä DataFrame –¥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–æ–ª–∏, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø–æ —É–±—ã–≤–∞–Ω–∏—é &#39;estimated_owners&#39;.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        df (pd.DataFrame): –ò—Å—Ö–æ–¥–Ω—ã–π DataFrame.
        percentage (float): –î–æ–ª—è DataFrame, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å (–∑–Ω–∞—á–µ–Ω–∏–µ –æ—Ç 0 –¥–æ 1).

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        pd.DataFrame: –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π DataFrame.

    –í—ã–∑—ã–≤–∞–µ—Ç ValueError, –µ—Å–ª–∏ percentage –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 1].
    &#34;&#34;&#34;
    if not 0 &lt;= percentage &lt;= 1:
        raise ValueError(&#34;‚ùå –ü—Ä–æ—Ü–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç 0 –¥–æ 1&#34;)

    print(f&#34;üìâ –£–º–µ–Ω—å—à–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–æ {percentage * 100}%...&#34;)
    df_sorted = df.sort_values(by=&#39;estimated_owners&#39;, ascending=False)
    num_rows = int(len(df_sorted) * percentage)
    reduced_df = df_sorted.head(num_rows)
    print(f&#34;‚úÖ –î–∞—Ç–∞—Å–µ—Ç —É–º–µ–Ω—å—à–µ–Ω –¥–æ {len(reduced_df)} —Å—Ç—Ä–æ–∫.&#34;)
    return reduced_df</code></pre>
</details>
<div class="desc"><p>–£–º–µ–Ω—å—à–∞–µ—Ç —Ä–∞–∑–º–µ—Ä DataFrame –¥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–æ–ª–∏, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø–æ —É–±—ã–≤–∞–Ω–∏—é 'estimated_owners'.</p>
<p>–ê—Ä–≥—É–º–µ–Ω—Ç—ã:
df (pd.DataFrame): –ò—Å—Ö–æ–¥–Ω—ã–π DataFrame.
percentage (float): –î–æ–ª—è DataFrame, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å (–∑–Ω–∞—á–µ–Ω–∏–µ –æ—Ç 0 –¥–æ 1).</p>
<p>–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
pd.DataFrame: –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π DataFrame.</p>
<p>–í—ã–∑—ã–≤–∞–µ—Ç ValueError, –µ—Å–ª–∏ percentage –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 1].</p></div>
</dd>
<dt id="src.vectorizer.vectorize_descriptions"><code class="name flex">
<span>def <span class="ident">vectorize_descriptions</span></span>(<span>df, nmf_params=None, lda_params=None, vectorizer_cuml=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vectorize_descriptions(df, nmf_params=None, lda_params=None, vectorizer_cuml=None):
    &#34;&#34;&#34;–í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –∏–≥—Ä, –∏—Å–ø–æ–ª—å–∑—É—è TF-IDF –∏ NMF –∏–ª–∏ LDA –¥–ª—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        df (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Å—Ç–æ–ª–±–µ—Ü &#39;short_description_clean&#39; —Å –æ—á–∏—â–µ–Ω–Ω—ã–º–∏ –æ–ø–∏—Å–∞–Ω–∏—è–º–∏.
        nmf_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è NMF. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è NMF. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.
        lda_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è LDA. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è LDA. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.
        vectorizer_cuml (CumlTfidfVectorizer, optional): –û–±—É—á–µ–Ω–Ω—ã–π CumlTfidfVectorizer. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        tuple: –ö–æ—Ä—Ç–µ–∂, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π:
            - np.ndarray: –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è (—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤–µ–∫—Ç–æ—Ä—ã).
            - NMF –∏–ª–∏ LatentDirichletAllocation: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å NMF –∏–ª–∏ LDA.

    –í—ã–∑—ã–≤–∞–µ—Ç ValueError, –µ—Å–ª–∏ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω –æ–±—É—á–µ–Ω–Ω—ã–π CumlTfidfVectorizer –∏–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã nmf_params –∏–ª–∏ lda_params.
    &#34;&#34;&#34;
    if vectorizer_cuml is None:
        raise ValueError(&#34;‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –æ–±—É—á–µ–Ω–Ω—ã–π CumlTfidfVectorizer.&#34;)
    desc_vectorized_cuml = vectorizer_cuml.transform(df[&#39;short_description_clean&#39;])
    desc_vectorized_cuml_cpu = desc_vectorized_cuml.get()
    data = cp.asnumpy(desc_vectorized_cuml_cpu.data)
    indices = cp.asnumpy(desc_vectorized_cuml_cpu.indices)
    indptr = cp.asnumpy(desc_vectorized_cuml_cpu.indptr)
    shape = desc_vectorized_cuml_cpu.shape
    desc_vectorized_cpu = csr_matrix((data, indices, indptr), shape=shape)

    if nmf_params:
        nmf = NMF(**nmf_params)
        nmf_vectorized = nmf.fit_transform(desc_vectorized_cpu)
        return nmf_vectorized, nmf
    elif lda_params:
        lda = LatentDirichletAllocation(**lda_params)
        lda_vectorized = lda.fit_transform(desc_vectorized_cpu)
        return lda_vectorized, lda
    else:
        raise ValueError(&#34;‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å nmf_params –∏–ª–∏ lda_params&#34;)</code></pre>
</details>
<div class="desc"><p>–í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –∏–≥—Ä, –∏—Å–ø–æ–ª—å–∑—É—è TF-IDF –∏ NMF –∏–ª–∏ LDA –¥–ª—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è.</p>
<p>–ê—Ä–≥—É–º–µ–Ω—Ç—ã:
df (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Å—Ç–æ–ª–±–µ—Ü 'short_description_clean' —Å –æ—á–∏—â–µ–Ω–Ω—ã–º–∏ –æ–ø–∏—Å–∞–Ω–∏—è–º–∏.
nmf_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è NMF. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è NMF. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.
lda_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è LDA. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è LDA. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.
vectorizer_cuml (CumlTfidfVectorizer, optional): –û–±—É—á–µ–Ω–Ω—ã–π CumlTfidfVectorizer. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.</p>
<p>–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
tuple: –ö–æ—Ä—Ç–µ–∂, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π:
- np.ndarray: –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è (—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤–µ–∫—Ç–æ—Ä—ã).
- NMF –∏–ª–∏ LatentDirichletAllocation: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å NMF –∏–ª–∏ LDA.</p>
<p>–í—ã–∑—ã–≤–∞–µ—Ç ValueError, –µ—Å–ª–∏ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω –æ–±—É—á–µ–Ω–Ω—ã–π CumlTfidfVectorizer –∏–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã nmf_params –∏–ª–∏ lda_params.</p></div>
</dd>
<dt id="src.vectorizer.vectorize_owners"><code class="name flex">
<span>def <span class="ident">vectorize_owners</span></span>(<span>df, method='log_scale', scaler=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vectorize_owners(df, method=&#39;log_scale&#39;, scaler=None):
    &#34;&#34;&#34;–í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö –∏–≥—Ä, –∏—Å–ø–æ–ª—å–∑—É—è –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        df (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Å—Ç–æ–ª–±–µ—Ü &#39;estimated_owners&#39;.
        method (str, optional): –ú–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: &#39;log_scale&#39; –∏–ª–∏ &#39;standard&#39;. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é &#39;log_scale&#39;.
        scaler (CumlMinMaxScaler, optional): –û–±—É—á–µ–Ω–Ω—ã–π scaler –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        np.ndarray: –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö –∏–≥—Ä.

    –í—ã–∑—ã–≤–∞–µ—Ç ValueError, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π –º–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤.
    &#34;&#34;&#34;
    owners = df[&#39;estimated_owners&#39;].values.reshape(-1, 1)
    owners = np.array(owners, dtype=float)
    owners = np.nan_to_num(owners, nan=0)
    if method == &#39;log_scale&#39;:
        owners = np.log1p(owners)
        if scaler is not None:
           owners = scaler.transform(owners)
        owners_weighted = owners * (1 + (owners * 2))
        return owners_weighted
    elif method == &#39;standard&#39;:
        if scaler is not None:
           owners = scaler.transform(owners)
        return owners
    else:
        raise ValueError(&#34;‚ùå –ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π –º–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤.&#34;)</code></pre>
</details>
<div class="desc"><p>–í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö –∏–≥—Ä, –∏—Å–ø–æ–ª—å–∑—É—è –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ.</p>
<p>–ê—Ä–≥—É–º–µ–Ω—Ç—ã:
df (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Å—Ç–æ–ª–±–µ—Ü 'estimated_owners'.
method (str, optional): –ú–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: 'log_scale' –∏–ª–∏ 'standard'. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 'log_scale'.
scaler (CumlMinMaxScaler, optional): –û–±—É—á–µ–Ω–Ω—ã–π scaler –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.</p>
<p>–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
np.ndarray: –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö –∏–≥—Ä.</p>
<p>–í—ã–∑—ã–≤–∞–µ—Ç ValueError, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π –º–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤.</p></div>
</dd>
<dt id="src.vectorizer.vectorize_tags"><code class="name flex">
<span>def <span class="ident">vectorize_tags</span></span>(<span>df, multilabel_params=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vectorize_tags(df, multilabel_params=None):
    &#34;&#34;&#34;–í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç —Ç–µ–≥–∏ –∏–≥—Ä, –∏—Å–ø–æ–ª—å–∑—É—è MultiLabelBinarizer –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ –±–∏–Ω–∞—Ä–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        df (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Å—Ç–æ–ª–±–µ—Ü &#39;all_tags&#39; —Å–æ —Å–ø–∏—Å–∫–∞–º–∏ —Ç–µ–≥–æ–≤.
        multilabel_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è MultiLabelBinarizer. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        tuple: –ö–æ—Ä—Ç–µ–∂, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π:
            - np.ndarray: –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–≥–∏.
            - MultiLabelBinarizer: –û–±—É—á–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç MultiLabelBinarizer.
    &#34;&#34;&#34;
    default_params = {&#39;sparse_output&#39;: False}
    params = multilabel_params if multilabel_params else default_params
    mlb = MultiLabelBinarizer(**params)
    mlb.fit(df[&#39;all_tags&#39;])
    tags_vectorized = mlb.transform(df[&#39;all_tags&#39;])
    return tags_vectorized, mlb</code></pre>
</details>
<div class="desc"><p>–í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç —Ç–µ–≥–∏ –∏–≥—Ä, –∏—Å–ø–æ–ª—å–∑—É—è MultiLabelBinarizer –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ –±–∏–Ω–∞—Ä–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã.</p>
<p>–ê—Ä–≥—É–º–µ–Ω—Ç—ã:
df (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Å—Ç–æ–ª–±–µ—Ü 'all_tags' —Å–æ —Å–ø–∏—Å–∫–∞–º–∏ —Ç–µ–≥–æ–≤.
multilabel_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è MultiLabelBinarizer. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.</p>
<p>–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
tuple: –ö–æ—Ä—Ç–µ–∂, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π:
- np.ndarray: –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–≥–∏.
- MultiLabelBinarizer: –û–±—É—á–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç MultiLabelBinarizer.</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.vectorizer.CombinedVectorizer"><code class="flex name class">
<span>class <span class="ident">CombinedVectorizer</span></span>
<span>(</span><span>owners_method='log_scale',<br>multilabel_params=None,<br>nmf_params=None,<br>lda_params=None,<br>tag_weight=1.0,<br>tfidf_cuml_params=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CombinedVectorizer(BaseEstimator, TransformerMixin):
    &#34;&#34;&#34;–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

    –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö, —Ç–µ–≥–∏ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –∏–≥—Ä, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä—ã.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        owners_method (str, optional): –ú–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö (&#39;log_scale&#39; –∏–ª–∏ &#39;standard&#39;). –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é &#39;log_scale&#39;.
        multilabel_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è MultiLabelBinarizer. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.
        nmf_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è NMF. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è NMF –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏–π. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.
        lda_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è LDA. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è LDA –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏–π. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.
        tag_weight (float, optional): –í–µ—Å, –ø—Ä–∏–º–µ–Ω—è–µ–º—ã–π –∫ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–º —Ç–µ–≥–∞–º. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 1.0.
        tfidf_cuml_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è CumlTfidfVectorizer. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.
    &#34;&#34;&#34;
    def __init__(self, owners_method=&#39;log_scale&#39;, multilabel_params=None, nmf_params=None, lda_params=None, tag_weight=1.0, tfidf_cuml_params=None):
        self.owners_method = owners_method
        self.multilabel_params = multilabel_params
        self.nmf_params = nmf_params
        self.lda_params = lda_params
        self.tag_weight = tag_weight
        self.tfidf_cuml_params = tfidf_cuml_params if tfidf_cuml_params else {}
        self.mlb = None
        self.nmf = None
        self.lda = None
        self.tfidf_feature_names_out_ = None
        self.scaler = CumlMinMaxScaler()
        self.transformed_owners_vectors = None
        self.transformed_tags_vectors = None
        self.transformed_desc_vectors = None
        self.transformed_combined_vectors = None

    def fit(self, X, y=None):
        &#34;&#34;&#34;–û–±—É—á–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

        –í—ã–ø–æ–ª–Ω—è–µ—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤, —Ç–µ–≥–æ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–π, –∞ —Ç–∞–∫–∂–µ –æ–±—É—á–∞–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä—ã –∏ —Å–∫–∞–ª–µ—Ä—ã.

        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            X (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ (&#39;estimated_owners&#39;, &#39;all_tags&#39;, &#39;short_description_clean&#39;).
            y (None): –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω—É–∂–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ API scikit-learn.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            CombinedVectorizer: –û–±—É—á–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä.
        &#34;&#34;&#34;
        self.owners_vectors = vectorize_owners(X, method=self.owners_method)
        self.tags_vectors, self.mlb = vectorize_tags(X, multilabel_params=self.multilabel_params)
        if isinstance(self.tags_vectors, cp.sparse.csr_matrix):
            print(&#34;‚ÑπÔ∏è –í–µ–∫—Ç–æ—Ä—ã —Ç–µ–≥–æ–≤ - cupy sparse matrix, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ numpy...&#34;)
            self.tags_vectors = np.array(cp.asnumpy(self.tags_vectors.todense()), dtype=np.float64)
        if self.tags_vectors.ndim == 1:
            self.tags_vectors = self.tags_vectors.reshape(-1, 1)

        cleaned_descriptions = X[&#39;short_description_clean&#39;].str.lower()
        self.tfidf_cuml.fit(cleaned_descriptions)
        self.tfidf_feature_names_out_ = [word for word, index in sorted(self.tfidf_cuml.vocabulary_.to_pandas().items(), key=lambda item: item[1])]

        if self.nmf_params and self.lda_params is None:
            self.desc_vectors, self.nmf = vectorize_descriptions(X, nmf_params=self.nmf_params, vectorizer_cuml=self.tfidf_cuml)
            self.lda = None
        elif self.lda_params and self.nmf_params is None:
             self.desc_vectors, self.lda = vectorize_descriptions(X, lda_params=self.lda_params, vectorizer_cuml=self.tfidf_cuml)
             self.nmf = None
        else:
            raise ValueError(&#34;‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å nmf_params –∏–ª–∏ lda_params&#34;)
        print(&#34;‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞&#34;)

        if self.nmf and hasattr(self.nmf, &#39;components_&#39;):
             if np.isnan(self.nmf.components_).any():
                print(&#34;‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN –∑–Ω–∞—á–µ–Ω–∏—è –≤ self.nmf.components_ –≤ fit()!&#34;)
        if self.lda and hasattr(self.lda, &#39;components_&#39;):
             if np.isnan(self.lda.components_).any():
                 print(&#34;‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN –∑–Ω–∞—á–µ–Ω–∏—è –≤ self.lda.components_ –≤ fit()!&#34;)

        owners_vectors = vectorize_owners(X, method=self.owners_method)
        self.scaler.fit(owners_vectors)
        return self

    def transform(self, X, y=None):
        &#34;&#34;&#34;–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä—ã –∏ —Å–∫–∞–ª–µ—Ä—ã –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö, —Ç–µ–≥–æ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–π –≤ –µ–¥–∏–Ω–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ.

        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            X (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏.
            y (None): –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω—É–∂–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ API scikit-learn.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            np.ndarray: –ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
        &#34;&#34;&#34;
        owners_vectors = vectorize_owners(X, method=self.owners_method, scaler=self.scaler)
        owners_vectors = owners_vectors.reshape(owners_vectors.shape[0], -1)
        tags_vectors = self.mlb.transform(X[&#39;all_tags&#39;])

        tag_weight = self.tag_weight
        tags_vectors_weighted = tags_vectors * tag_weight
        tags_vectors = tags_vectors_weighted

        tfidf_transformed_cuml = self.tfidf_cuml.transform(X[&#39;short_description_clean&#39;])
        tfidf_transformed_cpu = tfidf_transformed_cuml.get()
        data = cp.asnumpy(tfidf_transformed_cpu.data)
        indices = cp.asnumpy(tfidf_transformed_cpu.indices)
        indptr = cp.asnumpy(tfidf_transformed_cpu.indptr)
        shape = tfidf_transformed_cpu.shape
        tfidf_transformed = csr_matrix((data, indices, indptr), shape=shape)

        desc_vectors = None
        if self.nmf_params:
            desc_vectors = self.nmf.transform(tfidf_transformed)
        elif self.lda_params:
            desc_vectors = self.lda.transform(tfidf_transformed)

        if desc_vectors is not None and desc_vectors.shape[0] != owners_vectors.shape[0]:
            raise ValueError(f&#34;‚ùå –ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—Ä–∞–∑—Ü–æ–≤ –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–π: {owners_vectors.shape[0]} vs {desc_vectors.shape[0]}&#34;)

        self.transformed_owners_vectors = owners_vectors
        self.transformed_tags_vectors = tags_vectors
        self.transformed_desc_vectors = desc_vectors

        combined_vectors = np.hstack([owners_vectors, tags_vectors.toarray() if hasattr(tags_vectors, &#39;toarray&#39;) else tags_vectors, desc_vectors])
        self.transformed_combined_vectors = combined_vectors

        return combined_vectors

    def get_params(self, deep=True):
        &#34;&#34;&#34;–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–∞–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞, –≤–∫–ª—é—á–∞—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤—Å–µ—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–æ–≤ –∏ –º–µ—Ç–æ–¥–æ–≤.

        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            deep (bool, optional): –ï—Å–ª–∏ True, —Ç–∞–∫–∂–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –æ—Ü–µ–Ω—â–∏–∫–∞–º–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é True.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            dict: –°–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.
        &#34;&#34;&#34;
        return {
            &#39;owners_method&#39;: self.owners_method,
            &#39;multilabel_params&#39;: self.multilabel_params,
             &#39;nmf_params&#39;: self.nmf_params,
            &#39;lda_params&#39;: self.lda_params,
            &#39;tag_weight&#39;: self.tag_weight,
            &#39;tfidf_cuml_params&#39;: self.tfidf_cuml_params
        }

    def set_params(self, **params):
        &#34;&#34;&#34;–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.

        –ü–æ–∑–≤–æ–ª—è–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏.

        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            **params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ –≤ –≤–∏–¥–µ keyword arguments.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            CombinedVectorizer: –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
        &#34;&#34;&#34;
        if &#39;owners_method&#39; in params:
            self.owners_method = params[&#39;owners_method&#39;]
        if &#39;multilabel_params&#39; in params:
            self.multilabel_params = params[&#39;multilabel_params&#39;]
        if &#39;nmf_params&#39; in params:
            self.nmf_params = params[&#39;nmf_params&#39;]
        if &#39;lda_params&#39; in params:
             self.lda_params = params[&#39;lda_params&#39;]
        if &#39;tag_weight&#39; in params:
            self.tag_weight = params[&#39;tag_weight&#39;]
        if &#39;tfidf_cuml_params&#39; in params:
            self.tfidf_cuml_params = params[&#39;tfidf_cuml_params&#39;]
            self.tfidf_cuml.set_params(**params[&#39;tfidf_cuml_params&#39;])
        return self</code></pre>
</details>
<div class="desc"><p>–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.</p>
<p>–í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö, —Ç–µ–≥–∏ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –∏–≥—Ä, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä—ã.</p>
<p>–ê—Ä–≥—É–º–µ–Ω—Ç—ã:
owners_method (str, optional): –ú–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö ('log_scale' –∏–ª–∏ 'standard'). –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 'log_scale'.
multilabel_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è MultiLabelBinarizer. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.
nmf_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è NMF. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è NMF –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏–π. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.
lda_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è LDA. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è LDA –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏–π. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.
tag_weight (float, optional): –í–µ—Å, –ø—Ä–∏–º–µ–Ω—è–µ–º—ã–π –∫ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–º —Ç–µ–≥–∞–º. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 1.0.
tfidf_cuml_params (dict, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è CumlTfidfVectorizer. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin</li>
<li>sklearn.utils._metadata_requests._MetadataRequester</li>
<li>sklearn.base.TransformerMixin</li>
<li>sklearn.utils._set_output._SetOutputMixin</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.vectorizer.CombinedVectorizer.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X, y=None):
    &#34;&#34;&#34;–û–±—É—á–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

    –í—ã–ø–æ–ª–Ω—è–µ—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤, —Ç–µ–≥–æ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–π, –∞ —Ç–∞–∫–∂–µ –æ–±—É—á–∞–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä—ã –∏ —Å–∫–∞–ª–µ—Ä—ã.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        X (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ (&#39;estimated_owners&#39;, &#39;all_tags&#39;, &#39;short_description_clean&#39;).
        y (None): –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω—É–∂–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ API scikit-learn.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        CombinedVectorizer: –û–±—É—á–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä.
    &#34;&#34;&#34;
    self.owners_vectors = vectorize_owners(X, method=self.owners_method)
    self.tags_vectors, self.mlb = vectorize_tags(X, multilabel_params=self.multilabel_params)
    if isinstance(self.tags_vectors, cp.sparse.csr_matrix):
        print(&#34;‚ÑπÔ∏è –í–µ–∫—Ç–æ—Ä—ã —Ç–µ–≥–æ–≤ - cupy sparse matrix, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ numpy...&#34;)
        self.tags_vectors = np.array(cp.asnumpy(self.tags_vectors.todense()), dtype=np.float64)
    if self.tags_vectors.ndim == 1:
        self.tags_vectors = self.tags_vectors.reshape(-1, 1)

    cleaned_descriptions = X[&#39;short_description_clean&#39;].str.lower()
    self.tfidf_cuml.fit(cleaned_descriptions)
    self.tfidf_feature_names_out_ = [word for word, index in sorted(self.tfidf_cuml.vocabulary_.to_pandas().items(), key=lambda item: item[1])]

    if self.nmf_params and self.lda_params is None:
        self.desc_vectors, self.nmf = vectorize_descriptions(X, nmf_params=self.nmf_params, vectorizer_cuml=self.tfidf_cuml)
        self.lda = None
    elif self.lda_params and self.nmf_params is None:
         self.desc_vectors, self.lda = vectorize_descriptions(X, lda_params=self.lda_params, vectorizer_cuml=self.tfidf_cuml)
         self.nmf = None
    else:
        raise ValueError(&#34;‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å nmf_params –∏–ª–∏ lda_params&#34;)
    print(&#34;‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞&#34;)

    if self.nmf and hasattr(self.nmf, &#39;components_&#39;):
         if np.isnan(self.nmf.components_).any():
            print(&#34;‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN –∑–Ω–∞—á–µ–Ω–∏—è –≤ self.nmf.components_ –≤ fit()!&#34;)
    if self.lda and hasattr(self.lda, &#39;components_&#39;):
         if np.isnan(self.lda.components_).any():
             print(&#34;‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN –∑–Ω–∞—á–µ–Ω–∏—è –≤ self.lda.components_ –≤ fit()!&#34;)

    owners_vectors = vectorize_owners(X, method=self.owners_method)
    self.scaler.fit(owners_vectors)
    return self</code></pre>
</details>
<div class="desc"><p>–û–±—É—á–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.</p>
<p>–í—ã–ø–æ–ª–Ω—è–µ—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤, —Ç–µ–≥–æ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–π, –∞ —Ç–∞–∫–∂–µ –æ–±—É—á–∞–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä—ã –∏ —Å–∫–∞–ª–µ—Ä—ã.</p>
<p>–ê—Ä–≥—É–º–µ–Ω—Ç—ã:
X (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ ('estimated_owners', 'all_tags', 'short_description_clean').
y (None): –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω—É–∂–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ API scikit-learn.</p>
<p>–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
CombinedVectorizer: –û–±—É—á–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä.</p></div>
</dd>
<dt id="src.vectorizer.CombinedVectorizer.get_params"><code class="name flex">
<span>def <span class="ident">get_params</span></span>(<span>self, deep=True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_params(self, deep=True):
    &#34;&#34;&#34;–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–∞–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞, –≤–∫–ª—é—á–∞—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤—Å–µ—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–æ–≤ –∏ –º–µ—Ç–æ–¥–æ–≤.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        deep (bool, optional): –ï—Å–ª–∏ True, —Ç–∞–∫–∂–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –æ—Ü–µ–Ω—â–∏–∫–∞–º–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é True.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        dict: –°–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.
    &#34;&#34;&#34;
    return {
        &#39;owners_method&#39;: self.owners_method,
        &#39;multilabel_params&#39;: self.multilabel_params,
         &#39;nmf_params&#39;: self.nmf_params,
        &#39;lda_params&#39;: self.lda_params,
        &#39;tag_weight&#39;: self.tag_weight,
        &#39;tfidf_cuml_params&#39;: self.tfidf_cuml_params
    }</code></pre>
</details>
<div class="desc"><p>–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.</p>
<p>–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–∞–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞, –≤–∫–ª—é—á–∞—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤—Å–µ—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–æ–≤ –∏ –º–µ—Ç–æ–¥–æ–≤.</p>
<p>–ê—Ä–≥—É–º–µ–Ω—Ç—ã:
deep (bool, optional): –ï—Å–ª–∏ True, —Ç–∞–∫–∂–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –æ—Ü–µ–Ω—â–∏–∫–∞–º–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é True.</p>
<p>–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
dict: –°–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.</p></div>
</dd>
<dt id="src.vectorizer.CombinedVectorizer.set_params"><code class="name flex">
<span>def <span class="ident">set_params</span></span>(<span>self, **params)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_params(self, **params):
    &#34;&#34;&#34;–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.

    –ü–æ–∑–≤–æ–ª—è–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        **params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ –≤ –≤–∏–¥–µ keyword arguments.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        CombinedVectorizer: –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
    &#34;&#34;&#34;
    if &#39;owners_method&#39; in params:
        self.owners_method = params[&#39;owners_method&#39;]
    if &#39;multilabel_params&#39; in params:
        self.multilabel_params = params[&#39;multilabel_params&#39;]
    if &#39;nmf_params&#39; in params:
        self.nmf_params = params[&#39;nmf_params&#39;]
    if &#39;lda_params&#39; in params:
         self.lda_params = params[&#39;lda_params&#39;]
    if &#39;tag_weight&#39; in params:
        self.tag_weight = params[&#39;tag_weight&#39;]
    if &#39;tfidf_cuml_params&#39; in params:
        self.tfidf_cuml_params = params[&#39;tfidf_cuml_params&#39;]
        self.tfidf_cuml.set_params(**params[&#39;tfidf_cuml_params&#39;])
    return self</code></pre>
</details>
<div class="desc"><p>–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞.</p>
<p>–ü–æ–∑–≤–æ–ª—è–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏.</p>
<p>–ê—Ä–≥—É–º–µ–Ω—Ç—ã:
**params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ –≤ –≤–∏–¥–µ keyword arguments.</p>
<p>–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
CombinedVectorizer: –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.</p></div>
</dd>
<dt id="src.vectorizer.CombinedVectorizer.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, X, y=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, X, y=None):
    &#34;&#34;&#34;–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä—ã –∏ —Å–∫–∞–ª–µ—Ä—ã –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö, —Ç–µ–≥–æ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–π –≤ –µ–¥–∏–Ω–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        X (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏.
        y (None): –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω—É–∂–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ API scikit-learn.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        np.ndarray: –ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    &#34;&#34;&#34;
    owners_vectors = vectorize_owners(X, method=self.owners_method, scaler=self.scaler)
    owners_vectors = owners_vectors.reshape(owners_vectors.shape[0], -1)
    tags_vectors = self.mlb.transform(X[&#39;all_tags&#39;])

    tag_weight = self.tag_weight
    tags_vectors_weighted = tags_vectors * tag_weight
    tags_vectors = tags_vectors_weighted

    tfidf_transformed_cuml = self.tfidf_cuml.transform(X[&#39;short_description_clean&#39;])
    tfidf_transformed_cpu = tfidf_transformed_cuml.get()
    data = cp.asnumpy(tfidf_transformed_cpu.data)
    indices = cp.asnumpy(tfidf_transformed_cpu.indices)
    indptr = cp.asnumpy(tfidf_transformed_cpu.indptr)
    shape = tfidf_transformed_cpu.shape
    tfidf_transformed = csr_matrix((data, indices, indptr), shape=shape)

    desc_vectors = None
    if self.nmf_params:
        desc_vectors = self.nmf.transform(tfidf_transformed)
    elif self.lda_params:
        desc_vectors = self.lda.transform(tfidf_transformed)

    if desc_vectors is not None and desc_vectors.shape[0] != owners_vectors.shape[0]:
        raise ValueError(f&#34;‚ùå –ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—Ä–∞–∑—Ü–æ–≤ –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–π: {owners_vectors.shape[0]} vs {desc_vectors.shape[0]}&#34;)

    self.transformed_owners_vectors = owners_vectors
    self.transformed_tags_vectors = tags_vectors
    self.transformed_desc_vectors = desc_vectors

    combined_vectors = np.hstack([owners_vectors, tags_vectors.toarray() if hasattr(tags_vectors, &#39;toarray&#39;) else tags_vectors, desc_vectors])
    self.transformed_combined_vectors = combined_vectors

    return combined_vectors</code></pre>
</details>
<div class="desc"><p>–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.</p>
<p>–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä—ã –∏ —Å–∫–∞–ª–µ—Ä—ã –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö, —Ç–µ–≥–æ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–π –≤ –µ–¥–∏–Ω–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ.</p>
<p>–ê—Ä–≥—É–º–µ–Ω—Ç—ã:
X (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏.
y (None): –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω—É–∂–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ API scikit-learn.</p>
<p>–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
np.ndarray: –ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.</p></div>
</dd>
</dl>
</dd>
<dt id="src.vectorizer.TorchLDA"><code class="flex name class">
<span>class <span class="ident">TorchLDA</span></span>
<span>(</span><span>n_topics, n_vocab, device, alpha=0.1, beta=0.01, max_iterations=100, tolerance=0.0001)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TorchLDA(nn.Module):
    &#34;&#34;&#34;–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ LDA (Latent Dirichlet Allocation) –Ω–∞ PyTorch.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è LDA, –ø–æ–∑–≤–æ–ª—è—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        n_topics (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è.
        n_vocab (int): –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤).
        device (torch.device): –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è –æ–±—É—á–µ–Ω–∏–µ (CPU –∏–ª–∏ GPU).
        alpha (float, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä –∞–ø—Ä–∏–æ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –î–∏—Ä–∏—Ö–ª–µ –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.1.
        beta (float, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä –∞–ø—Ä–∏–æ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –î–∏—Ä–∏—Ö–ª–µ –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º –ø–æ —Å–ª–æ–≤–∞–º. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.01.
        max_iterations (int, optional): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 100.
        tolerance (float, optional): –ü–æ—Ä–æ–≥ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–ª—è EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 1e-4.
    &#34;&#34;&#34;
    def __init__(self, n_topics, n_vocab, device, alpha=0.1, beta=0.01, max_iterations=100, tolerance=1e-4):
        super().__init__()
        self.n_topics = n_topics
        self.n_vocab = n_vocab
        self.device = device
        self.alpha = alpha
        self.beta = beta
        self.max_iterations = max_iterations
        self.tolerance = tolerance
        self.topic_term_matrix = nn.Parameter(torch.randn(n_topics, n_vocab, device=device).abs())
        self.doc_topic_matrix = None
        self.norm_topic_term_matrix = None

    def initialize_parameters(self, docs):
        &#34;&#34;&#34;–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ LDA —Å–ª—É—á–∞–π–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏.

        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏, –Ω–æ –æ–∂–∏–¥–∞–µ—Ç—Å—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞).
        &#34;&#34;&#34;
        self.doc_topic_matrix = torch.rand(docs.shape[0], self.n_topics, device=self.device).abs()
        self.topic_term_matrix.data = torch.randn(self.n_topics, self.n_vocab, device=self.device).abs()

    def fit(self, docs, log=False):
        &#34;&#34;&#34;–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å LDA –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è EM-–∞–ª–≥–æ—Ä–∏—Ç–º.

        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ x —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è).
            log (bool, optional): –í–∫–ª—é—á–∞–µ—Ç –≤—ã–≤–æ–¥ –ª–æ–≥–æ–≤ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é False.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            TorchLDA: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å LDA.
        &#34;&#34;&#34;
        if log: print(&#34;LDA Fit started&#34;)
        self.initialize_parameters(docs)
        docs = docs.to(self.device)
        prev_likelihood = float(&#39;-inf&#39;)
        for iteration in range(self.max_iterations):
            doc_topic_distribution = self.expect(docs)
            self.topic_term_matrix = self.maximize(docs, doc_topic_distribution)
            current_likelihood = self.likelihood(docs, doc_topic_distribution)
            if log: print(f&#34;Iteration {iteration+1}, Likelihood {current_likelihood:.2f}&#34;)
            if abs(current_likelihood - prev_likelihood) &lt; self.tolerance:
                if log: print(&#34;LDA Converged&#34;)
                break
            prev_likelihood = current_likelihood
        self.norm_topic_term_matrix = self.normalize(self.topic_term_matrix)
        if log: print(&#34;LDA Fit ended&#34;)
        return self

    def expect(self, docs):
        &#34;&#34;&#34;–í—ã–ø–æ–ª–Ω—è–µ—Ç E-—à–∞–≥ EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞ –¥–ª—è LDA: –æ—Ü–µ–Ω–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.

        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            torch.Tensor: –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.
        &#34;&#34;&#34;
        doc_topic_distribution = torch.matmul(docs, self.topic_term_matrix.T) + self.alpha
        doc_topic_distribution = self.normalize(doc_topic_distribution)
        return doc_topic_distribution

    def maximize(self, docs, doc_topic_distribution):
        &#34;&#34;&#34;–í—ã–ø–æ–ª–Ω—è–µ—Ç M-—à–∞–≥ EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞ –¥–ª—è LDA: –æ—Ü–µ–Ω–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º –ø–æ —Å–ª–æ–≤–∞–º.

        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
            doc_topic_distribution (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            torch.Tensor: –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º –ø–æ —Å–ª–æ–≤–∞–º.
        &#34;&#34;&#34;
        topic_term_matrix = torch.matmul(doc_topic_distribution.T, docs) + self.beta
        return topic_term_matrix

    def likelihood(self, docs, doc_topic_distribution):
        &#34;&#34;&#34;–í—ã—á–∏—Å–ª—è–µ—Ç –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞.

        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
            doc_topic_distribution (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            float: –ó–Ω–∞—á–µ–Ω–∏–µ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è.
        &#34;&#34;&#34;
        log_likelihood = torch.sum(docs * torch.log(torch.matmul(doc_topic_distribution, self.normalize(self.topic_term_matrix))))
        return log_likelihood.item()

    def normalize(self, matrix):
        &#34;&#34;&#34;–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –º–∞—Ç—Ä–∏—Ü—É, –ø—Ä–∏–≤–æ–¥—è —Å—É–º–º—ã —Å—Ç—Ä–æ–∫ –∫ –µ–¥–∏–Ω–∏—Ü–µ.

        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            matrix (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            torch.Tensor: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞.
        &#34;&#34;&#34;
        row_sums = matrix.sum(axis=1, keepdim=True)
        return matrix / row_sums

    def transform(self, docs):
        &#34;&#34;&#34;–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ç–µ–º.

        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            torch.Tensor: –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º –¥–ª—è –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

        –í—ã–∑—ã–≤–∞–µ—Ç ValueError, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å LDA –µ—â–µ –Ω–µ –æ–±—É—á–µ–Ω–∞.
        &#34;&#34;&#34;
        if self.norm_topic_term_matrix is None:
            raise ValueError(&#34;‚ùå LDA model has not been fitted yet.&#34;)
        docs = docs.to(self.device)
        doc_topic_distribution = torch.matmul(docs, self.norm_topic_term_matrix.T) + self.alpha
        return self.normalize(doc_topic_distribution)</code></pre>
</details>
<div class="desc"><p>–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ LDA (Latent Dirichlet Allocation) –Ω–∞ PyTorch.</p>
<p>–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è LDA, –ø–æ–∑–≤–æ–ª—è—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.</p>
<p>–ê—Ä–≥—É–º–µ–Ω—Ç—ã:
n_topics (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è.
n_vocab (int): –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤).
device (torch.device): –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è –æ–±—É—á–µ–Ω–∏–µ (CPU –∏–ª–∏ GPU).
alpha (float, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä –∞–ø—Ä–∏–æ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –î–∏—Ä–∏—Ö–ª–µ –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.1.
beta (float, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä –∞–ø—Ä–∏–æ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –î–∏—Ä–∏—Ö–ª–µ –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º –ø–æ —Å–ª–æ–≤–∞–º. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.01.
max_iterations (int, optional): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 100.
tolerance (float, optional): –ü–æ—Ä–æ–≥ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–ª—è EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 1e-4.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.vectorizer.TorchLDA.expect"><code class="name flex">
<span>def <span class="ident">expect</span></span>(<span>self, docs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expect(self, docs):
    &#34;&#34;&#34;–í—ã–ø–æ–ª–Ω—è–µ—Ç E-—à–∞–≥ EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞ –¥–ª—è LDA: –æ—Ü–µ–Ω–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        torch.Tensor: –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.
    &#34;&#34;&#34;
    doc_topic_distribution = torch.matmul(docs, self.topic_term_matrix.T) + self.alpha
    doc_topic_distribution = self.normalize(doc_topic_distribution)
    return doc_topic_distribution</code></pre>
</details>
<div class="desc"><p>–í—ã–ø–æ–ª–Ω—è–µ—Ç E-—à–∞–≥ EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞ –¥–ª—è LDA: –æ—Ü–µ–Ω–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.</p>
<p>–ê—Ä–≥—É–º–µ–Ω—Ç—ã:
docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.</p>
<p>–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
torch.Tensor: –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.</p></div>
</dd>
<dt id="src.vectorizer.TorchLDA.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, docs, log=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, docs, log=False):
    &#34;&#34;&#34;–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å LDA –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è EM-–∞–ª–≥–æ—Ä–∏—Ç–º.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ x —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è).
        log (bool, optional): –í–∫–ª—é—á–∞–µ—Ç –≤—ã–≤–æ–¥ –ª–æ–≥–æ–≤ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é False.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        TorchLDA: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å LDA.
    &#34;&#34;&#34;
    if log: print(&#34;LDA Fit started&#34;)
    self.initialize_parameters(docs)
    docs = docs.to(self.device)
    prev_likelihood = float(&#39;-inf&#39;)
    for iteration in range(self.max_iterations):
        doc_topic_distribution = self.expect(docs)
        self.topic_term_matrix = self.maximize(docs, doc_topic_distribution)
        current_likelihood = self.likelihood(docs, doc_topic_distribution)
        if log: print(f&#34;Iteration {iteration+1}, Likelihood {current_likelihood:.2f}&#34;)
        if abs(current_likelihood - prev_likelihood) &lt; self.tolerance:
            if log: print(&#34;LDA Converged&#34;)
            break
        prev_likelihood = current_likelihood
    self.norm_topic_term_matrix = self.normalize(self.topic_term_matrix)
    if log: print(&#34;LDA Fit ended&#34;)
    return self</code></pre>
</details>
<div class="desc"><p>–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å LDA –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è EM-–∞–ª–≥–æ—Ä–∏—Ç–º.</p>
<p>–ê—Ä–≥—É–º–µ–Ω—Ç—ã:
docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ x —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è).
log (bool, optional): –í–∫–ª—é—á–∞–µ—Ç –≤—ã–≤–æ–¥ –ª–æ–≥–æ–≤ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é False.</p>
<p>–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
TorchLDA: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å LDA.</p></div>
</dd>
<dt id="src.vectorizer.TorchLDA.initialize_parameters"><code class="name flex">
<span>def <span class="ident">initialize_parameters</span></span>(<span>self, docs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize_parameters(self, docs):
    &#34;&#34;&#34;–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ LDA —Å–ª—É—á–∞–π–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏, –Ω–æ –æ–∂–∏–¥–∞–µ—Ç—Å—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞).
    &#34;&#34;&#34;
    self.doc_topic_matrix = torch.rand(docs.shape[0], self.n_topics, device=self.device).abs()
    self.topic_term_matrix.data = torch.randn(self.n_topics, self.n_vocab, device=self.device).abs()</code></pre>
</details>
<div class="desc"><p>–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ LDA —Å–ª—É—á–∞–π–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏.</p>
<p>–ê—Ä–≥—É–º–µ–Ω—Ç—ã:
docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏, –Ω–æ –æ–∂–∏–¥–∞–µ—Ç—Å—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞).</p></div>
</dd>
<dt id="src.vectorizer.TorchLDA.likelihood"><code class="name flex">
<span>def <span class="ident">likelihood</span></span>(<span>self, docs, doc_topic_distribution)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def likelihood(self, docs, doc_topic_distribution):
    &#34;&#34;&#34;–í—ã—á–∏—Å–ª—è–µ—Ç –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
        doc_topic_distribution (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        float: –ó–Ω–∞—á–µ–Ω–∏–µ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è.
    &#34;&#34;&#34;
    log_likelihood = torch.sum(docs * torch.log(torch.matmul(doc_topic_distribution, self.normalize(self.topic_term_matrix))))
    return log_likelihood.item()</code></pre>
</details>
<div class="desc"><p>–í—ã—á–∏—Å–ª—è–µ—Ç –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞.</p>
<p>–ê—Ä–≥—É–º–µ–Ω—Ç—ã:
docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
doc_topic_distribution (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.</p>
<p>–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
float: –ó–Ω–∞—á–µ–Ω–∏–µ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è.</p></div>
</dd>
<dt id="src.vectorizer.TorchLDA.maximize"><code class="name flex">
<span>def <span class="ident">maximize</span></span>(<span>self, docs, doc_topic_distribution)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def maximize(self, docs, doc_topic_distribution):
    &#34;&#34;&#34;–í—ã–ø–æ–ª–Ω—è–µ—Ç M-—à–∞–≥ EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞ –¥–ª—è LDA: –æ—Ü–µ–Ω–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º –ø–æ —Å–ª–æ–≤–∞–º.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
        doc_topic_distribution (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        torch.Tensor: –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º –ø–æ —Å–ª–æ–≤–∞–º.
    &#34;&#34;&#34;
    topic_term_matrix = torch.matmul(doc_topic_distribution.T, docs) + self.beta
    return topic_term_matrix</code></pre>
</details>
<div class="desc"><p>–í—ã–ø–æ–ª–Ω—è–µ—Ç M-—à–∞–≥ EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞ –¥–ª—è LDA: –æ—Ü–µ–Ω–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º –ø–æ —Å–ª–æ–≤–∞–º.</p>
<p>–ê—Ä–≥—É–º–µ–Ω—Ç—ã:
docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
doc_topic_distribution (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º.</p>
<p>–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
torch.Tensor: –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º –ø–æ —Å–ª–æ–≤–∞–º.</p></div>
</dd>
<dt id="src.vectorizer.TorchLDA.normalize"><code class="name flex">
<span>def <span class="ident">normalize</span></span>(<span>self, matrix)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def normalize(self, matrix):
    &#34;&#34;&#34;–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –º–∞—Ç—Ä–∏—Ü—É, –ø—Ä–∏–≤–æ–¥—è —Å—É–º–º—ã —Å—Ç—Ä–æ–∫ –∫ –µ–¥–∏–Ω–∏—Ü–µ.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        matrix (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        torch.Tensor: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞.
    &#34;&#34;&#34;
    row_sums = matrix.sum(axis=1, keepdim=True)
    return matrix / row_sums</code></pre>
</details>
<div class="desc"><p>–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –º–∞—Ç—Ä–∏—Ü—É, –ø—Ä–∏–≤–æ–¥—è —Å—É–º–º—ã —Å—Ç—Ä–æ–∫ –∫ –µ–¥–∏–Ω–∏—Ü–µ.</p>
<p>–ê—Ä–≥—É–º–µ–Ω—Ç—ã:
matrix (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.</p>
<p>–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
torch.Tensor: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞.</p></div>
</dd>
<dt id="src.vectorizer.TorchLDA.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, docs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, docs):
    &#34;&#34;&#34;–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ç–µ–º.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        torch.Tensor: –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º –¥–ª—è –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

    –í—ã–∑—ã–≤–∞–µ—Ç ValueError, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å LDA –µ—â–µ –Ω–µ –æ–±—É—á–µ–Ω–∞.
    &#34;&#34;&#34;
    if self.norm_topic_term_matrix is None:
        raise ValueError(&#34;‚ùå LDA model has not been fitted yet.&#34;)
    docs = docs.to(self.device)
    doc_topic_distribution = torch.matmul(docs, self.norm_topic_term_matrix.T) + self.alpha
    return self.normalize(doc_topic_distribution)</code></pre>
</details>
<div class="desc"><p>–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ç–µ–º.</p>
<p>–ê—Ä–≥—É–º–µ–Ω—Ç—ã:
docs (torch.Tensor): –ú–∞—Ç—Ä–∏—Ü–∞ –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.</p>
<p>–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
torch.Tensor: –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º –¥–ª—è –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.</p>
<p>–í—ã–∑—ã–≤–∞–µ—Ç ValueError, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å LDA –µ—â–µ –Ω–µ –æ–±—É—á–µ–Ω–∞.</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src" href="index.html">src</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="src.vectorizer.calculate_intra_topic_diversity" href="#src.vectorizer.calculate_intra_topic_diversity">calculate_intra_topic_diversity</a></code></li>
<li><code><a title="src.vectorizer.calculate_topic_coherence" href="#src.vectorizer.calculate_topic_coherence">calculate_topic_coherence</a></code></li>
<li><code><a title="src.vectorizer.calculate_topic_diversity" href="#src.vectorizer.calculate_topic_diversity">calculate_topic_diversity</a></code></li>
<li><code><a title="src.vectorizer.clean_text" href="#src.vectorizer.clean_text">clean_text</a></code></li>
<li><code><a title="src.vectorizer.display_topics" href="#src.vectorizer.display_topics">display_topics</a></code></li>
<li><code><a title="src.vectorizer.display_topics_with_diversity" href="#src.vectorizer.display_topics_with_diversity">display_topics_with_diversity</a></code></li>
<li><code><a title="src.vectorizer.reduce_dataset" href="#src.vectorizer.reduce_dataset">reduce_dataset</a></code></li>
<li><code><a title="src.vectorizer.vectorize_descriptions" href="#src.vectorizer.vectorize_descriptions">vectorize_descriptions</a></code></li>
<li><code><a title="src.vectorizer.vectorize_owners" href="#src.vectorizer.vectorize_owners">vectorize_owners</a></code></li>
<li><code><a title="src.vectorizer.vectorize_tags" href="#src.vectorizer.vectorize_tags">vectorize_tags</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.vectorizer.CombinedVectorizer" href="#src.vectorizer.CombinedVectorizer">CombinedVectorizer</a></code></h4>
<ul class="">
<li><code><a title="src.vectorizer.CombinedVectorizer.fit" href="#src.vectorizer.CombinedVectorizer.fit">fit</a></code></li>
<li><code><a title="src.vectorizer.CombinedVectorizer.get_params" href="#src.vectorizer.CombinedVectorizer.get_params">get_params</a></code></li>
<li><code><a title="src.vectorizer.CombinedVectorizer.set_params" href="#src.vectorizer.CombinedVectorizer.set_params">set_params</a></code></li>
<li><code><a title="src.vectorizer.CombinedVectorizer.transform" href="#src.vectorizer.CombinedVectorizer.transform">transform</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.vectorizer.TorchLDA" href="#src.vectorizer.TorchLDA">TorchLDA</a></code></h4>
<ul class="">
<li><code><a title="src.vectorizer.TorchLDA.expect" href="#src.vectorizer.TorchLDA.expect">expect</a></code></li>
<li><code><a title="src.vectorizer.TorchLDA.fit" href="#src.vectorizer.TorchLDA.fit">fit</a></code></li>
<li><code><a title="src.vectorizer.TorchLDA.initialize_parameters" href="#src.vectorizer.TorchLDA.initialize_parameters">initialize_parameters</a></code></li>
<li><code><a title="src.vectorizer.TorchLDA.likelihood" href="#src.vectorizer.TorchLDA.likelihood">likelihood</a></code></li>
<li><code><a title="src.vectorizer.TorchLDA.maximize" href="#src.vectorizer.TorchLDA.maximize">maximize</a></code></li>
<li><code><a title="src.vectorizer.TorchLDA.normalize" href="#src.vectorizer.TorchLDA.normalize">normalize</a></code></li>
<li><code><a title="src.vectorizer.TorchLDA.transform" href="#src.vectorizer.TorchLDA.transform">transform</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.5</a>.</p>
</footer>
</body>
</html>
